[
  {
    "filename": "auto-approve.yml",
    "name": "Auto-Approve Content",
    "on": {
      "schedule": [
        {
          "cron": "0 */6 * * *"
        }
      ],
      "workflow_dispatch": {
        "inputs": {
          "force_approval": {
            "description": "Force approval of more content",
            "required": false,
            "default": "false",
            "type": "boolean"
          },
          "max_items": {
            "description": "Maximum items to approve",
            "required": false,
            "default": "200",
            "type": "string"
          },
          "min_confidence": {
            "description": "Minimum confidence score",
            "required": false,
            "default": "0.4",
            "type": "string"
          }
        }
      }
    },
    "jobs": {
      "auto-approve": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Check Auto-Approval Status",
            "id": "check-status",
            "run": "echo \"üîç Checking auto-approval candidates...\"\n\nresponse=$(curl -L -s -w \"%{http_code}\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  \"${{ secrets.SITE_URL }}/api/admin/auto-approve\")\n\nhttp_code=\"${response: -3}\"\nbody=\"${response%???}\"\n\nif [ \"$http_code\" = \"200\" ]; then\n  echo \"‚úÖ Status check successful\"\n  echo \"$body\" | jq -r '.estimation.recommendedAction // \"No action needed\"'\n  \n  # Extract candidate count for decision making\n  candidates=$(echo \"$body\" | jq -r '.totalCandidates // 0')\n  echo \"candidates=$candidates\" >> $GITHUB_OUTPUT\n  \n  echo \"üìä Found $candidates approval candidates\"\nelse\n  echo \"‚ùå Status check failed with code $http_code\"\n  echo \"$body\"\n  exit 1\nfi\n"
          },
          {
            "name": "Run Auto-Approval",
            "if": "steps.check-status.outputs.candidates > 20 || github.event_name == 'workflow_dispatch'",
            "run": "echo \"ü§ñ Running auto-approval process...\"\n\n# Set parameters from workflow inputs or defaults\nFORCE_APPROVAL=\"${{ github.event.inputs.force_approval || 'false' }}\"\nMAX_ITEMS=\"${{ github.event.inputs.max_items || '200' }}\"\nMIN_CONFIDENCE=\"${{ github.event.inputs.min_confidence || '0.4' }}\"\n\necho \"‚öôÔ∏è Parameters: force=$FORCE_APPROVAL, max=$MAX_ITEMS, min_confidence=$MIN_CONFIDENCE\"\n\nresponse=$(curl -L -s -w \"%{http_code}\" \\\n  -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -d \"{\n    \\\"forceApproval\\\": $FORCE_APPROVAL,\n    \\\"maxItems\\\": $MAX_ITEMS,\n    \\\"minConfidenceScore\\\": $MIN_CONFIDENCE\n  }\" \\\n  \"${{ secrets.SITE_URL }}/api/admin/auto-approve\")\n\nhttp_code=\"${response: -3}\"\nbody=\"${response%???}\"\n\nif [ \"$http_code\" = \"200\" ]; then\n  echo \"‚úÖ Auto-approval completed successfully\"\n  echo \"$body\" | jq -r '.message'\n  \n  # Extract results for reporting\n  total_approved=$(echo \"$body\" | jq -r '.approvalResults.total // 0')\n  days_of_content=$(echo \"$body\" | jq -r '.updatedStats.daysOfContent // 0')\n  \n  echo \"üìä Results:\"\n  echo \"  ‚Ä¢ Total approved: $total_approved items\"\n  echo \"  ‚Ä¢ Days of content remaining: $days_of_content days\"\n  echo \"  ‚Ä¢ Breakdown: $(echo \"$body\" | jq -c '.approvalResults')\"\n  \n  # Check if we need alerts\n  if [ \"$days_of_content\" -lt 7 ]; then\n    echo \"‚ö†Ô∏è WARNING: Only $days_of_content days of content remaining!\"\n    echo \"::warning title=Low Content Buffer::Only $days_of_content days of content remaining. Consider running emergency scan.\"\n  fi\nelse\n  echo \"‚ùå Auto-approval failed with code $http_code\"\n  echo \"$body\"\n  echo \"::error title=Auto-Approval Failed::Auto-approval process failed with HTTP $http_code\"\n  exit 1\nfi\n"
          },
          {
            "name": "Skip Auto-Approval",
            "if": "steps.check-status.outputs.candidates <= 20 && github.event_name != 'workflow_dispatch'",
            "run": "echo \"‚úÖ Auto-approval not needed\"\necho \"üìä Only ${{ steps.check-status.outputs.candidates }} candidates found (threshold: 20)\"\necho \"üéØ System is healthy - maintaining current approval levels\"\n"
          },
          {
            "name": "Verify Content Pipeline Health",
            "run": "echo \"üè• Checking overall content pipeline health...\"\n\nresponse=$(curl -L -s -w \"%{http_code}\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  \"${{ secrets.SITE_URL }}/api/admin/automation-health\")\n\nhttp_code=\"${response: -3}\"\nbody=\"${response%???}\"\n\nif [ \"$http_code\" = \"200\" ]; then\n  overall_health=$(echo \"$body\" | jq -r '.overallHealth // \"unknown\"')\n  days_remaining=$(echo \"$body\" | jq -r '.contentSummary.daysRemaining // 0')\n  \n  echo \"üìä Overall Health: $overall_health\"\n  echo \"üìÖ Days Remaining: $days_remaining\"\n  \n  # Set workflow status based on health\n  case \"$overall_health\" in\n    \"critical\")\n      echo \"::error title=Critical Content Health::Content pipeline is in critical state\"\n      ;;\n    \"warning\") \n      echo \"::warning title=Content Health Warning::Content pipeline needs attention\"\n      ;;\n    \"healthy\")\n      echo \"‚úÖ Content pipeline is healthy\"\n      ;;\n  esac\n  \n  # Output recommendations\n  recommendations=$(echo \"$body\" | jq -r '.recommendations[]?' 2>/dev/null || echo \"No specific recommendations\")\n  if [ \"$recommendations\" != \"No specific recommendations\" ]; then\n    echo \"üí° Recommendations:\"\n    echo \"$body\" | jq -r '.recommendations[]' | sed 's/^/  ‚Ä¢ /'\n  fi\nelse\n  echo \"‚ö†Ô∏è Could not verify pipeline health (HTTP $http_code)\"\nfi\n"
          },
          {
            "name": "Report Summary",
            "run": "echo \"üìã Auto-Approval Workflow Summary\"\necho \"=================================\"\necho \"‚è∞ Triggered: $(date -u)\"\necho \"üéØ Trigger: ${{ github.event_name }}\"\necho \"üèóÔ∏è Environment: $([ '${{ github.event_name }}' = 'workflow_dispatch' ] && echo 'Manual' || echo 'Scheduled')\"\necho \"üìä Candidates checked: ${{ steps.check-status.outputs.candidates || 'N/A' }}\"\necho \"‚úÖ Workflow completed successfully\"\n"
          }
        ]
      }
    },
    "intent": "Scans social media platforms for hotdog-related content",
    "category": "scanning"
  },
  {
    "filename": "auto-queue-manager.yml",
    "name": "Auto Queue Manager",
    "on": {
      "schedule": [
        {
          "cron": "0 */6 * * *"
        }
      ],
      "workflow_dispatch": {
        "inputs": {
          "mode": {
            "description": "Scan mode",
            "required": false,
            "default": "auto",
            "type": "choice",
            "options": [
              "auto",
              "emergency",
              "status-only"
            ]
          },
          "force": {
            "description": "Force scan regardless of recommendations",
            "required": false,
            "default": false,
            "type": "boolean"
          }
        }
      }
    },
    "jobs": {
      "refresh-token": {
        "uses": "./.github/workflows/token-refresh.yml",
        "secrets": {
          "SITE_URL": "${{ secrets.SITE_URL }}",
          "SERVICE_ACCOUNT_SECRET": "${{ secrets.SERVICE_ACCOUNT_SECRET }}",
          "REFRESH_TOKEN": "${{ secrets.REFRESH_TOKEN }}",
          "AUTH_TOKEN": "${{ secrets.AUTH_TOKEN }}"
        }
      },
      "auto-queue-manager": {
        "needs": "refresh-token",
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Check Queue Health",
            "id": "queue-health",
            "env": {
              "AUTH_TOKEN": "${{ needs.refresh-token.outputs.auth_token }}"
            },
            "run": "echo \"ü©∫ Checking queue health...\"\n\n# Get current queue health status\nHEALTH_RESPONSE=$(curl -s -w \"HTTP_CODE:%{http_code}\" -L \\\n  \"${{ secrets.SITE_URL }}/api/admin/queue-health\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  --retry 2 --retry-delay 5 2>&1 || true)\n\n# Extract HTTP code and response body\nHTTP_CODE=$(echo \"$HEALTH_RESPONSE\" | grep -o 'HTTP_CODE:[0-9]*' | cut -d':' -f2)\nRESPONSE_BODY=$(echo \"$HEALTH_RESPONSE\" | sed 's/HTTP_CODE:[0-9]*$//')\n\necho \"Queue Health HTTP Status: $HTTP_CODE\"\n\nif [[ \"$HTTP_CODE\" == \"200\" ]]; then\n  echo \"‚úÖ Queue health check successful\"\n  \n  # Extract key metrics using jq\n  HEALTH_STATUS=$(echo \"$RESPONSE_BODY\" | jq -r '.health.status // \"unknown\"')\n  DAYS_CONTENT=$(echo \"$RESPONSE_BODY\" | jq -r '.queue.daysOfContent // 0')\n  APPROVED_CONTENT=$(echo \"$RESPONSE_BODY\" | jq -r '.queue.totalApproved // 0')\n  \n  echo \"üìä Queue Status: $HEALTH_STATUS\"\n  echo \"üìÖ Days of Content: $DAYS_CONTENT\"\n  echo \"‚úÖ Approved Content: $APPROVED_CONTENT\"\n  \n  # Set outputs for next step\n  echo \"health_status=$HEALTH_STATUS\" >> $GITHUB_OUTPUT\n  echo \"days_content=$DAYS_CONTENT\" >> $GITHUB_OUTPUT\n  echo \"approved_content=$APPROVED_CONTENT\" >> $GITHUB_OUTPUT\n  echo \"health_check_success=true\" >> $GITHUB_OUTPUT\nelse\n  echo \"‚ùå Queue health check failed with HTTP $HTTP_CODE\"\n  echo \"Response: $RESPONSE_BODY\"\n  echo \"health_check_success=false\" >> $GITHUB_OUTPUT\nfi\n"
          },
          {
            "name": "Determine Scan Mode",
            "id": "scan-mode",
            "run": "# Get input mode or default to auto\nINPUT_MODE=\"${{ github.event.inputs.mode || 'auto' }}\"\nFORCE_SCAN=\"${{ github.event.inputs.force || 'false' }}\"\n\n# Get health metrics from previous step\nHEALTH_STATUS=\"${{ steps.queue-health.outputs.health_status }}\"\nDAYS_CONTENT=\"${{ steps.queue-health.outputs.days_content }}\"\nAPPROVED_CONTENT=\"${{ steps.queue-health.outputs.approved_content }}\"\n\necho \"üéØ Determining scan mode...\"\necho \"Input mode: $INPUT_MODE\"\necho \"Health status: $HEALTH_STATUS\"\necho \"Days of content: $DAYS_CONTENT\"\necho \"Force scan: $FORCE_SCAN\"\n\n# Determine final scan mode based on health\nFINAL_MODE=\"$INPUT_MODE\"\n\nif [[ \"$HEALTH_STATUS\" == \"emergency\" ]] || [[ \"${APPROVED_CONTENT%.*}\" -eq 0 ]]; then\n  FINAL_MODE=\"emergency\"\n  echo \"üö® EMERGENCY: Queue is empty - forcing emergency mode\"\nelif [[ \"$HEALTH_STATUS\" == \"critical\" ]] && [[ \"$INPUT_MODE\" == \"auto\" ]]; then\n  FINAL_MODE=\"emergency\" \n  echo \"‚ö†Ô∏è CRITICAL: Queue is very low - upgrading to emergency mode\"\nelif [[ \"$FORCE_SCAN\" == \"true\" ]]; then\n  echo \"üîß Force scan enabled - proceeding with requested mode\"\nfi\n\necho \"final_mode=$FINAL_MODE\" >> $GITHUB_OUTPUT\necho \"should_scan=true\" >> $GITHUB_OUTPUT\necho \"üìã Final scan mode: $FINAL_MODE\"\n"
          },
          {
            "name": "Execute Auto-Scan",
            "if": "steps.scan-mode.outputs.should_scan == 'true'",
            "env": {
              "AUTH_TOKEN": "${{ needs.refresh-token.outputs.auth_token }}"
            },
            "run": "SCAN_MODE=\"${{ steps.scan-mode.outputs.final_mode }}\"\n\necho \"üöÄ Executing auto-scan with mode: $SCAN_MODE\"\n\n# Build request payload\nif [[ \"$SCAN_MODE\" == \"emergency\" ]]; then\n  PAYLOAD='{\"mode\": \"emergency\"}'\nelif [[ \"$SCAN_MODE\" == \"status-only\" ]]; then\n  PAYLOAD='{\"mode\": \"status-only\"}'\nelse\n  PAYLOAD='{\"mode\": \"auto\"}'\nfi\n\necho \"üì§ Request payload: $PAYLOAD\"\n\n# Execute auto-scan\nSCAN_RESPONSE=$(curl -s -w \"HTTP_CODE:%{http_code}\" -L -X POST \\\n  \"${{ secrets.SITE_URL }}/api/admin/auto-scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"$PAYLOAD\" \\\n  --retry 3 --retry-delay 10 2>&1 || true)\n\n# Extract results\nHTTP_CODE=$(echo \"$SCAN_RESPONSE\" | grep -o 'HTTP_CODE:[0-9]*' | cut -d':' -f2)\nRESPONSE_BODY=$(echo \"$SCAN_RESPONSE\" | sed 's/HTTP_CODE:[0-9]*$//')\n\necho \"Auto-Scan HTTP Status: $HTTP_CODE\"\n\nif [[ \"$HTTP_CODE\" == \"200\" ]]; then\n  echo \"‚úÖ Auto-scan completed successfully\"\n  \n  # Extract scan results\n  TRIGGERED=$(echo \"$RESPONSE_BODY\" | jq -r '.summary.totalTriggered // 0')\n  SKIPPED=$(echo \"$RESPONSE_BODY\" | jq -r '.summary.totalSkipped // 0') \n  ERRORS=$(echo \"$RESPONSE_BODY\" | jq -r '.summary.totalErrors // 0')\n  DAYS_AFTER=$(echo \"$RESPONSE_BODY\" | jq -r '.summary.queueDaysAfter // 0')\n  \n  echo \"üìä Scan Results:\"\n  echo \"  - Triggered: $TRIGGERED scans\"\n  echo \"  - Skipped: $SKIPPED scans\"\n  echo \"  - Errors: $ERRORS\"\n  echo \"  - Days of content after: $DAYS_AFTER\"\n  \n  # Show triggered scans\n  TRIGGERED_SCANS=$(echo \"$RESPONSE_BODY\" | jq -r '.triggeredScans[]? // empty')\n  if [[ -n \"$TRIGGERED_SCANS\" ]]; then\n    echo \"üéØ Triggered scans:\"\n    echo \"$TRIGGERED_SCANS\" | sed 's/^/  - /'\n  fi\n  \n  # Show any errors\n  SCAN_ERRORS=$(echo \"$RESPONSE_BODY\" | jq -r '.errors[]? // empty')\n  if [[ -n \"$SCAN_ERRORS\" ]]; then\n    echo \"‚ö†Ô∏è Scan errors:\"\n    echo \"$SCAN_ERRORS\" | sed 's/^/  - /'\n  fi\n  \nelse\n  echo \"‚ùå Auto-scan failed with HTTP $HTTP_CODE\"\n  echo \"Response: $RESPONSE_BODY\"\n  exit 1\nfi\n"
          },
          {
            "name": "Final Health Check",
            "if": "success()",
            "env": {
              "AUTH_TOKEN": "${{ needs.refresh-token.outputs.auth_token }}"
            },
            "run": "echo \"üîÑ Performing final health check...\"\n\n# Brief delay to allow scan results to process\nsleep 30\n\n# Get updated queue health\nFINAL_HEALTH=$(curl -s -L \\\n  \"${{ secrets.SITE_URL }}/api/admin/queue-health\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  --retry 2 2>/dev/null | jq -r '.health.status // \"unknown\"')\n\nFINAL_DAYS=$(curl -s -L \\\n  \"${{ secrets.SITE_URL }}/api/admin/queue-health\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  --retry 2 2>/dev/null | jq -r '.queue.daysOfContent // 0')\n\necho \"üìà Final queue status: $FINAL_HEALTH\"\necho \"üìÖ Final days of content: $FINAL_DAYS\"\n\nif [[ \"$FINAL_HEALTH\" == \"healthy\" ]] || [[ \"$FINAL_HEALTH\" == \"warning\" ]]; then\n  echo \"‚úÖ Queue is in acceptable condition\"\nelse\n  echo \"‚ö†Ô∏è Queue still needs attention: $FINAL_HEALTH\"\nfi\n"
          },
          {
            "name": "Handle Failure",
            "if": "failure()",
            "run": "echo \"‚ùå CRITICAL: Auto queue manager failed\"\necho \"\"\necho \"üîß TROUBLESHOOTING STEPS:\"\necho \"  1. Check AUTH_TOKEN in GitHub secrets\"\necho \"  2. Verify Vercel deployment status\"\necho \"  3. Check API endpoint availability\"\necho \"  4. Review platform API credentials\"\necho \"\"\necho \"üìû IMMEDIATE ACTIONS:\"\necho \"  - Manual content approval may be needed\"\necho \"  - Consider running emergency replenishment\"\necho \"  - Check individual platform scan workflows\"\n"
          }
        ]
      }
    },
    "intent": "General workflow (purpose unclear from name)",
    "category": "maintenance"
  },
  {
    "filename": "ci-new.yml",
    "name": "CI Test",
    "on": {
      "push": {
        "branches": [
          "main"
        ]
      }
    },
    "jobs": {
      "test": {
        "name": "Test Job",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 5,
        "steps": [
          {
            "name": "Checkout repository",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js",
            "uses": "actions/setup-node@v4",
            "with": {
              "node-version": "20",
              "cache": "npm"
            }
          },
          {
            "name": "Cache node_modules",
            "id": "node-modules-cache",
            "uses": "actions/cache@v4",
            "with": {
              "path": "node_modules",
              "key": "node-modules-${{ runner.os }}-${{ hashFiles('package-lock.json') }}",
              "restore-keys": "node-modules-${{ runner.os }}-\n"
            }
          },
          {
            "name": "Install dependencies",
            "if": "steps.node-modules-cache.outputs.cache-hit != 'true'",
            "run": "npm ci"
          },
          {
            "name": "Simple test",
            "run": "echo \"‚úÖ CI workflow syntax is working!\""
          }
        ]
      }
    },
    "intent": "Runs automated tests and builds",
    "category": "test"
  },
  {
    "filename": "ci-test.yml",
    "name": "CI Test",
    "on": {
      "push": {
        "branches": [
          "main"
        ]
      }
    },
    "jobs": {
      "test": {
        "name": "Test Job",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 5,
        "steps": [
          {
            "name": "Checkout repository",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js",
            "uses": "actions/setup-node@v4",
            "with": {
              "node-version": "20",
              "cache": "npm"
            }
          },
          {
            "name": "Cache node_modules",
            "id": "node-modules-cache",
            "uses": "actions/cache@v4",
            "with": {
              "path": "node_modules",
              "key": "node-modules-${{ runner.os }}-${{ hashFiles('package-lock.json') }}",
              "restore-keys": "node-modules-${{ runner.os }}-\n"
            }
          },
          {
            "name": "Install dependencies",
            "if": "steps.node-modules-cache.outputs.cache-hit != 'true'",
            "run": "npm ci"
          },
          {
            "name": "Simple test",
            "run": "echo \"‚úÖ CI workflow syntax is working!\""
          }
        ]
      }
    },
    "intent": "Runs automated tests and builds",
    "category": "test"
  },
  {
    "filename": "ci.yml",
    "name": "CI",
    "on": {
      "push": {
        "branches": [
          "main",
          "develop"
        ]
      },
      "pull_request": {
        "branches": [
          "main"
        ]
      },
      "workflow_call": {
        "inputs": {
          "node-version": {
            "description": "Node.js version to use",
            "type": "string",
            "default": "20"
          },
          "cache-strategy": {
            "description": "Cache strategy for dependencies",
            "type": "string",
            "default": "aggressive"
          }
        },
        "outputs": {
          "test-results": {
            "description": "Test results summary",
            "value": "${{ jobs.test.outputs.results }}"
          }
        }
      }
    },
    "concurrency": {
      "group": "ci-${{ github.ref }}",
      "cancel-in-progress": true
    },
    "env": {
      "NODE_ENV": "test",
      "CI": true
    },
    "jobs": {
      "lint": {
        "name": "Lint & Format Check",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 10,
        "if": "!contains(github.event.head_commit.message, '[skip ci]')",
        "outputs": {
          "lint-status": "${{ steps.lint.outcome }}"
        },
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "${{ inputs.node-version || '20' }}",
              "cache-key-suffix": "lint"
            }
          },
          {
            "name": "Install tsx for scripts",
            "run": "npm install --no-save tsx"
          },
          {
            "name": "Patch legacy path resolver",
            "run": "npx tsx scripts/patchPathResolver.ts"
          },
          {
            "name": "Validate route patterns",
            "run": "npx tsx scripts/validateRoutes.ts"
          },
          {
            "name": "Run ESLint",
            "id": "lint",
            "run": "echo \"üîç Running ESLint...\"\n# Run ESLint but don't fail the build on warnings - capture exit code separately\nnpx next lint --max-warnings=2000 2>&1 | tee lint-results.txt\nLINT_EXIT_CODE=${PIPESTATUS[0]}\n\n# Count warnings for reporting\nWARNING_COUNT=$(grep -c \"Warning:\" lint-results.txt || echo \"0\")\necho \"Found $WARNING_COUNT ESLint warnings\"\n\n# Only capture first 100 lines to avoid HEREDOC issues\necho \"lint-output<<EOF\" >> $GITHUB_OUTPUT\nhead -n 100 lint-results.txt >> $GITHUB_OUTPUT\necho \"EOF\" >> $GITHUB_OUTPUT\n\necho \"warning-count=$WARNING_COUNT\" >> $GITHUB_OUTPUT\n\n# Only fail if there are actual ESLint errors (not warnings)\nif [ $LINT_EXIT_CODE -ne 0 ] && grep -q \"Error:\" lint-results.txt; then\n  echo \"‚ùå ESLint found errors (not just warnings)\"\n  exit 1\nelse\n  echo \"‚úÖ ESLint completed (warnings: $WARNING_COUNT)\"\n  exit 0\nfi\n"
          },
          {
            "name": "Check Prettier formatting",
            "run": "echo \"üé® Checking code formatting...\"\nnpm run format:check || true\n"
          },
          {
            "name": "Upload lint results",
            "if": "failure()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "lint-results",
              "path": "lint-results.txt",
              "retention-days": 7
            }
          }
        ]
      },
      "typecheck": {
        "name": "TypeScript Type Check",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 8,
        "outputs": {
          "typecheck-status": "${{ steps.typecheck.outcome }}"
        },
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "${{ inputs.node-version || '20' }}",
              "cache-key-suffix": "typecheck"
            }
          },
          {
            "name": "Run TypeScript compiler",
            "id": "typecheck",
            "run": "echo \"üîç Running TypeScript type checking...\"\nnpm run type-check 2>&1 | tee typecheck-results.txt\necho \"typecheck-output<<EOF\" >> $GITHUB_OUTPUT\ncat typecheck-results.txt >> $GITHUB_OUTPUT\necho \"EOF\" >> $GITHUB_OUTPUT\n"
          },
          {
            "name": "Upload typecheck results",
            "if": "failure()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "typecheck-results",
              "path": "typecheck-results.txt",
              "retention-days": 7
            }
          }
        ]
      },
      "test": {
        "name": "Unit Tests",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 15,
        "env": {
          "DATABASE_URL_SQLITE": "./test_hotdog_diaries.db",
          "JWT_SECRET": "test-jwt-secret-for-ci"
        },
        "outputs": {
          "results": "${{ steps.test.outputs.results }}",
          "coverage": "${{ steps.test.outputs.coverage }}"
        },
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "${{ inputs.node-version || '20' }}",
              "cache-key-suffix": "test"
            }
          },
          {
            "name": "Initialize test database",
            "run": "echo \"üóÑÔ∏è Setting up test database...\"\nNODE_ENV=test npx tsx scripts/init-sqlite.ts || echo \"Database init attempted\"\n"
          },
          {
            "name": "Run unit tests",
            "id": "test",
            "run": "echo \"üß™ Running unit tests...\"\n# Run tests but don't fail CI on test failures\nnpm test 2>&1 | tee test-results.txt || true\n\n# Extract test summary\nTESTS_PASSED=$(grep -o \"Tests:.*passed\" test-results.txt | head -1 || echo \"Tests: unknown\")\nTESTS_FAILED=$(grep -o \"Tests:.*failed\" test-results.txt | head -1 || echo \"Tests: unknown\")\nTEST_SUITES=$(grep -o \"Test Suites:.*\" test-results.txt | head -1 || echo \"Test Suites: unknown\")\n\necho \"results=${TESTS_PASSED}\" >> $GITHUB_OUTPUT\necho \"failures=${TESTS_FAILED}\" >> $GITHUB_OUTPUT\necho \"suites=${TEST_SUITES}\" >> $GITHUB_OUTPUT\n\necho \"‚ÑπÔ∏è Test results: ${TEST_SUITES}\"\necho \"‚ÑπÔ∏è Individual tests: ${TESTS_PASSED}, ${TESTS_FAILED}\"\n"
          },
          {
            "name": "Run unit tests with coverage",
            "run": "echo \"üìä Running tests with coverage...\"\n# Run coverage but don't fail CI on coverage issues\nnpm run test:coverage || echo \"‚ö†Ô∏è Coverage completed with issues\"\n"
          },
          {
            "name": "Upload test results",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "test-results",
              "path": "test-results.txt\ncoverage/\n",
              "retention-days": 7
            }
          }
        ]
      },
      "planner-tests": {
        "name": "Planner Contract Tests",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 5,
        "env": {
          "SCAN_MIN_PER_PLATFORM": 40,
          "SCAN_MAX_PER_PLATFORM": 120,
          "SCAN_GLOBAL_MAX": 800,
          "SCAN_COOLDOWN_MIN": 180,
          "MIN_CONF": 0.7,
          "MIN_CANDIDATES": 20,
          "PLATFORM_ALLOW": "reddit,youtube,giphy,imgur,bluesky,tumblr,lemmy,pixabay"
        },
        "outputs": {
          "planner-status": "${{ steps.planner-test.outcome }}"
        },
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "${{ inputs.node-version || '20' }}",
              "cache-key-suffix": "planner"
            }
          },
          {
            "name": "Run planner contract tests",
            "id": "planner-test",
            "run": "echo \"üß™ Running demand-driven scanner planner tests...\"\nnpm run test:planner 2>&1 | tee planner-test-results.txt\n\n# Extract test summary\nif grep -q \"tests passed\" planner-test-results.txt; then\n  echo \"‚úÖ Planner tests passed\"\n  echo \"planner-results=passed\" >> $GITHUB_OUTPUT\nelse\n  echo \"‚ùå Planner tests failed\"\n  echo \"planner-results=failed\" >> $GITHUB_OUTPUT\nfi\n"
          },
          {
            "name": "Upload planner test results",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "planner-test-results",
              "path": "planner-test-results.txt\nscan_plan.json\nscan_matrix.json\n",
              "retention-days": 7,
              "if-no-files-found": "ignore"
            }
          }
        ]
      },
      "security": {
        "name": "Security & Dependency Audit",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 10,
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "${{ inputs.node-version || '20' }}",
              "cache-key-suffix": "security"
            }
          },
          {
            "name": "Install tsx for scripts",
            "run": "npm install --no-save tsx"
          },
          {
            "name": "Audit for legacy dependencies",
            "run": "echo \"üîç Scanning for problematic legacy dependencies...\"\nnpx tsx scripts/findOffender.ts || (echo \"‚ùå Legacy modules found\" && exit 1)\necho \"‚úÖ Dependency audit passed\"\n"
          },
          {
            "name": "Run security audit",
            "run": "echo \"üîí Running npm security audit...\"\nnpm audit --audit-level=high || echo \"‚ö†Ô∏è Vulnerabilities found - review required\"\n"
          }
        ]
      },
      "build": {
        "name": "Build Check",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 10,
        "needs": [
          "lint",
          "typecheck",
          "planner-tests"
        ],
        "if": "always() && !cancelled()",
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "${{ inputs.node-version || '20' }}",
              "cache-key-suffix": "build"
            }
          },
          {
            "name": "Install tsx for scripts",
            "run": "npm install --no-save tsx"
          },
          {
            "name": "Patch legacy path resolver (build phase)",
            "run": "npx tsx scripts/patchPathResolver.ts"
          },
          {
            "name": "Validate route patterns (build phase)",
            "run": "npx tsx scripts/validateRoutes.ts"
          },
          {
            "name": "Cache Next.js build",
            "uses": "actions/cache@v4",
            "with": {
              "path": ".next/cache\n",
              "key": "nextjs-${{ runner.os }}-${{ hashFiles('package-lock.json') }}-${{ hashFiles('**/*.js', '**/*.jsx', '**/*.ts', '**/*.tsx') }}",
              "restore-keys": "nextjs-${{ runner.os }}-${{ hashFiles('package-lock.json') }}-\n"
            }
          },
          {
            "name": "Build application",
            "env": {
              "DATABASE_URL_SQLITE": "./test_build.db",
              "NODE_ENV": "test"
            },
            "run": "echo \"üèóÔ∏è Building application...\"\nnpm run build\n"
          },
          {
            "name": "Verify build artifacts",
            "run": "echo \"‚úÖ Verifying build output...\"\nls -la .next/\necho \"Build completed successfully\"\n"
          }
        ]
      },
      "auto-healing": {
        "name": "Auto-Healing Gate",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 10,
        "needs": [
          "lint",
          "typecheck",
          "test",
          "planner-tests",
          "security",
          "build"
        ],
        "if": "always() && contains(needs.*.result, 'failure')",
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4",
            "with": {
              "token": "${{ secrets.GITHUB_TOKEN }}",
              "fetch-depth": 0
            }
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "${{ inputs.node-version || '20' }}",
              "cache-key-suffix": "healing"
            }
          },
          {
            "name": "Install tsx for scripts",
            "run": "npm install --no-save tsx"
          },
          {
            "name": "Run critical failure gatekeeper",
            "id": "gatekeeper",
            "run": "echo \"üõ°Ô∏è Running CI Auto-Healing System...\"\nnpx tsx scripts/checkCriticalFailures.ts || GATEKEEPER_EXIT=$?\n\n# Check if auto-fixes were applied\nif [ -f \"reports/ci-health-gate.md\" ]; then\n  AUTO_FIXES=$(grep -c \"Auto-Fixes Applied\" reports/ci-health-gate.md || echo \"0\")\n  echo \"auto-fixes-applied=$AUTO_FIXES\" >> $GITHUB_OUTPUT\n  echo \"‚úÖ Auto-healing analysis completed\"\nelse\n  echo \"auto-fixes-applied=0\" >> $GITHUB_OUTPUT\nfi\n\nexit ${GATEKEEPER_EXIT:-0}\n"
          },
          {
            "name": "Upload healing reports",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "ci-healing-reports",
              "path": "reports/ci-health-gate.md\nreports/lint-auto-fix.md\nreports/security-audit.md\n",
              "retention-days": 30
            }
          },
          {
            "name": "Commit auto-fixes",
            "if": "steps.gatekeeper.outputs.auto-fixes-applied != '0'",
            "run": "echo \"üîß Auto-fixes applied, committing changes...\"\ngit config --local user.email \"action@github.com\"\ngit config --local user.name \"GitHub Action Auto-Fix\"\ngit add -A\n\nif ! git diff --staged --quiet; then\n  git commit -m \"fix: auto-fix CI issues\n  \n  - Applied ${{ steps.gatekeeper.outputs.auto-fixes-applied }} automatic fixes\n  - Generated by CI Auto-Healing System\n  \n  [skip ci]\"\n  git push\n  echo \"‚úÖ Auto-fixes committed and pushed\"\nfi\n"
          }
        ]
      },
      "summary": {
        "name": "CI Summary",
        "runs-on": "ubuntu-latest",
        "needs": [
          "lint",
          "typecheck",
          "test",
          "planner-tests",
          "security",
          "build"
        ],
        "if": "always()",
        "steps": [
          {
            "name": "Generate CI summary",
            "run": "echo \"## üöÄ CI Results Summary\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"| Job | Status | Details |\" >> $GITHUB_STEP_SUMMARY\necho \"|-----|--------|---------|\" >> $GITHUB_STEP_SUMMARY\necho \"| Lint | ${{ needs.lint.result }} | ${{ needs.lint.outputs.lint-status }} |\" >> $GITHUB_STEP_SUMMARY\necho \"| TypeCheck | ${{ needs.typecheck.result }} | ${{ needs.typecheck.outputs.typecheck-status }} |\" >> $GITHUB_STEP_SUMMARY\necho \"| Unit Tests | ${{ needs.test.result }} | ${{ needs.test.outputs.results }} |\" >> $GITHUB_STEP_SUMMARY\necho \"| Planner Tests | ${{ needs.planner-tests.result }} | ${{ needs.planner-tests.outputs.planner-status }} |\" >> $GITHUB_STEP_SUMMARY\necho \"| Security | ${{ needs.security.result }} | Dependency audit |\" >> $GITHUB_STEP_SUMMARY\necho \"| Build | ${{ needs.build.result }} | Build verification |\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Overall status\nif [[ \"${{ needs.lint.result }}\" == \"success\" && \"${{ needs.typecheck.result }}\" == \"success\" && \"${{ needs.test.result }}\" == \"success\" && \"${{ needs.planner-tests.result }}\" == \"success\" && \"${{ needs.security.result }}\" == \"success\" && \"${{ needs.build.result }}\" == \"success\" ]]; then\n  echo \"## ‚úÖ All CI checks passed!\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"## ‚ùå Some CI checks failed - Auto-healing may have been triggered\" >> $GITHUB_STEP_SUMMARY\nfi\n"
          }
        ]
      }
    },
    "intent": "Runs automated tests and builds",
    "category": "test"
  },
  {
    "filename": "cleanup-duplicates.yml",
    "name": "Database Cleanup",
    "on": {
      "schedule": [
        {
          "cron": "0 6 * * *"
        }
      ],
      "workflow_dispatch": {
        "inputs": {
          "dry_run": {
            "description": "Dry run (no actual deletion)",
            "required": false,
            "type": "boolean",
            "default": false
          }
        }
      }
    },
    "permissions": {
      "contents": "read"
    },
    "jobs": {
      "cleanup-duplicates": {
        "runs-on": "ubuntu-latest",
        "name": "Remove Duplicate Content",
        "steps": [
          {
            "name": "Cleanup Duplicates",
            "run": "echo \"üßπ Starting duplicate cleanup...\"\n\n# Call the cleanup API endpoint\nresponse=$(curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/cleanup-duplicates\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"dry_run\": ${{ github.event.inputs.dry_run || false }}}' \\\n  --fail-with-body \\\n  -w \"\\n%{http_code}\")\n\n# Extract HTTP status code\nhttp_code=$(echo \"$response\" | tail -n 1)\nbody=$(echo \"$response\" | head -n -1)\n\necho \"Response: $body\"\n\nif [ \"$http_code\" -ne 200 ]; then\n  echo \"‚ùå Cleanup failed with status $http_code\"\n  exit 1\nfi\n\necho \"‚úÖ Cleanup completed successfully\"\n"
          },
          {
            "name": "Check Duplicate Status",
            "if": "success()",
            "run": "echo \"üìä Checking for remaining duplicates...\"\n\n# Call the monitor endpoint\nresponse=$(curl -L -X GET \"${{ secrets.SITE_URL }}/api/admin/monitor/duplicates\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  --fail-with-body)\n\necho \"Duplicate status: $response\"\n\n# Parse response and check if duplicates exist\nif echo \"$response\" | grep -q '\"duplicates_found\":0'; then\n  echo \"‚úÖ No duplicates found\"\nelse\n  echo \"‚ö†Ô∏è  Some duplicates may still exist\"\nfi\n"
          },
          {
            "name": "Log Failure Details",
            "if": "failure()",
            "run": "echo \"üö® Duplicate cleanup job failed at $(date)\"\necho \"Check the workflow logs for details\"\necho \"Workflow run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"\n"
          }
        ]
      },
      "sync-posted-flags": {
        "runs-on": "ubuntu-latest",
        "name": "Sync Posted Flags",
        "needs": "cleanup-duplicates",
        "steps": [
          {
            "name": "Sync is_posted Flags",
            "run": "echo \"üîÑ Syncing is_posted flags...\"\n\nresponse=$(curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/sync/posted-flags\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  --fail-with-body)\n\necho \"Sync response: $response\"\necho \"‚úÖ Sync completed\"\n"
          }
        ]
      }
    },
    "intent": "Performs maintenance and cleanup tasks",
    "category": "maintenance"
  },
  {
    "filename": "daily-ingestion-report.yml",
    "name": "Daily Ingestion Balance Report",
    "on": {
      "schedule": [
        {
          "cron": "0 9 * * *"
        }
      ],
      "workflow_dispatch": {
        "inputs": {
          "format": {
            "description": "Output format",
            "required": false,
            "default": "table",
            "type": "choice",
            "options": [
              "table",
              "json"
            ]
          }
        }
      }
    },
    "jobs": {
      "ingestion-report": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "uses": "actions/checkout@v4"
          },
          {
            "uses": "actions/setup-node@v4",
            "with": {
              "node-version": "18",
              "cache": "npm"
            }
          },
          {
            "run": "npm ci"
          },
          {
            "name": "Generate Ingestion Balance Report",
            "env": {
              "SUPABASE_URL": "${{ secrets.SUPABASE_URL }}",
              "SUPABASE_SERVICE_ROLE_KEY": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}"
            },
            "run": "if [ \"${{ github.event.inputs.format }}\" = \"json\" ]; then\n  npx tsx scripts/ingestion-balance-report.ts --format=json\nelse\n  npx tsx scripts/ingestion-balance-report.ts\nfi\n"
          },
          {
            "name": "Check Alert Thresholds",
            "env": {
              "SUPABASE_URL": "${{ secrets.SUPABASE_URL }}",
              "SUPABASE_SERVICE_ROLE_KEY": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}"
            },
            "run": "npx tsx scripts/ingestion-balance-report.ts --alert-thresholds",
            "continue-on-error": true,
            "id": "alerts"
          },
          {
            "name": "Create Issue on Alerts",
            "if": "steps.alerts.outcome == 'failure'",
            "uses": "actions/github-script@v7",
            "with": {
              "script": "github.rest.issues.create({\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  title: 'Ingestion Balance Alert - ' + new Date().toISOString().split('T')[0],\n  body: 'Automated ingestion balance monitoring detected issues. Check the workflow logs for details.',\n  labels: ['ingestion', 'alert', 'automated']\n})\n"
            }
          }
        ]
      }
    },
    "intent": "General workflow (purpose unclear from name)",
    "category": "maintenance"
  },
  {
    "filename": "daily-report.yml",
    "name": "Daily Summary Report",
    "on": {
      "schedule": [
        {
          "cron": "0 0 * * *"
        }
      ],
      "workflow_dispatch": null
    },
    "jobs": {
      "generate-report": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Generate Daily Report",
            "run": "echo \"üìä Generating daily summary report...\"\n\n# Get comprehensive stats from schedule endpoint\nSTATS=$(curl -s \"${{ secrets.SITE_URL }}/api/admin/schedule\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Accept: application/json\")\n\nif [ $? -ne 0 ]; then\n  echo \"‚ùå Failed to fetch daily stats\"\n  exit 1\nfi\n\n# Extract key metrics\nPOSTS_TODAY=$(echo \"$STATS\" | jq -r '.stats.todaysPosts // 0')\nQUEUE_SIZE=$(echo \"$STATS\" | jq -r '.queueStatus.totalPosted // 0')\nAPPROVED=$(echo \"$STATS\" | jq -r '.queueStatus.totalApproved // 0')\nDAYS_LEFT=$(echo \"scale=1; $APPROVED / 6\" | bc -l)\n\n# Get posting schedule status\nSCHEDULE=$(curl -s \"${{ secrets.SITE_URL }}/api/admin/schedule\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\")\n\nMISSED_POSTS=$(echo \"$SCHEDULE\" | jq -r '[.schedule.todaysSchedule[] | select(.posted == false and (.time | strptime(\"%H:%M\") | mktime) < now)] | length')\n\necho \"üìà Daily Report Generated:\"\necho \"  Posts Today: $POSTS_TODAY\"\necho \"  Missed Posts: $MISSED_POSTS\"\necho \"  Queue Size: $QUEUE_SIZE\"\necho \"  Approved: $APPROVED\"\necho \"  Days of Content: $DAYS_LEFT\"\n\n# Save report to file\ncat << EOF > daily-report.md\n# Hotdog Diaries Daily Report\n**Date:** $(date -u +\"%Y-%m-%d\")\n\n## üìä Performance Metrics\n- **Posts Today:** $POSTS_TODAY / 6 expected\n- **Missed Posts:** $MISSED_POSTS\n- **Success Rate:** $(echo \"scale=1; $POSTS_TODAY * 100 / 6\" | bc -l)%\n\n## üì¶ Content Queue Status\n- **Total Items:** $QUEUE_SIZE\n- **Approved & Ready:** $APPROVED\n- **Days of Content:** $DAYS_LEFT days\n- **Health Status:** $([ $(echo \"$DAYS_LEFT > 3\" | bc -l) -eq 1 ] && echo \"‚úÖ Healthy\" || echo \"‚ö†Ô∏è Low\")\n\n## üîß Recommended Actions\n$(if [ $MISSED_POSTS -gt 0 ]; then echo \"- üö® **URGENT:** Run catch-up posting for $MISSED_POSTS missed meals\"; fi)\n$(if [ $(echo \"$DAYS_LEFT < 3\" | bc -l) -eq 1 ]; then echo \"- üì° **HIGH PRIORITY:** Emergency content scanning needed\"; fi)\n$(if [ $(echo \"$DAYS_LEFT < 7\" | bc -l) -eq 1 ]; then echo \"- üîç **NORMAL:** Schedule regular content scanning\"; fi)\n\n---\n*Generated by GitHub Actions at $(date -u)*\nEOF\n\ncat daily-report.md\n"
          },
          {
            "name": "Check for Critical Issues",
            "run": "echo \"üö® Checking for critical issues...\"\n\nSTATS=$(curl -s \"${{ secrets.SITE_URL }}/api/admin/schedule\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\")\n\nAPPROVED=$(echo \"$STATS\" | jq -r '.queueStatus.totalApproved // 0')\nDAYS_LEFT=$(echo \"scale=1; $APPROVED / 6\" | bc -l)\n\n# Check if we need immediate action\nif [ $(echo \"$DAYS_LEFT < 1\" | bc -l) -eq 1 ]; then\n  echo \"üö® CRITICAL ALERT: Less than 1 day of content remaining!\"\n  echo \"Triggering emergency scan workflow...\"\n  \n  # Could trigger emergency scan here\n  curl -X POST \\\n    -H \"Authorization: token ${{ secrets.GITHUB_TOKEN }}\" \\\n    -H \"Accept: application/vnd.github.v3+json\" \\\n    \"https://api.github.com/repos/${{ github.repository }}/actions/workflows/queue-monitor.yml/dispatches\" \\\n    -d '{\"ref\":\"main\"}'\n  \nelif [ $(echo \"$DAYS_LEFT < 3\" | bc -l) -eq 1 ]; then\n  echo \"‚ö†Ô∏è WARNING: Less than 3 days of content remaining\"\n  \nelse\n  echo \"‚úÖ Queue status is acceptable ($DAYS_LEFT days)\"\nfi\n"
          },
          {
            "name": "Archive Report",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "daily-report-${{ github.run_number }}",
              "path": "daily-report.md",
              "retention-days": 30
            }
          }
        ]
      }
    },
    "intent": "General workflow (purpose unclear from name)",
    "category": "maintenance"
  },
  {
    "filename": "deploy-gate.yml",
    "name": "Deploy Gate",
    "on": {
      "deployment_status": null,
      "push": {
        "branches": [
          "main"
        ]
      },
      "workflow_run": {
        "workflows": [
          "Vercel Production Deployment"
        ],
        "types": [
          "completed"
        ]
      }
    },
    "permissions": {
      "contents": "read",
      "issues": "write"
    },
    "env": {
      "PROD_URL": "https://hotdog-diaries.vercel.app"
    },
    "jobs": {
      "auth-token-validation": {
        "runs-on": "ubuntu-latest",
        "name": "Validate AUTH_TOKEN Deploy Gate",
        "if": "github.event_name == 'push' || (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success')",
        "steps": [
          {
            "name": "Wait for deployment",
            "if": "github.event_name == 'push'",
            "run": "echo \"‚è≥ Waiting for deployment to be available...\"\nsleep 30\n"
          },
          {
            "name": "Test health probe with valid token",
            "run": "echo \"üîç Testing auth token health probe...\"\n\nRESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  \"${{ env.PROD_URL }}/api/admin/health/auth-token\")\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//g')\n\necho \"HTTP Status: $HTTP_STATUS\"\necho \"Response Body: $BODY\"\n\nif [ \"$HTTP_STATUS\" -eq 200 ]; then\n  echo \"‚úÖ Auth token validation passed\"\n  \n  # Verify response contains success code\n  if echo \"$BODY\" | jq -e '.code == \"AUTH_TOKEN_VALID\"' > /dev/null; then\n    echo \"‚úÖ Response contains valid success code\"\n  else\n    echo \"‚ùå Response missing valid success code\"\n    exit 1\n  fi\nelse\n  echo \"‚ùå Auth token validation failed with status $HTTP_STATUS\"\n  \n  # Check if it's an auth mismatch\n  if [ \"$HTTP_STATUS\" -eq 401 ]; then\n    CODE=$(echo \"$BODY\" | jq -r '.code // \"unknown\"')\n    if [ \"$CODE\" = \"AUTH_TOKEN_MISMATCH\" ]; then\n      echo \"üö® AUTH_TOKEN_MISMATCH detected - deployment blocked!\"\n      echo \"The AUTH_TOKEN secret may be incorrect or expired.\"\n    fi\n  fi\n  \n  exit 1\nfi\n"
          },
          {
            "name": "Test health probe with invalid token",
            "run": "echo \"üß™ Testing auth token health probe with invalid token...\"\n\nRESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -H \"Authorization: Bearer invalid-token-12345\" \\\n  \"${{ env.PROD_URL }}/api/admin/health/auth-token\")\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//g')\n\necho \"HTTP Status: $HTTP_STATUS\"\necho \"Response Body: $BODY\"\n\nif [ \"$HTTP_STATUS\" -eq 401 ]; then\n  echo \"‚úÖ Invalid token correctly rejected\"\n  \n  # Verify response contains mismatch code\n  CODE=$(echo \"$BODY\" | jq -r '.code // \"unknown\"')\n  if [ \"$CODE\" = \"AUTH_TOKEN_MISMATCH\" ]; then\n    echo \"‚úÖ Response contains correct error code: AUTH_TOKEN_MISMATCH\"\n  else\n    echo \"‚ö†Ô∏è Response contains unexpected error code: $CODE\"\n  fi\nelse\n  echo \"‚ùå Invalid token should have been rejected with 401\"\n  exit 1\nfi\n"
          },
          {
            "name": "Test health probe without token",
            "run": "echo \"üß™ Testing auth token health probe without token...\"\n\nRESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  \"${{ env.PROD_URL }}/api/admin/health/auth-token\")\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//g')\n\necho \"HTTP Status: $HTTP_STATUS\"\necho \"Response Body: $BODY\"\n\nif [ \"$HTTP_STATUS\" -eq 401 ]; then\n  echo \"‚úÖ Missing token correctly rejected\"\n  \n  # Verify response contains missing token code\n  CODE=$(echo \"$BODY\" | jq -r '.code // \"unknown\"')\n  if [ \"$CODE\" = \"AUTH_TOKEN_MISSING\" ]; then\n    echo \"‚úÖ Response contains correct error code: AUTH_TOKEN_MISSING\"\n  else\n    echo \"‚ö†Ô∏è Response contains unexpected error code: $CODE\"\n  fi\nelse\n  echo \"‚ùå Missing token should have been rejected with 401\"\n  exit 1\nfi\n"
          }
        ]
      },
      "comprehensive-health-check": {
        "runs-on": "ubuntu-latest",
        "name": "Comprehensive Health Validation",
        "needs": "auth-token-validation",
        "steps": [
          {
            "name": "Deep health check",
            "run": "echo \"üè• Running comprehensive health validation...\"\n\nRESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  \"${{ env.PROD_URL }}/api/admin/health/deep\")\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//g')\n\necho \"HTTP Status: $HTTP_STATUS\"\n\nif [ \"$HTTP_STATUS\" -eq 200 ]; then\n  echo \"‚úÖ Deep health check passed\"\n  \n  # Verify response structure\n  OK_STATUS=$(echo \"$BODY\" | jq -r '.ok // false')\n  if [ \"$OK_STATUS\" = \"true\" ]; then\n    echo \"‚úÖ Health status is OK\"\n    \n    # Show component health\n    echo \"üìä Component Health:\"\n    echo \"$BODY\" | jq '.components // {}' || echo \"No component details available\"\n    \n  else\n    echo \"‚ùå Health status indicates issues\"\n    echo \"$BODY\" | jq '.components // {}' || echo \"No component details available\"\n    exit 1\n  fi\nelse\n  echo \"‚ùå Deep health check failed with status $HTTP_STATUS\"\n  echo \"Response: $BODY\"\n  exit 1\nfi\n"
          },
          {
            "name": "Admin endpoint smoke test",
            "run": "echo \"üí® Running admin endpoint smoke test...\"\n\n# Test a few critical admin endpoints\nENDPOINTS=(\n  \"/api/admin/dashboard/stats\"\n  \"/api/admin/queue/health\"\n  \"/api/admin/platforms/status\"\n)\n\nfor ENDPOINT in \"${ENDPOINTS[@]}\"; do\n  echo \"Testing $ENDPOINT...\"\n  \n  RESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n    -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n    \"${{ env.PROD_URL }}$ENDPOINT\")\n  \n  HTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\n  \n  if [ \"$HTTP_STATUS\" -eq 200 ]; then\n    echo \"‚úÖ $ENDPOINT responding\"\n  else\n    echo \"‚ùå $ENDPOINT failed with status $HTTP_STATUS\"\n    # Don't fail the entire gate for individual endpoint issues\n    # Just log them for monitoring\n  fi\ndone\n"
          }
        ]
      },
      "deployment-gate-result": {
        "runs-on": "ubuntu-latest",
        "name": "Deployment Gate Result",
        "needs": [
          "auth-token-validation",
          "comprehensive-health-check"
        ],
        "if": "always()",
        "steps": [
          {
            "name": "Check gate results",
            "run": "echo \"üö™ Deployment Gate Results\"\necho \"==========================\"\n\nAUTH_RESULT=\"${{ needs.auth-token-validation.result }}\"\nHEALTH_RESULT=\"${{ needs.comprehensive-health-check.result }}\"\n\necho \"Auth Token Validation: $AUTH_RESULT\"\necho \"Health Check: $HEALTH_RESULT\"\n\nif [ \"$AUTH_RESULT\" = \"success\" ] && [ \"$HEALTH_RESULT\" = \"success\" ]; then\n  echo \"\"\n  echo \"‚úÖ DEPLOYMENT GATE PASSED\"\n  echo \"All security and health validations successful.\"\n  echo \"Deployment is approved for production traffic.\"\nelse\n  echo \"\"\n  echo \"‚ùå DEPLOYMENT GATE FAILED\"\n  echo \"Security or health validations failed.\"\n  echo \"Deployment should be rolled back or investigated.\"\n  exit 1\nfi\n"
          },
          {
            "name": "‚úÖ Deployment Success",
            "if": "needs.auth-token-validation.result == 'success' && needs.comprehensive-health-check.result == 'success'",
            "run": "echo \"üéâ DEPLOYMENT GATE PASSED\"\necho \"=========================\"\necho \"‚úÖ Auth Token Validation: Success\"\necho \"‚úÖ Comprehensive Health Check: Success\"\necho \"‚úÖ Admin Endpoint Smoke Test: Success\"\necho \"\"\necho \"üöÄ Deployment approved for production traffic\"\necho \"üåê Production URL: ${{ env.PROD_URL }}\"\necho \"üè• Health Probe: ${{ env.PROD_URL }}/api/admin/health/auth-token\"\necho \"\"\necho \"## üéâ Deployment Gate Success\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Deployment**: \\`${{ github.sha }}\\`\" >> $GITHUB_STEP_SUMMARY\necho \"**Timestamp**: $(date -u -Iseconds)\" >> $GITHUB_STEP_SUMMARY\necho \"**Environment**: Production\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"### ‚úÖ Validation Results\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"- ‚úÖ **Auth Token Validation**: Passed\" >> $GITHUB_STEP_SUMMARY\necho \"- ‚úÖ **Comprehensive Health Check**: Passed\" >> $GITHUB_STEP_SUMMARY\necho \"- ‚úÖ **Admin Endpoint Smoke Test**: Passed\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"üöÄ All security and health validations passed - deployment approved!\" >> $GITHUB_STEP_SUMMARY\n"
          },
          {
            "name": "Create deployment failure issue",
            "if": "needs.auth-token-validation.result != 'success' || needs.comprehensive-health-check.result != 'success'",
            "uses": "actions/github-script@v7",
            "with": {
              "script": "const title = `üö® Deployment Gate Failed - ${new Date().toISOString().split('T')[0]}`\nconst body = `## Deployment Gate Failure\n\n**Deployment**: ${{ github.sha }}  \n**Timestamp**: ${new Date().toISOString()}  \n**Environment**: Production  \n\n### Validation Results\n\n- ${{ needs.auth-token-validation.result == 'success' && '‚úÖ' || '‚ùå' }} **Auth Token Validation**: ${{ needs.auth-token-validation.result }}\n- ${{ needs.comprehensive-health-check.result == 'success' && '‚úÖ' || '‚ùå' }} **Comprehensive Health Check**: ${{ needs.comprehensive-health-check.result }}\n\n### Immediate Actions Required\n\n1. **Check workflow logs** for specific failure details\n2. **Verify AUTH_TOKEN secret** is correct and not expired\n3. **Test health endpoints** manually to isolate issues\n4. **Consider rollback** if critical functionality is impacted\n\n### Investigation\n\n- **Health Probe**: ${{ env.PROD_URL }}/api/admin/health/auth-token\n- **Deep Health**: ${{ env.PROD_URL }}/api/admin/health/deep\n- **Workflow**: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\n\n### Security Note\n\nIf the failure is due to AUTH_TOKEN_MISMATCH, this indicates a security configuration issue that must be resolved before deployment approval.\n\n/cc @security-team @devops-team\n\nThis alert was generated automatically by the deployment gate workflow.\n`\n\ngithub.rest.issues.create({\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  title: title,\n  body: body,\n  labels: ['deployment', 'failure', 'security', 'urgent', 'automated']\n})\n"
            }
          }
        ]
      }
    },
    "intent": "Validates deployment health and security gates",
    "category": "deploy"
  },
  {
    "filename": "deployment-gate.yml",
    "name": "üö™ Deployment Gate",
    "on": {
      "workflow_dispatch": {
        "inputs": {
          "target_url": {
            "description": "Deployment base URL (e.g. https://hotdog-diaries.vercel.app)",
            "required": false,
            "type": "string"
          }
        }
      },
      "deployment_status": null,
      "workflow_call": {
        "inputs": {
          "target_url": {
            "description": "Deployment base URL",
            "required": false,
            "type": "string"
          }
        },
        "secrets": {
          "AUTH_TOKEN": {
            "description": "Admin authentication token",
            "required": true
          }
        }
      }
    },
    "permissions": {
      "contents": "read"
    },
    "jobs": {
      "gate": {
        "name": "üîê Security & Health Gate",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 10,
        "if": "github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name == github.repository\n",
        "env": {
          "TARGET_URL": "${{ inputs.target_url || github.event.deployment_status.environment_url || vars.SITE_URL || secrets.SITE_URL }}",
          "AUTH_TOKEN": "${{ secrets.AUTH_TOKEN }}"
        },
        "steps": [
          {
            "name": "üß™ Preflight Validation",
            "shell": "bash",
            "run": "set -euo pipefail\necho \"üîç Validating deployment gate requirements...\"\necho \"Event: ${{ github.event_name }}\"\necho \"Environment URL: ${{ github.event.deployment_status.environment_url || 'none' }}\"\necho \"Target URL: ${TARGET_URL:-<empty>}\"\n\nmissing=0\nif [ -z \"${TARGET_URL:-}\" ]; then\n  echo \"::error title=Missing Target URL::No TARGET_URL available. Provide 'inputs.target_url' OR set repository variable 'SITE_URL' OR ensure 'deployment_status.environment_url' is present.\"\n  missing=1\nfi\n\nif [ -z \"${AUTH_TOKEN:-}\" ]; then\n  echo \"::error title=Missing Auth Token::AUTH_TOKEN secret not configured. Add repository secret AUTH_TOKEN with admin JWT token.\"\n  missing=1\nfi\n\nif [ \"$missing\" -ne 0 ]; then\n  echo \"‚ùå Preflight failed ‚Äî required context not available.\"\n  echo \"::error::Deployment gate cannot proceed without TARGET_URL and AUTH_TOKEN\"\n  exit 1\nfi\n\necho \"‚úÖ Preflight validation passed\"\necho \"üéØ Target: ${TARGET_URL}\"\n"
          },
          {
            "name": "üîê Authentication Validation",
            "id": "auth",
            "shell": "bash",
            "run": "set -euo pipefail\necho \"üîê Validating admin authentication token...\"\n\nresponse=$(curl -fsS --retry 3 --retry-delay 2 --retry-connrefused -w \"HTTP_CODE:%{http_code}\" \\\n  -H \"x-admin-token: ${AUTH_TOKEN}\" \\\n  -H \"Authorization: Bearer ${AUTH_TOKEN}\" \\\n  \"${TARGET_URL}/api/admin/health/auth-token\")\n\nhttp_code=$(echo \"$response\" | grep -o \"HTTP_CODE:[0-9]*\" | cut -d: -f2)\nbody=$(echo \"$response\" | sed 's/HTTP_CODE:[0-9]*$//')\n\nif [ \"$http_code\" != \"200\" ]; then\n  echo \"‚ùå Auth validation failed with HTTP $http_code\"\n  echo \"Response: $body\"\n  exit 1\nfi\n\necho \"‚úÖ Authentication token valid\"\necho \"$body\" | jq . || echo \"$body\"\n"
          },
          {
            "name": "üíö Deep Health Check",
            "id": "health",
            "shell": "bash",
            "run": "set -euo pipefail\necho \"üíö Running comprehensive system health check...\"\n\nresponse=$(curl -fsS --retry 3 --retry-delay 2 --retry-connrefused -w \"HTTP_CODE:%{http_code}\" \\\n  -H \"x-admin-token: ${AUTH_TOKEN}\" \\\n  -H \"Authorization: Bearer ${AUTH_TOKEN}\" \\\n  \"${TARGET_URL}/api/admin/health/deep\")\n\nhttp_code=$(echo \"$response\" | grep -o \"HTTP_CODE:[0-9]*\" | cut -d: -f2)\nbody=$(echo \"$response\" | sed 's/HTTP_CODE:[0-9]*$//')\n\nif [ \"$http_code\" != \"200\" ]; then\n  echo \"‚ùå Health check failed with HTTP $http_code\"\n  echo \"Response: $body\"\n  exit 1\nfi\n\necho \"‚úÖ Deep health check passed\"\necho \"$body\" | jq '{\n  database: .database.status,\n  apis: (.apis | length),\n  scheduler: .scheduler.status,\n  overall: .status\n}' || echo \"$body\"\n"
          },
          {
            "name": "‚úÖ Gate Summary",
            "if": "always()",
            "run": "echo \"## üö™ Deployment Gate Results\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"| Check | Status |\" >> $GITHUB_STEP_SUMMARY\necho \"|-------|--------|\" >> $GITHUB_STEP_SUMMARY\necho \"| Auth Token | ${{ steps.auth.outcome == 'success' && '‚úÖ Valid' || '‚ùå Failed' }} |\" >> $GITHUB_STEP_SUMMARY\necho \"| Health Check | ${{ steps.health.outcome == 'success' && '‚úÖ Healthy' || '‚ùå Failed' }} |\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Target:** \\`${TARGET_URL}\\`\" >> $GITHUB_STEP_SUMMARY\n\necho \"üö™ Deployment Gate Results\"\necho \"==========================\"\necho \"Auth Token Validation: ${{ steps.auth.outcome }}\"\necho \"Health Check:          ${{ steps.health.outcome }}\"\n"
          },
          {
            "name": "‚ùå Fail on Security or Health Issues",
            "if": "steps.auth.outcome != 'success' ||\nsteps.health.outcome != 'success'\n",
            "run": "echo \"‚ùå DEPLOYMENT GATE FAILED\"\necho \"Security or health validations failed - deployment blocked\"\necho \"::error title=Gate Failed::One or more security/health checks failed\"\nexit 1\n"
          },
          {
            "name": "üéâ Gate Success",
            "run": "echo \"üéâ DEPLOYMENT GATE PASSED\"\necho \"All security and health validations successful\"\necho \"Deployment is cleared to proceed\"\n"
          }
        ]
      }
    },
    "intent": "Validates deployment health and security gates",
    "category": "deploy"
  },
  {
    "filename": "housekeeping.yml",
    "name": "Housekeeping",
    "on": {
      "schedule": [
        {
          "cron": "0 3 * * 1"
        },
        {
          "cron": "0 6 * * *"
        }
      ],
      "workflow_dispatch": {
        "inputs": {
          "tasks": {
            "description": "Tasks to run (comma-separated: cleanup, dead-links, licenses, audit, queue-monitor, secrets)",
            "type": "string",
            "default": "all"
          },
          "force-cleanup": {
            "description": "Force aggressive cleanup",
            "type": "boolean",
            "default": false
          }
        }
      },
      "workflow_call": {
        "inputs": {
          "tasks": {
            "description": "Tasks to run",
            "type": "string",
            "default": "all"
          },
          "force-cleanup": {
            "description": "Force aggressive cleanup",
            "type": "boolean",
            "default": false
          }
        }
      }
    },
    "concurrency": {
      "group": "housekeeping-${{ github.ref }}",
      "cancel-in-progress": true
    },
    "env": {
      "NODE_ENV": "production",
      "CI": true
    },
    "jobs": {
      "determine-tasks": {
        "name": "Determine Tasks",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 2,
        "outputs": {
          "tasks": "${{ steps.strategy.outputs.tasks }}",
          "schedule-type": "${{ steps.strategy.outputs.schedule-type }}",
          "force-cleanup": "${{ steps.strategy.outputs.force-cleanup }}"
        },
        "steps": [
          {
            "name": "Determine housekeeping strategy",
            "id": "strategy",
            "run": "TASKS=\"${{ inputs.tasks || 'all' }}\"\nFORCE_CLEANUP=\"${{ inputs.force-cleanup || 'false' }}\"\nSCHEDULE_TYPE=\"manual\"\n\n# Determine tasks based on schedule\nif [[ \"${{ github.event_name }}\" == \"schedule\" ]]; then\n  SCHEDULE_TYPE=\"scheduled\"\n  HOUR=$(date +%H)\n  DAY=$(date +%u)  # 1=Monday, 7=Sunday\n  \n  if [[ $HOUR -eq 3 && $DAY -eq 1 ]]; then\n    # Weekly comprehensive housekeeping\n    TASKS=\"cleanup,dead-links,licenses,audit,queue-monitor,secrets\"\n    FORCE_CLEANUP=\"true\"\n    echo \"üè† Weekly comprehensive housekeeping\"\n  elif [[ $HOUR -eq 6 ]]; then\n    # Daily light housekeeping\n    TASKS=\"cleanup,queue-monitor\"\n    FORCE_CLEANUP=\"false\"\n    echo \"üßπ Daily light housekeeping\"\n  fi\nfi\n\nif [[ \"$TASKS\" == \"all\" ]]; then\n  TASKS=\"cleanup,dead-links,licenses,audit,queue-monitor,secrets\"\nfi\n\necho \"tasks=$TASKS\" >> $GITHUB_OUTPUT\necho \"schedule-type=$SCHEDULE_TYPE\" >> $GITHUB_OUTPUT\necho \"force-cleanup=$FORCE_CLEANUP\" >> $GITHUB_OUTPUT\necho \"Selected tasks: $TASKS (force: $FORCE_CLEANUP)\"\n"
          }
        ]
      },
      "cleanup-duplicates": {
        "name": "Cleanup Duplicates",
        "runs-on": "ubuntu-latest",
        "needs": "determine-tasks",
        "if": "contains(needs.determine-tasks.outputs.tasks, 'cleanup')",
        "timeout-minutes": 10,
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "20",
              "cache-key-suffix": "housekeeping-cleanup"
            }
          },
          {
            "name": "Setup Supabase environment",
            "uses": "./.github/actions/setup-supabase-rest",
            "with": {
              "supabase-url": "${{ secrets.SUPABASE_URL }}",
              "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}",
              "database-url": "${{ secrets.DATABASE_URL }}"
            }
          },
          {
            "name": "Clean up duplicate content",
            "run": "echo \"üßπ Cleaning up duplicate content...\"\n\nAUTH_TOKEN=\"${{ secrets.AUTH_TOKEN }}\"\nFORCE_CLEANUP=\"${{ needs.determine-tasks.outputs.force-cleanup }}\"\n\n# Call the cleanup API endpoint\nRESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -X POST \\\n  -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\\\"force\\\": $FORCE_CLEANUP}\" \\\n  \"${{ secrets.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/admin/cleanup/duplicates\" \\\n  --max-time 300)\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//')\n\nif [ \"$HTTP_STATUS\" -eq 200 ]; then\n  echo \"‚úÖ Duplicate cleanup completed\"\n  echo \"$BODY\" > cleanup-duplicates-report.json\nelse\n  echo \"‚ö†Ô∏è Duplicate cleanup failed with status $HTTP_STATUS\"\n  echo \"Response: $BODY\"\nfi\n"
          },
          {
            "name": "Upload cleanup report",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "cleanup-duplicates-report",
              "path": "cleanup-duplicates-report.json",
              "retention-days": 7,
              "if-no-files-found": "ignore"
            }
          }
        ]
      },
      "dead-links-check": {
        "name": "Dead Links Check",
        "runs-on": "ubuntu-latest",
        "needs": "determine-tasks",
        "if": "contains(needs.determine-tasks.outputs.tasks, 'dead-links')",
        "timeout-minutes": 15,
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "20",
              "cache-key-suffix": "housekeeping-deadlinks"
            }
          },
          {
            "name": "Setup Supabase environment",
            "uses": "./.github/actions/setup-supabase-rest",
            "with": {
              "supabase-url": "${{ secrets.SUPABASE_URL }}",
              "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}",
              "database-url": "${{ secrets.DATABASE_URL }}"
            }
          },
          {
            "name": "Check for dead links",
            "run": "echo \"üîó Checking for dead links in content...\"\n\n# Run dead link detection script\nnpm run check:dead-links || echo \"Dead link check completed with warnings\"\n\n# Count and report results\nif [ -f \"reports/dead-links.json\" ]; then\n  DEAD_COUNT=$(jq '.deadLinks | length' reports/dead-links.json 2>/dev/null || echo \"0\")\n  echo \"Found $DEAD_COUNT dead links\"\n  \n  if [ \"$DEAD_COUNT\" -gt 0 ]; then\n    echo \"‚ö†Ô∏è Dead links detected - content may need review\"\n  else\n    echo \"‚úÖ No dead links found\"\n  fi\nfi\n"
          },
          {
            "name": "Upload dead links report",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "dead-links-report",
              "path": "reports/dead-links.json\nreports/dead-links.md\n",
              "retention-days": 14,
              "if-no-files-found": "ignore"
            }
          }
        ]
      },
      "license-audit": {
        "name": "License Audit",
        "runs-on": "ubuntu-latest",
        "needs": "determine-tasks",
        "if": "contains(needs.determine-tasks.outputs.tasks, 'licenses')",
        "timeout-minutes": 8,
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "20",
              "cache-key-suffix": "housekeeping-licenses"
            }
          },
          {
            "name": "Audit licenses",
            "run": "echo \"üìú Auditing dependency licenses...\"\n\n# Generate license report\nnpm run licenses:check || echo \"License check completed\"\n\n# Check for problematic licenses\nif [ -f \"reports/licenses.json\" ]; then\n  PROBLEMATIC=$(jq '.problematic | length' reports/licenses.json 2>/dev/null || echo \"0\")\n  echo \"Found $PROBLEMATIC potentially problematic licenses\"\n  \n  if [ \"$PROBLEMATIC\" -gt 0 ]; then\n    echo \"‚ö†Ô∏è License issues detected - manual review required\"\n  else\n    echo \"‚úÖ All licenses appear compatible\"\n  fi\nfi\n"
          },
          {
            "name": "Upload license report",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "license-audit-report",
              "path": "reports/licenses.json\nreports/licenses.md\n",
              "retention-days": 30,
              "if-no-files-found": "ignore"
            }
          }
        ]
      },
      "security-audit": {
        "name": "Security Audit",
        "runs-on": "ubuntu-latest",
        "needs": "determine-tasks",
        "if": "contains(needs.determine-tasks.outputs.tasks, 'audit')",
        "timeout-minutes": 10,
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "20",
              "cache-key-suffix": "housekeeping-security"
            }
          },
          {
            "name": "Run security audit",
            "run": "echo \"üîí Running security audit...\"\n\n# Run npm audit\nnpm audit --audit-level=moderate > security-audit.txt 2>&1 || echo \"Audit completed with findings\"\n\n# Count vulnerabilities\nHIGH_VULNS=$(grep -c \"high\" security-audit.txt || echo \"0\")\nCRITICAL_VULNS=$(grep -c \"critical\" security-audit.txt || echo \"0\")\n\necho \"Found $CRITICAL_VULNS critical and $HIGH_VULNS high severity vulnerabilities\"\n\nif [ \"$CRITICAL_VULNS\" -gt 0 ]; then\n  echo \"‚ùå Critical vulnerabilities detected - immediate action required\"\n  exit 1\nelif [ \"$HIGH_VULNS\" -gt 0 ]; then\n  echo \"‚ö†Ô∏è High severity vulnerabilities detected - review recommended\"\nelse\n  echo \"‚úÖ No critical or high severity vulnerabilities\"\nfi\n"
          },
          {
            "name": "Upload security audit report",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "security-audit-report",
              "path": "security-audit.txt",
              "retention-days": 14
            }
          }
        ]
      },
      "queue-monitor": {
        "name": "Queue Monitor",
        "runs-on": "ubuntu-latest",
        "needs": "determine-tasks",
        "if": "contains(needs.determine-tasks.outputs.tasks, 'queue-monitor')",
        "timeout-minutes": 5,
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "20",
              "cache-key-suffix": "housekeeping-queue"
            }
          },
          {
            "name": "Setup Supabase environment",
            "uses": "./.github/actions/setup-supabase-rest",
            "with": {
              "supabase-url": "${{ secrets.SUPABASE_URL }}",
              "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}",
              "database-url": "${{ secrets.DATABASE_URL }}"
            }
          },
          {
            "name": "Monitor content queue health",
            "run": "echo \"üìä Monitoring content queue health...\"\n\nAUTH_TOKEN=\"${{ secrets.AUTH_TOKEN }}\"\n\n# Get queue metrics\nRESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n  \"${{ secrets.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/admin/metrics\" \\\n  --max-time 30)\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//')\n\nif [ \"$HTTP_STATUS\" -eq 200 ]; then\n  echo \"‚úÖ Queue monitoring completed\"\n  echo \"$BODY\" > queue-health.json\n  \n  # Extract key metrics if possible\n  if command -v jq >/dev/null 2>&1; then\n    TOTAL_CONTENT=$(echo \"$BODY\" | jq '.totalContent // 0' 2>/dev/null || echo \"unknown\")\n    APPROVED_CONTENT=$(echo \"$BODY\" | jq '.approvedContent // 0' 2>/dev/null || echo \"unknown\")\n    echo \"üìà Queue stats: $TOTAL_CONTENT total, $APPROVED_CONTENT approved\"\n  fi\nelse\n  echo \"‚ö†Ô∏è Queue monitoring failed with status $HTTP_STATUS\"\n  echo \"Response: $BODY\"\nfi\n"
          },
          {
            "name": "Upload queue health report",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "queue-health-report",
              "path": "queue-health.json",
              "retention-days": 7,
              "if-no-files-found": "ignore"
            }
          }
        ]
      },
      "secrets-validation": {
        "name": "Secrets Validation",
        "runs-on": "ubuntu-latest",
        "needs": "determine-tasks",
        "if": "contains(needs.determine-tasks.outputs.tasks, 'secrets')",
        "timeout-minutes": 8,
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "20",
              "cache-key-suffix": "housekeeping-secrets"
            }
          },
          {
            "name": "Validate secrets configuration",
            "run": "echo \"üîê Validating secrets configuration...\"\n\n# Check essential secrets\nMISSING_SECRETS=\"\"\n\nif [[ -z \"${{ secrets.SUPABASE_URL }}\" ]]; then\n  MISSING_SECRETS=\"$MISSING_SECRETS SUPABASE_URL\"\nfi\n\nif [[ -z \"${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}\" ]]; then\n  MISSING_SECRETS=\"$MISSING_SECRETS SUPABASE_SERVICE_ROLE_KEY\"\nfi\n\nif [[ -z \"${{ secrets.AUTH_TOKEN }}\" ]]; then\n  MISSING_SECRETS=\"$MISSING_SECRETS AUTH_TOKEN\"\nfi\n\nif [[ -z \"${{ secrets.DATABASE_URL }}\" ]]; then\n  MISSING_SECRETS=\"$MISSING_SECRETS DATABASE_URL\"\nfi\n\n# Report results\nif [[ -n \"$MISSING_SECRETS\" ]]; then\n  echo \"‚ùå Missing essential secrets:$MISSING_SECRETS\"\n  echo \"missing-secrets=$MISSING_SECRETS\" >> $GITHUB_OUTPUT\n  exit 1\nelse\n  echo \"‚úÖ All essential secrets are configured\"\nfi\n\n# Check API keys\nAPI_KEYS=\"\"\nif [[ -n \"${{ secrets.REDDIT_CLIENT_ID }}\" ]]; then\n  API_KEYS=\"$API_KEYS reddit\"\nfi\nif [[ -n \"${{ secrets.YOUTUBE_API_KEY }}\" ]]; then\n  API_KEYS=\"$API_KEYS youtube\"\nfi\nif [[ -n \"${{ secrets.GIPHY_API_KEY }}\" ]]; then\n  API_KEYS=\"$API_KEYS giphy\"\nfi\n\necho \"üìä Configured API integrations:$API_KEYS\"\n"
          }
        ]
      },
      "summary": {
        "name": "Housekeeping Summary",
        "runs-on": "ubuntu-latest",
        "needs": [
          "determine-tasks",
          "cleanup-duplicates",
          "dead-links-check",
          "license-audit",
          "security-audit",
          "queue-monitor",
          "secrets-validation"
        ],
        "if": "always()",
        "steps": [
          {
            "name": "Generate housekeeping summary",
            "run": "echo \"## üè† Housekeeping Results\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Tasks:** ${{ needs.determine-tasks.outputs.tasks }}\" >> $GITHUB_STEP_SUMMARY\necho \"**Schedule Type:** ${{ needs.determine-tasks.outputs.schedule-type }}\" >> $GITHUB_STEP_SUMMARY\necho \"**Force Cleanup:** ${{ needs.determine-tasks.outputs.force-cleanup }}\" >> $GITHUB_STEP_SUMMARY\necho \"**Trigger:** ${{ github.event_name }}\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"| Task | Status | Notes |\" >> $GITHUB_STEP_SUMMARY\necho \"|------|--------|-------|\" >> $GITHUB_STEP_SUMMARY\n\n# Add task statuses\nif [[ \"${{ needs.cleanup-duplicates.result }}\" != \"\" ]]; then\n  echo \"| Cleanup Duplicates | ${{ needs.cleanup-duplicates.result }} | Content deduplication |\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [[ \"${{ needs.dead-links-check.result }}\" != \"\" ]]; then\n  echo \"| Dead Links Check | ${{ needs.dead-links-check.result }} | URL validation |\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [[ \"${{ needs.license-audit.result }}\" != \"\" ]]; then\n  echo \"| License Audit | ${{ needs.license-audit.result }} | Dependency licensing |\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [[ \"${{ needs.security-audit.result }}\" != \"\" ]]; then\n  echo \"| Security Audit | ${{ needs.security-audit.result }} | Vulnerability scanning |\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [[ \"${{ needs.queue-monitor.result }}\" != \"\" ]]; then\n  echo \"| Queue Monitor | ${{ needs.queue-monitor.result }} | Content queue health |\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [[ \"${{ needs.secrets-validation.result }}\" != \"\" ]]; then\n  echo \"| Secrets Validation | ${{ needs.secrets-validation.result }} | Configuration check |\" >> $GITHUB_STEP_SUMMARY\nfi\n\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Overall status\nFAILED_TASKS=0\n\nif [[ \"${{ needs.cleanup-duplicates.result }}\" == \"failure\" ]]; then\n  ((FAILED_TASKS++))\nfi\nif [[ \"${{ needs.dead-links-check.result }}\" == \"failure\" ]]; then\n  ((FAILED_TASKS++))\nfi\nif [[ \"${{ needs.license-audit.result }}\" == \"failure\" ]]; then\n  ((FAILED_TASKS++))\nfi\nif [[ \"${{ needs.security-audit.result }}\" == \"failure\" ]]; then\n  ((FAILED_TASKS++))\nfi\nif [[ \"${{ needs.queue-monitor.result }}\" == \"failure\" ]]; then\n  ((FAILED_TASKS++))\nfi\nif [[ \"${{ needs.secrets-validation.result }}\" == \"failure\" ]]; then\n  ((FAILED_TASKS++))\nfi\n\nif [[ $FAILED_TASKS -eq 0 ]]; then\n  echo \"## ‚úÖ All housekeeping tasks completed successfully\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"## ‚ö†Ô∏è $FAILED_TASKS housekeeping task(s) failed\" >> $GITHUB_STEP_SUMMARY\n  echo \"Check individual task logs for details.\" >> $GITHUB_STEP_SUMMARY\nfi\n\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"üìà **Next Steps:**\" >> $GITHUB_STEP_SUMMARY\necho \"- Review artifact reports for detailed findings\" >> $GITHUB_STEP_SUMMARY\necho \"- Address any security or license issues found\" >> $GITHUB_STEP_SUMMARY\necho \"- Monitor content queue health metrics\" >> $GITHUB_STEP_SUMMARY\n"
          }
        ]
      }
    },
    "intent": "General workflow (purpose unclear from name)",
    "category": "maintenance"
  },
  {
    "filename": "manual-operations.yml",
    "name": "Manual Operations",
    "on": {
      "workflow_dispatch": {
        "inputs": {
          "operation": {
            "description": "Operation to perform",
            "required": true,
            "type": "choice",
            "options": [
              "post-now",
              "scan-all-emergency",
              "scan-all-normal",
              "catch-up-missed-posts",
              "approve-pending",
              "system-health-check",
              "clear-queue-test",
              "fix-duplicates"
            ]
          },
          "count": {
            "description": "Number of times (for catch-up posts)",
            "required": false,
            "default": "1",
            "type": "string"
          },
          "platform": {
            "description": "Specific platform (for single platform scans)",
            "required": false,
            "type": "choice",
            "options": [
              "all",
              "reddit",
              "youtube",
              "giphy",
              "pixabay",
              "bluesky",
              "imgur",
              "lemmy",
              "tumblr"
            ]
          }
        }
      }
    },
    "jobs": {
      "execute-operation": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Execute Operation",
            "run": "case \"${{ github.event.inputs.operation }}\" in\n  \"post-now\")\n    echo \"üöÄ Posting content immediately...\"\n    curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/posting/post-now\" \\\n      -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\"immediate\": true}' \\\n      --fail --show-error\n    ;;\n    \n  \"catch-up-missed-posts\")\n    echo \"üìÖ Catching up ${{ github.event.inputs.count }} missed posts...\"\n    for i in $(seq 1 ${{ github.event.inputs.count }}); do\n      echo \"  Posting #$i...\"\n      curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/posting/post-now\" \\\n        -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n        -H \"Content-Type: application/json\" \\\n        -d \"{\\\"catchUp\\\": true, \\\"sequence\\\": $i}\" \\\n        --fail --show-error\n      \n      if [ $i -lt ${{ github.event.inputs.count }} ]; then\n        echo \"  Waiting 10 seconds before next post...\"\n        sleep 10\n      fi\n    done\n    ;;\n    \n  \"scan-all-emergency\")\n    echo \"üö® EMERGENCY: Scanning all platforms with auto-approval...\"\n    curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/content/emergency-scan\" \\\n      -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\"autoApprove\": true, \"maxItems\": 100}' \\\n      --fail --show-error\n    ;;\n    \n  \"scan-all-normal\")\n    echo \"üì° Scanning all platforms (normal mode)...\"\n    curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/content/scan-all\" \\\n      -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\"maxItems\": 50}' \\\n      --fail --show-error\n    ;;\n    \n  \"approve-pending\")\n    echo \"‚úÖ Auto-approving pending content...\"\n    curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/content/auto-approve\" \\\n      -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\"maxItems\": 30, \"onlyHighConfidence\": true}' \\\n      --fail --show-error\n    ;;\n    \n  \"system-health-check\")\n    echo \"üè• Running system health check...\"\n    curl -L -X GET \"${{ secrets.SITE_URL }}/api/admin/health\" \\\n      -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n      --fail --show-error\n    ;;\n    \n  \"clear-queue-test\")\n    echo \"üßπ Clearing test/duplicate content...\"\n    curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/content/cleanup\" \\\n      -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\"removeTestContent\": true, \"removeDuplicates\": true}' \\\n      --fail --show-error\n    ;;\n    \n  \"fix-duplicates\")\n    echo \"üîß Fixing duplicate content to prevent reposting...\"\n    curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/fix-duplicate-content\" \\\n      -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n      -H \"Content-Type: application/json\" \\\n      --fail --show-error\n    ;;\n    \n  *)\n    echo \"‚ùå Unknown operation: ${{ github.event.inputs.operation }}\"\n    exit 1\n    ;;\nesac\n"
          },
          {
            "name": "Report Operation Results",
            "if": "always()",
            "run": "echo \"üìä Operation completed: ${{ github.event.inputs.operation }}\"\nif [ \"${{ github.event.inputs.count }}\" != \"1\" ]; then\n  echo \"Count: ${{ github.event.inputs.count }}\"\nfi\nif [ \"${{ github.event.inputs.platform }}\" != \"all\" ]; then\n  echo \"Platform: ${{ github.event.inputs.platform }}\"\nfi\n"
          }
        ]
      }
    },
    "intent": "General workflow (purpose unclear from name)",
    "category": "maintenance"
  },
  {
    "filename": "meta-ci-audit.yml",
    "name": "Meta CI Audit",
    "on": {
      "schedule": [
        {
          "cron": "0 8 * * 1"
        }
      ],
      "workflow_dispatch": null
    },
    "jobs": {
      "audit-ci-health": {
        "name": "Audit CI Health",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 15,
        "steps": [
          {
            "name": "Checkout repository",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js",
            "uses": "actions/setup-node@v4",
            "with": {
              "node-version": "20",
              "cache": "npm"
            }
          },
          {
            "name": "Install dependencies",
            "run": "npm ci"
          },
          {
            "name": "üß† Install tsx (temporary CI dependency)",
            "run": "npm install --no-save tsx"
          },
          {
            "name": "Run CI health audit",
            "run": "npx tsx scripts/validateRefactorPlan.ts",
            "id": "audit"
          },
          {
            "name": "Generate health report",
            "run": "npx tsx scripts/auditWorkflows.ts",
            "env": {
              "GH_TOKEN": "${{ secrets.GITHUB_TOKEN }}"
            }
          },
          {
            "name": "Notify on failure",
            "if": "failure()",
            "uses": "slackapi/slack-github-action@v1.27.0",
            "with": {
              "payload": "{\n  \"text\": \"üö® Weekly CI health audit failed\",\n  \"blocks\": [\n    {\n      \"type\": \"section\",\n      \"text\": {\n        \"type\": \"mrkdwn\",\n        \"text\": \"*CI Health Alert* üè•\\n\\nWeekly audit detected issues in workflow configuration.\\n\\n*Repository:* ${{ github.repository }}\\n*Run:* <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Details>\"\n      }\n    }\n  ]\n}\n"
            },
            "env": {
              "SLACK_WEBHOOK_URL": "${{ secrets.SLACK_WEBHOOK_URL }}"
            }
          },
          {
            "name": "Post success summary",
            "if": "success()",
            "uses": "slackapi/slack-github-action@v1.27.0",
            "with": {
              "payload": "{\n  \"text\": \"‚úÖ Weekly CI health audit passed - all workflows healthy!\"\n}\n"
            },
            "env": {
              "SLACK_WEBHOOK_URL": "${{ secrets.SLACK_WEBHOOK_URL }}"
            }
          }
        ]
      }
    },
    "intent": "Runs automated tests and builds",
    "category": "test"
  },
  {
    "filename": "phase3-auto-healing.yml",
    "name": "Phase 3 CI Auto-Healing: Security & Build Diagnostics",
    "on": {
      "workflow_call": {
        "inputs": {
          "trigger_deep_remediation": {
            "description": "Force deep remediation even if not triggered by failures",
            "required": false,
            "type": "boolean",
            "default": false
          }
        },
        "outputs": {
          "remediation_applied": {
            "description": "Whether Phase 3 auto-healing was applied",
            "value": "${{ jobs.deep-remediation.outputs.remediation_applied }}"
          },
          "security_score_improvement": {
            "description": "Security score improvement from remediation",
            "value": "${{ jobs.deep-remediation.outputs.security_improvement }}"
          },
          "build_diagnostic_available": {
            "description": "Whether build diagnostics were generated",
            "value": "${{ jobs.deep-remediation.outputs.build_diagnostics }}"
          }
        }
      },
      "workflow_dispatch": {
        "inputs": {
          "force_aggressive_mode": {
            "description": "Use aggressive security remediation",
            "required": false,
            "type": "boolean",
            "default": false
          }
        }
      }
    },
    "env": {
      "NODE_OPTIONS": "--max-old-space-size=4096"
    },
    "jobs": {
      "deep-remediation": {
        "name": "Phase 3: Deep Security & Build Remediation",
        "runs-on": "ubuntu-latest",
        "if": "failure() || inputs.trigger_deep_remediation",
        "outputs": {
          "remediation_applied": "${{ steps.remediation-summary.outputs.applied }}",
          "security_improvement": "${{ steps.remediation-summary.outputs.security_score }}",
          "build_diagnostics": "${{ steps.remediation-summary.outputs.build_report }}"
        },
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4",
            "with": {
              "token": "${{ secrets.GITHUB_TOKEN }}",
              "fetch-depth": 1
            }
          },
          {
            "name": "Setup Node.js with caching",
            "uses": "actions/setup-node@v4",
            "with": {
              "node-version": "18",
              "cache": "npm"
            }
          },
          {
            "name": "Install dependencies",
            "run": "npm ci --prefer-offline --no-audit --no-fund\nnpm install --no-save tsx\n"
          },
          {
            "name": "Phase 3.1 - Security Deep Remediation",
            "id": "security-remediation",
            "continue-on-error": true,
            "run": "echo \"üõ°Ô∏è Running Phase 3 Security Deep Remediation...\"\n\n# Set aggressive mode based on input or critical vulnerability count\nAGGRESSIVE_FLAG=\"\"\nif [[ \"${{ inputs.force_aggressive_mode }}\" == \"true\" ]]; then\n  AGGRESSIVE_FLAG=\"--aggressive\"\nfi\n\n# Run security deep fix\nnpx tsx scripts/securityDeepFix.ts $AGGRESSIVE_FLAG || true\n\n# Check if remediation was effective\nif [[ -f \"reports/security-deep-fix.md\" ]]; then\n  echo \"security_report_generated=true\" >> $GITHUB_OUTPUT\n  \n  # Extract effectiveness score if available\n  if grep -q \"Effectiveness Score:\" reports/security-deep-fix.md; then\n    EFFECTIVENESS=$(grep \"Effectiveness Score:\" reports/security-deep-fix.md | head -1 | grep -oE '[0-9]+' || echo \"0\")\n    echo \"effectiveness_score=$EFFECTIVENESS\" >> $GITHUB_OUTPUT\n  fi\nfi\n"
          },
          {
            "name": "Phase 3.2 - Build Failure Diagnostics",
            "id": "build-diagnostics",
            "continue-on-error": true,
            "run": "echo \"üèóÔ∏è Running Phase 3 Build Failure Diagnostics...\"\n\n# Run build diagnostics with comprehensive logging\nnpx tsx scripts/analyzeBuildFailure.ts --verbose --save-logs || true\n\n# Check if diagnostics were generated\nif [[ -f \"reports/build-diagnostics.md\" ]]; then\n  echo \"build_report_generated=true\" >> $GITHUB_OUTPUT\n  \n  # Check if quick fixes were identified\n  if grep -q \"Quick Fixes\" reports/build-diagnostics.md; then\n    QUICK_FIXES=$(grep -c \"Quick Fixes\" reports/build-diagnostics.md || echo \"0\")\n    echo \"quick_fixes_available=$QUICK_FIXES\" >> $GITHUB_OUTPUT\n  fi\nfi\n"
          },
          {
            "name": "Phase 3.3 - Apply Quick Fixes",
            "id": "apply-quick-fixes",
            "if": "steps.build-diagnostics.outputs.quick_fixes_available > 0",
            "continue-on-error": true,
            "run": "echo \"‚ö° Applying automated quick fixes...\"\n\n# Clear Next.js cache if build issues detected\nif [[ -d \".next\" ]]; then\n  echo \"üßπ Clearing Next.js cache...\"\n  rm -rf .next\n  echo \"cache_cleared=true\" >> $GITHUB_OUTPUT\nfi\n\n# Reinstall dependencies if dependency issues detected\nif grep -q \"dependency\" reports/build-diagnostics.md; then\n  echo \"üì¶ Reinstalling dependencies...\"\n  rm -f package-lock.json\n  npm install\n  echo \"dependencies_reinstalled=true\" >> $GITHUB_OUTPUT\nfi\n"
          },
          {
            "name": "Phase 3.4 - Re-run Critical Checks",
            "id": "post-remediation-check",
            "continue-on-error": true,
            "run": "echo \"üîÑ Re-running critical checks after remediation...\"\n\n# Re-run the main gatekeeper to measure improvement\nnpx tsx scripts/checkCriticalFailures.ts --report-only || true\n\n# Extract final health score\nif [[ -f \"reports/ci-health-gate.md\" ]]; then\n  if grep -q \"Confidence Score:\" reports/ci-health-gate.md; then\n    FINAL_SCORE=$(grep \"Confidence Score:\" reports/ci-health-gate.md | head -1 | grep -oE '[0-9]+' || echo \"0\")\n    echo \"final_health_score=$FINAL_SCORE\" >> $GITHUB_OUTPUT\n  fi\nfi\n"
          },
          {
            "name": "Summarize Remediation Results",
            "id": "remediation-summary",
            "run": "echo \"üìä Summarizing Phase 3 Auto-Healing Results...\"\n\nAPPLIED=\"false\"\nSECURITY_SCORE=\"0\"\nBUILD_REPORT=\"false\"\n\n# Check if security remediation was applied\nif [[ \"${{ steps.security-remediation.outputs.security_report_generated }}\" == \"true\" ]]; then\n  APPLIED=\"true\"\n  SECURITY_SCORE=\"${{ steps.security-remediation.outputs.effectiveness_score }}\"\nfi\n\n# Check if build diagnostics were generated\nif [[ \"${{ steps.build-diagnostics.outputs.build_report_generated }}\" == \"true\" ]]; then\n  BUILD_REPORT=\"true\"\nfi\n\necho \"applied=$APPLIED\" >> $GITHUB_OUTPUT\necho \"security_score=$SECURITY_SCORE\" >> $GITHUB_OUTPUT\necho \"build_report=$BUILD_REPORT\" >> $GITHUB_OUTPUT\n\n# Create summary for GitHub\ncat >> $GITHUB_STEP_SUMMARY << 'EOF'\n## üõ°Ô∏è Phase 3 Auto-Healing Summary\n\n### Security Deep Remediation\n- **Applied:** ${{ steps.security-remediation.outputs.security_report_generated == 'true' && '‚úÖ Yes' || '‚ùå No' }}\n- **Effectiveness Score:** ${{ steps.security-remediation.outputs.effectiveness_score || 'N/A' }}/100\n\n### Build Diagnostics\n- **Generated:** ${{ steps.build-diagnostics.outputs.build_report_generated == 'true' && '‚úÖ Yes' || '‚ùå No' }}\n- **Quick Fixes Available:** ${{ steps.build-diagnostics.outputs.quick_fixes_available || '0' }}\n\n### Quick Fixes Applied\n- **Cache Cleared:** ${{ steps.apply-quick-fixes.outputs.cache_cleared == 'true' && '‚úÖ Yes' || '‚ùå No' }}\n- **Dependencies Reinstalled:** ${{ steps.apply-quick-fixes.outputs.dependencies_reinstalled == 'true' && '‚úÖ Yes' || '‚ùå No' }}\n\n### Final Health Score\n- **Post-Remediation Score:** ${{ steps.post-remediation-check.outputs.final_health_score || 'N/A' }}/100\n\n### üìã Generated Reports\n- Security Deep Fix: `reports/security-deep-fix.md`\n- Build Diagnostics: `reports/build-diagnostics.md`\n- CI Health Gate: `reports/ci-health-gate.md`\nEOF\n"
          },
          {
            "name": "Upload Remediation Reports",
            "uses": "actions/upload-artifact@v4",
            "if": "always()",
            "with": {
              "name": "phase3-auto-healing-reports",
              "path": "reports/security-deep-fix.md\nreports/build-diagnostics.md\nreports/security-manual-review.md\nreports/build-log.txt\n",
              "retention-days": 30
            }
          },
          {
            "name": "Comment on PR with Remediation Results",
            "if": "github.event_name == 'pull_request' && always()",
            "uses": "actions/github-script@v7",
            "with": {
              "script": "const fs = require('fs');\nconst securityReportExists = fs.existsSync('reports/security-deep-fix.md');\nconst buildReportExists = fs.existsSync('reports/build-diagnostics.md');\n\nlet comment = '## üõ°Ô∏è Phase 3 Auto-Healing Results\\n\\n';\n\nif (securityReportExists || buildReportExists) {\n  comment += '### üìä Remediation Summary\\n';\n  comment += `- **Security Remediation:** ${securityReportExists ? '‚úÖ Applied' : '‚ùå Not needed'}\\n`;\n  comment += `- **Build Diagnostics:** ${buildReportExists ? '‚úÖ Generated' : '‚ùå Not needed'}\\n`;\n  comment += `- **Effectiveness Score:** ${{ steps.security-remediation.outputs.effectiveness_score || 'N/A' }}/100\\n\\n`;\n  \n  comment += '### üìã Available Reports\\n';\n  if (securityReportExists) comment += '- üõ°Ô∏è Security Deep Fix Report\\n';\n  if (buildReportExists) comment += '- üèóÔ∏è Build Diagnostics Report\\n';\n  comment += '\\n> Reports are available in the workflow artifacts.\\n';\n} else {\n  comment += '‚úÖ **No deep remediation required** - Initial CI checks were sufficient.\\n';\n}\n\ngithub.rest.issues.createComment({\n  issue_number: context.issue.number,\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  body: comment\n});\n"
            }
          }
        ]
      },
      "trigger-recheck": {
        "name": "Secure Re-Dispatch & Post-Remediation Check",
        "runs-on": "ubuntu-latest",
        "needs": "deep-remediation",
        "if": "needs.deep-remediation.outputs.remediation_applied == 'true'",
        "steps": [
          {
            "name": "Checkout code for re-dispatch",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "üîÑ Trigger Post-Remediation CI Run",
            "env": {
              "GH_TOKEN": "${{ secrets.CI_REDISPATCH_TOKEN }}"
            },
            "run": "echo \"üöÄ Triggering repository_dispatch for post-remediation CI...\"\n\n# Verify token is available\nif [ -z \"$GH_TOKEN\" ]; then\n  echo \"‚ùå CI_REDISPATCH_TOKEN secret not found\"\n  echo \"üìã Please follow docs/PHASE4_SECURE_TOKEN_SETUP.md to configure the token\"\n  exit 1\nfi\n\n# Prepare client payload\nSECURITY_IMPROVEMENT=\"${{ needs.deep-remediation.outputs.security_improvement }}\"\nBUILD_DIAGNOSTICS=\"${{ needs.deep-remediation.outputs.build_diagnostic_available }}\"\n\n# Create dispatch event with secure token\nHTTP_STATUS=$(curl -s -o /dev/null -w \"%{http_code}\" -X POST \\\n  -H \"Accept: application/vnd.github.v3+json\" \\\n  -H \"Authorization: Bearer $GH_TOKEN\" \\\n  https://api.github.com/repos/${{ github.repository }}/dispatches \\\n  -d \"{\n    \\\"event_type\\\": \\\"post-remediation-check\\\",\n    \\\"client_payload\\\": {\n      \\\"security_improvement\\\": \\\"$SECURITY_IMPROVEMENT\\\",\n      \\\"build_diagnostics\\\": \\\"$BUILD_DIAGNOSTICS\\\",\n      \\\"triggered_by\\\": \\\"phase3-auto-healing\\\",\n      \\\"commit_sha\\\": \\\"${{ github.sha }}\\\",\n      \\\"remediation_timestamp\\\": \\\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\\\"\n    }\n  }\")\n\n# Check response status\ncase $HTTP_STATUS in\n  200|201|204)\n    echo \"‚úÖ Repository dispatch successful (HTTP $HTTP_STATUS)\"\n    echo \"üîÑ Post-remediation validation workflow should start shortly\"\n    ;;\n  401)\n    echo \"‚ùå Authentication failed (HTTP 401)\"\n    echo \"üîß Check CI_REDISPATCH_TOKEN secret configuration\"\n    exit 1\n    ;;\n  403)\n    echo \"‚ùå Forbidden (HTTP 403)\"\n    echo \"üîß Token may lack required permissions (repo, workflow scopes needed)\"\n    exit 1\n    ;;\n  404)\n    echo \"‚ùå Repository not found (HTTP 404)\"\n    echo \"üîß Check repository path or token repository access\"\n    exit 1\n    ;;\n  *)\n    echo \"‚ùå Unexpected response (HTTP $HTTP_STATUS)\"\n    echo \"üîß Check GitHub API status and token configuration\"\n    exit 1\n    ;;\nesac\n"
          },
          {
            "name": "üìä Record Re-Dispatch Metrics",
            "run": "echo \"üìà Phase 4 Re-Dispatch Metrics:\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"- **Trigger Time:** $(date -u +%Y-%m-%dT%H:%M:%SZ)\" >> $GITHUB_STEP_SUMMARY\necho \"- **Security Improvement:** ${{ needs.deep-remediation.outputs.security_improvement }}/100\" >> $GITHUB_STEP_SUMMARY\necho \"- **Build Diagnostics:** ${{ needs.deep-remediation.outputs.build_diagnostic_available == 'true' && 'Available' || 'Not Generated' }}\" >> $GITHUB_STEP_SUMMARY\necho \"- **Remediation Applied:** ‚úÖ Yes\" >> $GITHUB_STEP_SUMMARY\necho \"- **Post-Validation:** üîÑ Triggered\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"> Post-remediation validation will run independently to verify fixes.\" >> $GITHUB_STEP_SUMMARY\n"
          },
          {
            "name": "üè∑Ô∏è Add Remediation Success Labels",
            "if": "needs.deep-remediation.outputs.security_improvement >= 70 && github.event_name == 'pull_request'",
            "env": {
              "GH_TOKEN": "${{ secrets.CI_REDISPATCH_TOKEN }}"
            },
            "run": "echo \"üè∑Ô∏è Adding success labels to PR...\"\n\n# Add labels indicating successful auto-healing\ncurl -X POST \\\n  -H \"Accept: application/vnd.github.v3+json\" \\\n  -H \"Authorization: Bearer $GH_TOKEN\" \\\n  https://api.github.com/repos/${{ github.repository }}/issues/${{ github.event.pull_request.number }}/labels \\\n  -d '{\"labels\": [\"auto-healing-applied\", \"security-improved\", \"phase3-remediation\"]}'\n\necho \"‚úÖ Labels added successfully\"\n"
          },
          {
            "name": "üìù Update PR with Remediation Summary",
            "if": "github.event_name == 'pull_request'",
            "env": {
              "GH_TOKEN": "${{ secrets.CI_REDISPATCH_TOKEN }}"
            },
            "run": "echo \"üìù Posting remediation summary to PR...\"\n\n# Create detailed PR comment\ncat > pr_comment.md << 'EOF'\n## üõ°Ô∏è Phase 3 Auto-Healing Complete\n\n### üìä Remediation Results\n- **Security Score Improvement:** ${{ needs.deep-remediation.outputs.security_improvement }}/100\n- **Build Diagnostics:** ${{ needs.deep-remediation.outputs.build_diagnostic_available == 'true' && '‚úÖ Generated' || '‚ùå Not needed' }}\n- **Automated Fixes Applied:** ‚úÖ Yes\n\n### üîÑ Next Steps\n1. **Post-Remediation Validation:** Automatically triggered\n2. **Review Reports:** Check workflow artifacts for detailed analysis\n3. **Manual Review:** Address any remaining issues in security reports\n\n### üìã Available Reports\n- üõ°Ô∏è Security Deep Fix Report\n- üèóÔ∏è Build Diagnostics Report\n- üìä Updated CI Health Gate Report\n\n> **Note:** This comment was generated by Phase 3 Auto-Healing system. \n> Post-remediation validation is running independently.\nEOF\n\n# Post comment to PR\ncurl -X POST \\\n  -H \"Accept: application/vnd.github.v3+json\" \\\n  -H \"Authorization: Bearer $GH_TOKEN\" \\\n  https://api.github.com/repos/${{ github.repository }}/issues/${{ github.event.pull_request.number }}/comments \\\n  -d \"{\\\"body\\\": $(cat pr_comment.md | jq -Rs .)}\"\n\necho \"‚úÖ PR comment posted successfully\"\n"
          },
          {
            "name": "üîç Verify Re-Dispatch Success",
            "run": "echo \"üîç Verifying Phase 4 re-dispatch completed successfully...\"\necho \"\"\necho \"‚úÖ Repository dispatch event sent\"\necho \"‚úÖ Remediation metrics recorded\"\necho \"‚úÖ PR labels and comments updated (if applicable)\"\necho \"\"\necho \"üéØ Phase 4 Secure Re-Dispatch System: OPERATIONAL\"\necho \"\"\necho \"üìã Post-remediation validation should begin within 1-2 minutes\"\necho \"üìä Check the 'Actions' tab for 'Post-Remediation Check' workflow\"\n"
          }
        ]
      }
    },
    "intent": "Runs automated tests and builds",
    "category": "test"
  },
  {
    "filename": "planner-contract.yml",
    "name": "üìã Planner Contract",
    "on": {
      "schedule": [
        {
          "cron": "30 3 * * *"
        }
      ],
      "workflow_dispatch": {
        "inputs": {
          "test_scenarios": {
            "description": "Test scenarios to run (all,basic,edge,stress)",
            "type": "choice",
            "default": "all",
            "options": [
              "all",
              "basic",
              "edge",
              "stress"
            ]
          },
          "fail_fast": {
            "description": "Stop on first test failure",
            "type": "boolean",
            "default": false
          }
        }
      }
    },
    "concurrency": {
      "group": "planner-contract-${{ github.ref }}",
      "cancel-in-progress": true
    },
    "env": {
      "NODE_ENV": "test",
      "CI": true
    },
    "jobs": {
      "planner-contract": {
        "name": "üìã Validate Planner Behavior",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 10,
        "permissions": {
          "contents": "read",
          "issues": "write"
        },
        "steps": [
          {
            "name": "üì• Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "üîß Setup Node.js",
            "uses": "actions/setup-node@v4",
            "with": {
              "node-version": 20,
              "cache": "npm"
            }
          },
          {
            "name": "üì¶ Install dependencies",
            "run": "echo \"üîß Installing test dependencies...\"\nnpm ci --prefer-offline --no-audit\n"
          },
          {
            "name": "üß™ Run planner unit tests",
            "id": "unit-tests",
            "run": "echo \"üß™ Running comprehensive planner unit tests...\"\n\n# Set test configuration\nexport SCAN_MIN_PER_PLATFORM=40\nexport SCAN_MAX_PER_PLATFORM=120\nexport SCAN_GLOBAL_MAX=800\nexport SCAN_COOLDOWN_MIN=180\nexport MIN_CONF=0.70\nexport MIN_CANDIDATES=20\nexport PLATFORM_ALLOW=\"reddit,youtube,giphy,imgur,bluesky,tumblr,lemmy,pixabay\"\n\n# Run Node.js tests with timeout and detailed output\ntimeout 300s node --test --test-reporter=spec scripts/scan-plan.test.mjs > test_output.txt 2>&1\nTEST_EXIT_CODE=$?\n\necho \"TEST_EXIT_CODE=$TEST_EXIT_CODE\" >> $GITHUB_ENV\n\n# Display results\ncat test_output.txt\n\nif [ $TEST_EXIT_CODE -eq 0 ]; then\n  echo \"‚úÖ All planner unit tests passed\"\n  echo \"UNIT_TESTS_STATUS=passed\" >> $GITHUB_ENV\nelse\n  echo \"‚ùå Planner unit tests failed with exit code: $TEST_EXIT_CODE\"\n  echo \"UNIT_TESTS_STATUS=failed\" >> $GITHUB_ENV\nfi\n"
          },
          {
            "name": "üîç Contract validation scenarios",
            "id": "contract-validation",
            "if": "inputs.test_scenarios == 'all' || inputs.test_scenarios == 'edge' || github.event_name == 'schedule'",
            "run": "echo \"üîç Running contract validation scenarios...\"\n\n# Test 1: Global cap enforcement\necho \"üìä Testing global cap enforcement...\"\nnode -e \"\nimport { spawn } from 'node:child_process';\nimport { writeFile } from 'node:fs/promises';\n\nconst wrapper = \\`\n  globalThis.fetch = async (url) => {\n    if (url.includes('/api/system/metrics')) {\n      return new Response(JSON.stringify({\n        queue_depth_by_platform: { reddit: 500, youtube: 400 },\n        last_scan_times: {}\n      }), { status: 200, headers: {'content-type':'application/json'}});\n    }\n    return new Response('[]', { status: 200, headers: {'content-type':'application/json'}});\n  };\n  import('./scripts/scan-plan.mjs').catch(e => { console.error(e); process.exit(1); });\n\\`;\n\nawait writeFile('__tmp_global_cap_test.mjs', wrapper);\n\nconst proc = spawn('node', ['__tmp_global_cap_test.mjs'], {\n  env: {\n    ...process.env,\n    TARGET_URL: 'https://example.com',\n    SUPABASE_URL: 'https://supabase.local',\n    SUPABASE_SERVICE_ROLE_KEY: 'test',\n    SCAN_GLOBAL_MAX: '800'\n  }\n});\n\nlet stdout = '';\nproc.stdout.on('data', d => stdout += d.toString());\n\nconst code = await new Promise(res => proc.on('close', res));\n\nif (stdout.includes('global_cap_reached')) {\n  console.log('‚úÖ Global cap enforcement: PASSED');\n} else {\n  console.log('‚ùå Global cap enforcement: FAILED');\n  console.log('Output:', stdout);\n  process.exit(1);\n}\n\" || exit 1\n\necho \"CONTRACT_VALIDATION_STATUS=passed\" >> $GITHUB_ENV\n"
          },
          {
            "name": "üöÄ Stress test scenarios",
            "id": "stress-tests",
            "if": "inputs.test_scenarios == 'all' || inputs.test_scenarios == 'stress' || github.event_name == 'schedule'",
            "run": "echo \"üöÄ Running stress test scenarios...\"\n\n# Test with extreme platform counts\nnode -e \"\nimport { spawn } from 'node:child_process';\nimport { writeFile } from 'node:fs/promises';\n\n// Generate 50 platforms with various queue depths\nconst platforms = {};\nfor (let i = 0; i < 50; i++) {\n  platforms[\\`platform\\${i}\\`] = Math.floor(Math.random() * 200);\n}\n\nconst wrapper = \\`\n  globalThis.fetch = async (url) => {\n    if (url.includes('/api/system/metrics')) {\n      return new Response(JSON.stringify({\n        queue_depth_by_platform: ${JSON.stringify(platforms)},\n        last_scan_times: {}\n      }), { status: 200, headers: {'content-type':'application/json'}});\n    }\n    // Generate large supabase response\n    const rows = Array.from({length: 1000}, (_, i) => ({\n      source_platform: \\`platform\\${i % 50}\\`,\n      confidence_score: Math.random(),\n      ingest_priority: 0,\n      is_posted: false,\n      is_approved: true\n    }));\n    return new Response(JSON.stringify(rows), { status: 200, headers: {'content-type':'application/json'}});\n  };\n  import('./scripts/scan-plan.mjs').catch(e => { console.error(e); process.exit(1); });\n\\`;\n\nawait writeFile('__tmp_stress_test.mjs', wrapper);\n\nconst start = Date.now();\nconst proc = spawn('node', ['__tmp_stress_test.mjs'], {\n  env: {\n    ...process.env,\n    TARGET_URL: 'https://example.com',\n    SUPABASE_URL: 'https://supabase.local',  \n    SUPABASE_SERVICE_ROLE_KEY: 'test',\n    PLATFORM_ALLOW: Object.keys(platforms).join(',')\n  }\n});\n\nconst code = await new Promise(res => proc.on('close', res));\nconst duration = Date.now() - start;\n\nif (code === 0 && duration < 5000) {\n  console.log(\\`‚úÖ Stress test: PASSED (completed in \\${duration}ms)\\`);\n} else {\n  console.log(\\`‚ùå Stress test: FAILED (exit code: \\${code}, duration: \\${duration}ms)\\`);\n  process.exit(1);\n}\n\" || exit 1\n\necho \"STRESS_TESTS_STATUS=passed\" >> $GITHUB_ENV\n"
          },
          {
            "name": "üìä Generate contract summary",
            "if": "always()",
            "run": "echo \"## üìã Planner Contract Validation Results\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Date:** $(date -u -Iseconds)\" >> $GITHUB_STEP_SUMMARY\necho \"**Test Scenarios:** ${{ inputs.test_scenarios || 'all' }}\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Results table\necho \"| Test Category | Status | Notes |\" >> $GITHUB_STEP_SUMMARY\necho \"|---------------|--------|-------|\" >> $GITHUB_STEP_SUMMARY\n\nif [ \"${{ env.UNIT_TESTS_STATUS }}\" = \"passed\" ]; then\n  echo \"| Unit Tests | ‚úÖ Passed | All core logic validated |\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"| Unit Tests | ‚ùå Failed | Critical planner logic broken |\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [ \"${{ env.CONTRACT_VALIDATION_STATUS }}\" = \"passed\" ]; then\n  echo \"| Contract Validation | ‚úÖ Passed | Policy enforcement working |\" >> $GITHUB_STEP_SUMMARY\nelif [ \"${{ env.CONTRACT_VALIDATION_STATUS }}\" = \"\" ]; then\n  echo \"| Contract Validation | ‚è≠Ô∏è Skipped | Not run in this scenario |\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"| Contract Validation | ‚ùå Failed | Policy violations detected |\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [ \"${{ env.STRESS_TESTS_STATUS }}\" = \"passed\" ]; then\n  echo \"| Stress Tests | ‚úÖ Passed | Performance within limits |\" >> $GITHUB_STEP_SUMMARY\nelif [ \"${{ env.STRESS_TESTS_STATUS }}\" = \"\" ]; then\n  echo \"| Stress Tests | ‚è≠Ô∏è Skipped | Not run in this scenario |\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"| Stress Tests | ‚ùå Failed | Performance degradation |\" >> $GITHUB_STEP_SUMMARY\nfi\n\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Overall status\nif [ \"${{ env.UNIT_TESTS_STATUS }}\" = \"passed\" ] && \\\n   ( [ \"${{ env.CONTRACT_VALIDATION_STATUS }}\" = \"passed\" ] || [ \"${{ env.CONTRACT_VALIDATION_STATUS }}\" = \"\" ] ) && \\\n   ( [ \"${{ env.STRESS_TESTS_STATUS }}\" = \"passed\" ] || [ \"${{ env.STRESS_TESTS_STATUS }}\" = \"\" ] ); then\n  echo \"### üéâ Overall Status: **HEALTHY**\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"The demand-driven scanner planner is operating within contract specifications.\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"### üö® Overall Status: **CONTRACT VIOLATION**\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"Critical planner behavior violations detected. Immediate attention required.\" >> $GITHUB_STEP_SUMMARY\nfi\n"
          },
          {
            "name": "üßπ Cleanup test artifacts",
            "if": "always()",
            "run": "rm -f __tmp_*.mjs test_output.txt scan_plan.json scan_matrix.json\n"
          },
          {
            "name": "üì§ Upload test artifacts",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "planner-contract-test-results-${{ github.run_number }}",
              "path": "test_output.txt\nscan_plan.json\nscan_matrix.json\n",
              "retention-days": 7,
              "if-no-files-found": "ignore"
            }
          },
          {
            "name": "üö® Create issue on contract violation",
            "if": "failure() && github.event_name == 'schedule'",
            "uses": "actions/github-script@v7",
            "with": {
              "script": "const issueTitle = `üö® Planner Contract Violation - ${new Date().toISOString().split('T')[0]}`;\nconst issueBody = `# Planner Contract Violation Report\n\nThe daily planner contract validation detected critical violations on ${new Date().toISOString()}.\n\n**Workflow Run:** [#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})\n**Test Scenarios:** ${{ inputs.test_scenarios || 'all' }}\n\n## Failure Details\n\n- **Unit Tests:** ${{ env.UNIT_TESTS_STATUS || 'unknown' }}\n- **Contract Validation:** ${{ env.CONTRACT_VALIDATION_STATUS || 'unknown' }}\n- **Stress Tests:** ${{ env.STRESS_TESTS_STATUS || 'unknown' }}\n\n## Impact Assessment\n\nContract violations in the demand-driven scanner planner can lead to:\n- Inefficient resource allocation\n- Platform diversity degradation  \n- API quota exhaustion\n- Poor content quality selection\n- Scanner infinite loops or crashes\n\n## Investigation Steps\n\n1. üîç Download test artifacts: \\`planner-contract-test-results-${{ github.run_number }}\\`\n2. üìä Review test output for specific failures\n3. üß™ Run tests locally: \\`node --test scripts/scan-plan.test.mjs\\`\n4. üîß Check recent changes to \\`scripts/scan-plan.mjs\\`\n5. üìà Verify environment variables and thresholds\n\n## Common Contract Violations\n\n- **Global Cap Bypass:** Planner allows scanning when total queue > SCAN_GLOBAL_MAX\n- **Cooldown Ignored:** Recent scans not respected for cooldown period\n- **Quality Gates Broken:** Low-confidence content allowed when high-quality alternatives exist\n- **Performance Regression:** Planning takes >5 seconds or excessive memory\n- **Logic Errors:** Incorrect deficit calculations or matrix generation\n\n## Immediate Actions\n\n- [ ] Stop all scheduled scanning workflows\n- [ ] Review and fix planner logic\n- [ ] Run manual validation: \\`npm run test:planner\\`\n- [ ] Deploy fix and verify contract compliance\n- [ ] Re-enable scheduled scans\n- [ ] Close this issue when resolved\n\n---\n\nThis issue was automatically created by the Planner Contract workflow.\n`;\n\nawait github.rest.issues.create({\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  title: issueTitle,\n  body: issueBody,\n  labels: ['critical', 'scanner', 'contract-violation', 'auto-created']\n});\n"
            }
          }
        ]
      }
    },
    "intent": "General workflow (purpose unclear from name)",
    "category": "maintenance"
  },
  {
    "filename": "post-breakfast.yml",
    "name": "Post Breakfast Content",
    "on": {
      "schedule": [
        {
          "cron": "0 7 * * *"
        }
      ],
      "workflow_dispatch": null
    },
    "jobs": {
      "refresh-token": {
        "uses": "./.github/workflows/token-refresh.yml",
        "secrets": {
          "SITE_URL": "${{ secrets.SITE_URL }}",
          "SERVICE_ACCOUNT_SECRET": "${{ secrets.SERVICE_ACCOUNT_SECRET }}",
          "REFRESH_TOKEN": "${{ secrets.REFRESH_TOKEN }}",
          "AUTH_TOKEN": "${{ secrets.AUTH_TOKEN }}"
        }
      },
      "post-breakfast": {
        "needs": "refresh-token",
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Post Breakfast Content",
            "env": {
              "AUTH_TOKEN": "${{ needs.refresh-token.outputs.auth_token }}"
            },
            "run": "echo \"üåÖ Posting breakfast content...\"\n\n# Store response in variable for error analysis\nRESPONSE=$(curl -s -w \"HTTP_CODE:%{http_code}\" -L -X POST \"${{ secrets.SITE_URL }}/api/admin/posting/post-now\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"immediate\": true}' \\\n  --retry 2 --retry-delay 10 2>&1 || true)\n\n# Extract HTTP code and response body\nHTTP_CODE=$(echo \"$RESPONSE\" | grep -o 'HTTP_CODE:[0-9]*' | cut -d':' -f2)\nRESPONSE_BODY=$(echo \"$RESPONSE\" | sed 's/HTTP_CODE:[0-9]*$//')\n\necho \"HTTP Status: $HTTP_CODE\"\necho \"Response: $RESPONSE_BODY\"\n\n# Check for specific error conditions\nif [[ \"$HTTP_CODE\" == \"401\" ]]; then\n  echo \"‚ùå CRITICAL: Authentication failed - AUTH_TOKEN is invalid or expired\"\n  echo \"This indicates a security breach recovery issue - AUTH_TOKEN needs to be updated in GitHub secrets\"\n  exit 1\nelif [[ \"$HTTP_CODE\" != \"200\" ]]; then\n  echo \"‚ùå FAILED: API returned HTTP $HTTP_CODE\"\n  echo \"Response body: $RESPONSE_BODY\"\n  exit 1\nelse\n  echo \"‚úÖ SUCCESS: Breakfast content posted (HTTP $HTTP_CODE)\"\nfi\n"
          },
          {
            "name": "Verify Post Success",
            "if": "success()",
            "run": "echo \"‚úÖ Breakfast content posted successfully\"\n# Optional: Send success notification\n"
          },
          {
            "name": "Handle Failure",
            "if": "failure()",
            "run": "echo \"‚ùå CRITICAL: Breakfast posting failed - this may indicate:\"\necho \"  1. AUTH_TOKEN is invalid/expired (most common after security breach)\"\necho \"  2. API endpoint is down\"\necho \"  3. No approved content available\"\necho \"  4. Database connectivity issues\"\necho \"\"\necho \"üîß TROUBLESHOOTING STEPS:\"\necho \"  1. Check AUTH_TOKEN in GitHub secrets matches current production token\"\necho \"  2. Verify Vercel deployment is live\"\necho \"  3. Check content queue has approved posts available\"\necho \"  4. Review API logs in Vercel dashboard\"\necho \"\"\necho \"üìû IMMEDIATE ACTIONS REQUIRED:\"\necho \"  - Update AUTH_TOKEN in GitHub secrets if authentication failed\"\necho \"  - Run content approval if queue is empty\"\necho \"  - Check Vercel deployment status\"\n"
          }
        ]
      }
    },
    "intent": "Posts scheduled content at 08:00 ET (breakfast slot)",
    "category": "posting"
  },
  {
    "filename": "post-deploy-check.yml",
    "name": "Post-Deploy Check",
    "on": {
      "deployment_status": null,
      "push": {
        "branches": [
          "main"
        ]
      },
      "workflow_run": {
        "workflows": [
          "Vercel Production Deployment"
        ],
        "types": [
          "completed"
        ]
      },
      "workflow_dispatch": {
        "inputs": {
          "skip_refill_check": {
            "description": "Skip refill dry-run check",
            "type": "boolean",
            "default": false
          },
          "target_url": {
            "description": "Target URL to check (defaults to production)",
            "type": "string",
            "default": "https://hotdog-diaries.vercel.app"
          }
        }
      },
      "workflow_call": {
        "inputs": {
          "skip_refill_check": {
            "description": "Skip refill dry-run check",
            "type": "boolean",
            "default": false
          },
          "target_url": {
            "description": "Target URL to check",
            "type": "string",
            "default": "https://hotdog-diaries.vercel.app"
          }
        }
      }
    },
    "concurrency": {
      "group": "post-deploy-${{ github.ref }}",
      "cancel-in-progress": false
    },
    "permissions": {
      "contents": "read",
      "issues": "write"
    },
    "env": {
      "PROD_URL": "${{ inputs.target_url || 'https://hotdog-diaries.vercel.app' }}",
      "AUTH_TOKEN": "${{ secrets.AUTH_TOKEN }}"
    },
    "jobs": {
      "health-check": {
        "runs-on": "ubuntu-latest",
        "name": "Health & Metrics Validation",
        "timeout-minutes": 10,
        "outputs": {
          "health-status": "${{ steps.health.outputs.status }}",
          "metrics-snapshot": "${{ steps.metrics.outputs.snapshot }}"
        },
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "20",
              "cache-key-suffix": "post-deploy",
              "install-dependencies": "false"
            }
          },
          {
            "name": "Wait for deployment readiness",
            "if": "github.event_name == 'push' || github.event_name == 'deployment_status'",
            "run": "echo \"‚è≥ Waiting for deployment to be ready...\"\nsleep 30\n"
          },
          {
            "name": "Test deep health endpoint",
            "id": "health",
            "run": "echo \"üè• Testing /admin/health/deep endpoint...\"\n\nRESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  --max-time 30 \\\n  \"$PROD_URL/api/admin/health/deep\")\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//g')\n\necho \"HTTP Status: $HTTP_STATUS\"\necho \"Response Body: $BODY\"\n\nif [ \"$HTTP_STATUS\" -eq 200 ]; then\n  echo \"‚úÖ Deep health check passed\"\n  \n  # Verify 'ok' status in response\n  OK_STATUS=$(echo \"$BODY\" | jq -r '.ok // false')\n  if [ \"$OK_STATUS\" = \"true\" ]; then\n    echo \"‚úÖ Health status is OK\"\n    echo \"status=healthy\" >> $GITHUB_OUTPUT\n    \n    # Extract component details for artifact\n    echo \"$BODY\" | jq '.components // {}' > health_components.json\n    \n  else\n    echo \"‚ùå Health status indicates issues\"\n    echo \"status=degraded\" >> $GITHUB_OUTPUT\n    echo \"$BODY\" | jq '.components // {}' > health_components.json\n    \n    # Don't fail immediately - continue to refill check\n    echo \"‚ö†Ô∏è Health check shows degraded status but continuing...\"\n  fi\nelse\n  echo \"‚ùå Deep health check failed with status $HTTP_STATUS\"\n  echo \"status=failed\" >> $GITHUB_OUTPUT\n  echo '{\"error\": \"health_check_failed\", \"status\": '$HTTP_STATUS'}' > health_components.json\n  exit 1\nfi\n"
          },
          {
            "name": "Get system metrics snapshot",
            "id": "metrics",
            "run": "echo \"üìä Collecting system metrics snapshot...\"\n\nMETRICS_RESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  --max-time 15 \\\n  \"$PROD_URL/api/system/metrics\")\n\nMETRICS_HTTP_STATUS=$(echo $METRICS_RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nMETRICS_BODY=$(echo $METRICS_RESPONSE | sed -e 's/HTTPSTATUS:.*//g')\n\necho \"Metrics HTTP Status: $METRICS_HTTP_STATUS\"\n\nif [ \"$METRICS_HTTP_STATUS\" -eq 200 ]; then\n  echo \"‚úÖ System metrics collected\"\n  echo \"$METRICS_BODY\" > metrics_snapshot.json\n  echo \"snapshot=available\" >> $GITHUB_OUTPUT\n  \n  # Extract key metrics for logs\n  QUEUE_DEPTH=$(echo \"$METRICS_BODY\" | jq '[.queue_depth_by_platform | to_entries[]] | map(.value) | add // 0')\n  POSTS_TODAY=$(echo \"$METRICS_BODY\" | jq '.posts_today // 0')\n  HEALTH_STATUS=$(echo \"$METRICS_BODY\" | jq -r '.health_status // \"unknown\"')\n  \n  echo \"üìà Key Metrics:\"\n  echo \"  Total queue depth: $QUEUE_DEPTH\"\n  echo \"  Posts today: $POSTS_TODAY\"\n  echo \"  Health status: $HEALTH_STATUS\"\n  \nelse\n  echo \"‚ö†Ô∏è Could not collect metrics (status $METRICS_HTTP_STATUS)\"\n  echo '{\"error\": \"metrics_unavailable\", \"timestamp\": \"'$(date -Iseconds)'\"}' > metrics_snapshot.json\n  echo \"snapshot=failed\" >> $GITHUB_OUTPUT\nfi\n"
          }
        ]
      },
      "refill-check": {
        "runs-on": "ubuntu-latest",
        "name": "Two-Day Refill Validation",
        "needs": "health-check",
        "if": "needs.health-check.outputs.health-status != 'failed' && inputs.skip_refill_check != true",
        "timeout-minutes": 8,
        "outputs": {
          "refill-status": "${{ steps.refill.outputs.status }}",
          "diversity-score": "${{ steps.refill.outputs.diversity_score }}"
        },
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "20",
              "cache-key-suffix": "post-deploy-refill",
              "install-dependencies": "false"
            }
          },
          {
            "name": "Test two-day forecast endpoint",
            "id": "refill",
            "run": "echo \"üîÆ Testing two-day forecast capability...\"\n\n# Get today and tomorrow dates\nTODAY=$(date -Iseconds | cut -d'T' -f1)\nTOMORROW=$(date -d \"+1 day\" -Iseconds | cut -d'T' -f1)\n\necho \"Testing forecast for: $TODAY and $TOMORROW\"\n\n# Test today's forecast\nTODAY_RESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n  --max-time 30 \\\n  \"$PROD_URL/api/admin/schedule/forecast?date=$TODAY\")\n\nTODAY_HTTP_STATUS=$(echo $TODAY_RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nTODAY_BODY=$(echo $TODAY_RESPONSE | sed -e 's/HTTPSTATUS:.*//g')\n\n# Test tomorrow's forecast\nTOMORROW_RESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n  --max-time 30 \\\n  \"$PROD_URL/api/admin/schedule/forecast?date=$TOMORROW\")\n\nTOMORROW_HTTP_STATUS=$(echo $TOMORROW_RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nTOMORROW_BODY=$(echo $TOMORROW_RESPONSE | sed -e 's/HTTPSTATUS:.*//g')\n\necho \"Today forecast status: $TODAY_HTTP_STATUS\"\necho \"Tomorrow forecast status: $TOMORROW_HTTP_STATUS\"\n\n# Validate both days have 6 slots\nif [ \"$TODAY_HTTP_STATUS\" -eq 200 ] && [ \"$TOMORROW_HTTP_STATUS\" -eq 200 ]; then\n  echo \"‚úÖ Forecast endpoints responding\"\n  \n  # Count slots for today\n  TODAY_SLOTS=$(echo \"$TODAY_BODY\" | jq '.slots | length // 0')\n  TODAY_FILLED=$(echo \"$TODAY_BODY\" | jq '[.slots[] | select(.content_id != null)] | length // 0')\n  \n  # Count slots for tomorrow  \n  TOMORROW_SLOTS=$(echo \"$TOMORROW_BODY\" | jq '.slots | length // 0')\n  TOMORROW_FILLED=$(echo \"$TOMORROW_BODY\" | jq '[.slots[] | select(.content_id != null)] | length // 0')\n  \n  echo \"üìÖ Today: $TODAY_FILLED/$TODAY_SLOTS slots filled\"\n  echo \"üìÖ Tomorrow: $TOMORROW_FILLED/$TOMORROW_SLOTS slots filled\"\n  \n  # Check if we have 6/6 for both days\n  if [ \"$TODAY_SLOTS\" -eq 6 ] && [ \"$TOMORROW_SLOTS\" -eq 6 ]; then\n    echo \"‚úÖ Both days have 6 time slots configured\"\n    \n    # Check fill status - warn if not fully filled but don't fail\n    TOTAL_FILLED=$((TODAY_FILLED + TOMORROW_FILLED))\n    if [ \"$TOTAL_FILLED\" -ge 10 ]; then\n      echo \"‚úÖ Good fill rate: $TOTAL_FILLED/12 slots filled\"\n      echo \"status=good\" >> $GITHUB_OUTPUT\n    else\n      echo \"‚ö†Ô∏è Low fill rate: $TOTAL_FILLED/12 slots filled\"\n      echo \"status=warning\" >> $GITHUB_OUTPUT\n    fi\n    \n    # Extract diversity information for analysis\n    TODAY_DIVERSITY=$(echo \"$TODAY_BODY\" | jq '.summary.diversity_score // 0')\n    TOMORROW_DIVERSITY=$(echo \"$TOMORROW_BODY\" | jq '.summary.diversity_score // 0')\n    AVG_DIVERSITY=$(echo \"($TODAY_DIVERSITY + $TOMORROW_DIVERSITY) / 2\" | bc -l)\n    \n    echo \"üéØ Diversity scores: Today=$TODAY_DIVERSITY, Tomorrow=$TOMORROW_DIVERSITY, Avg=$AVG_DIVERSITY\"\n    echo \"diversity_score=$AVG_DIVERSITY\" >> $GITHUB_OUTPUT\n    \n    # Save forecast data for artifact\n    echo \"$TODAY_BODY\" > today_forecast.json\n    echo \"$TOMORROW_BODY\" > tomorrow_forecast.json\n    \n  else\n    echo \"‚ùå Incorrect slot configuration (expected 6/6)\"\n    echo \"status=failed\" >> $GITHUB_OUTPUT\n    exit 1\n  fi\n  \nelse\n  echo \"‚ùå Forecast endpoints failed\"\n  echo \"Today status: $TODAY_HTTP_STATUS\"\n  echo \"Tomorrow status: $TOMORROW_HTTP_STATUS\"\n  echo \"status=failed\" >> $GITHUB_OUTPUT\n  exit 1\nfi\n"
          },
          {
            "name": "Diversity policy evaluation (warn-only)",
            "run": "echo \"üåà Evaluating diversity policy...\"\n\nDIVERSITY_SCORE=\"${{ steps.refill.outputs.diversity_score }}\"\n\nif [ -n \"$DIVERSITY_SCORE\" ]; then\n  # Convert to integer percentage for comparison\n  DIVERSITY_PCT=$(echo \"$DIVERSITY_SCORE * 100\" | bc -l | cut -d'.' -f1)\n  \n  echo \"Diversity score: $DIVERSITY_SCORE ($DIVERSITY_PCT%)\"\n  \n  if [ \"$DIVERSITY_PCT\" -ge 80 ]; then\n    echo \"‚úÖ Excellent diversity ($DIVERSITY_PCT% >= 80%)\"\n  elif [ \"$DIVERSITY_PCT\" -ge 60 ]; then\n    echo \"üü° Good diversity ($DIVERSITY_PCT% >= 60%)\"\n  elif [ \"$DIVERSITY_PCT\" -ge 40 ]; then\n    echo \"üü† Moderate diversity ($DIVERSITY_PCT% >= 40%)\"\n  else\n    echo \"üî¥ Low diversity ($DIVERSITY_PCT% < 40%)\"\n    echo \"‚ö†Ô∏è Consider running content scans to improve platform diversity\"\n  fi\n  \n  # This is warn-only, so don't fail the deployment\n  echo \"üìã Diversity policy result: INFORMATIONAL (warn-only)\"\nelse\n  echo \"‚ö†Ô∏è Could not calculate diversity score\"\nfi\n"
          }
        ]
      },
      "upload-artifacts": {
        "runs-on": "ubuntu-latest",
        "name": "Upload Metrics & Artifacts",
        "needs": [
          "health-check",
          "refill-check"
        ],
        "if": "always()",
        "steps": [
          {
            "name": "Download artifacts from previous jobs",
            "run": "echo \"üì¶ Collecting deployment artifacts...\"\nmkdir -p deployment_artifacts\n\n# Create summary artifact\ncat > deployment_artifacts/deployment_summary.json << EOF\n{\n  \"timestamp\": \"$(date -Iseconds)\",\n  \"deployment_sha\": \"${{ github.sha }}\",\n  \"workflow_run\": \"${{ github.run_id }}\",\n  \"health_status\": \"${{ needs.health-check.outputs.health-status }}\",\n  \"refill_status\": \"${{ needs.refill-check.outputs.refill-status }}\",\n  \"diversity_score\": \"${{ needs.refill-check.outputs.diversity_score }}\",\n  \"environment\": \"production\",\n  \"success\": ${{ needs.health-check.outputs.health-status != 'failed' && needs.refill-check.outputs.refill-status != 'failed' }}\n}\nEOF\n"
          },
          {
            "name": "Collect current metrics snapshot",
            "run": "echo \"üìä Collecting final metrics snapshot...\"\n\nFINAL_METRICS=$(curl -s --max-time 15 \"$PROD_URL/api/system/metrics\" || echo '{\"error\": \"collection_failed\"}')\necho \"$FINAL_METRICS\" > deployment_artifacts/final_metrics_snapshot.json\n\n# Extract summary for logs\nif echo \"$FINAL_METRICS\" | jq -e '.timestamp' > /dev/null 2>&1; then\n  HEALTH_STATUS=$(echo \"$FINAL_METRICS\" | jq -r '.health_status // \"unknown\"')\n  TOTAL_QUEUE=$(echo \"$FINAL_METRICS\" | jq '[.queue_depth_by_platform | to_entries[]] | map(.value) | add // 0')\n  POSTS_TODAY=$(echo \"$FINAL_METRICS\" | jq '.posts_today // 0')\n  \n  echo \"üìà Final Metrics Summary:\"\n  echo \"  Health: $HEALTH_STATUS\"\n  echo \"  Queue Depth: $TOTAL_QUEUE\"\n  echo \"  Posts Today: $POSTS_TODAY\"\nelse\n  echo \"‚ö†Ô∏è Could not collect final metrics\"\nfi\n"
          },
          {
            "name": "Upload deployment artifacts",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "post-deploy-check-${{ github.sha }}",
              "path": "deployment_artifacts/",
              "retention-days": 30
            }
          },
          {
            "name": "Generate deployment report",
            "run": "echo \"üìã Post-Deploy Check Report\"\necho \"==========================\"\necho \"Deployment: ${{ github.sha }}\"\necho \"Timestamp: $(date -Iseconds)\"\necho \"Workflow: ${{ github.run_id }}\"\necho \"\"\necho \"Results:\"\necho \"  Health Check: ${{ needs.health-check.outputs.health-status }}\"\necho \"  Refill Check: ${{ needs.refill-check.outputs.refill-status }}\"\necho \"  Diversity Score: ${{ needs.refill-check.outputs.diversity_score }}\"\necho \"\"\n\nOVERALL_SUCCESS=\"${{ needs.health-check.outputs.health-status != 'failed' && needs.refill-check.outputs.refill-status != 'failed' }}\"\n\nif [ \"$OVERALL_SUCCESS\" = \"true\" ]; then\n  echo \"‚úÖ DEPLOYMENT VALIDATION PASSED\"\n  echo \"All post-deploy checks completed successfully\"\nelse\n  echo \"‚ùå DEPLOYMENT VALIDATION FAILED\"\n  echo \"One or more critical checks failed\"\n  exit 1\nfi\n"
          }
        ]
      },
      "notification": {
        "runs-on": "ubuntu-latest",
        "name": "Post-Deploy Notification",
        "needs": [
          "health-check",
          "refill-check",
          "upload-artifacts"
        ],
        "if": "always()",
        "steps": [
          {
            "name": "‚úÖ Success Summary",
            "if": "needs.health-check.outputs.health-status != 'failed' && needs.refill-check.outputs.refill-status != 'failed'",
            "run": "echo \"üéâ POST-DEPLOY VALIDATION SUCCESS\"\necho \"==================================\"\necho \"‚úÖ Health Check: ${{ needs.health-check.outputs.health-status }}\"\necho \"‚úÖ Refill Check: ${{ needs.refill-check.outputs.refill-status }}\"\necho \"üìä Diversity Score: ${{ needs.refill-check.outputs.diversity_score }}\"\necho \"\"\necho \"üöÄ System Status: All validation checks passed\"\necho \"üåê Production URL: ${{ env.PROD_URL }}\"\necho \"üì¶ Artifacts: Metrics snapshot and deployment data uploaded\"\necho \"\"\necho \"## ‚úÖ Post-Deploy Validation Success\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Deployment**: \\`${{ github.sha }}\\`\" >> $GITHUB_STEP_SUMMARY\necho \"**Timestamp**: $(date -u -Iseconds)\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"### üìä Validation Results\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"- **Health Check**: ${{ needs.health-check.outputs.health-status }}\" >> $GITHUB_STEP_SUMMARY\necho \"- **Refill Check**: ${{ needs.refill-check.outputs.refill-status }}\" >> $GITHUB_STEP_SUMMARY\necho \"- **Diversity Score**: ${{ needs.refill-check.outputs.diversity_score }}\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"üéâ All post-deployment validation checks passed successfully!\" >> $GITHUB_STEP_SUMMARY\n  \n"
          },
          {
            "name": "Create failure notification",
            "if": "needs.health-check.outputs.health-status == 'failed' || needs.refill-check.outputs.refill-status == 'failed'",
            "uses": "actions/github-script@v7",
            "with": {
              "script": "const title = `üö® Post-Deploy Check Failed - ${new Date().toISOString().split('T')[0]}`\nconst body = `## Post-Deploy Validation Failure\n\n**Deployment**: \\`${{ github.sha }}\\`  \n**Timestamp**: ${new Date().toISOString()}  \n**Workflow**: [${{ github.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})\n\n### ‚ùå Validation Results\n\n- **Health Check**: ${{ needs.health-check.outputs.health-status }}\n- **Refill Check**: ${{ needs.refill-check.outputs.refill-status }}\n\n### üö® Immediate Actions Required\n\n1. **Check workflow logs** for specific failure details\n2. **Verify system health** manually via admin panel\n3. **Test critical endpoints** to isolate issues:\n   - Health: ${{ env.PROD_URL }}/api/admin/health/deep\n   - Metrics: ${{ env.PROD_URL }}/api/system/metrics\n   - Forecast: ${{ env.PROD_URL }}/api/admin/schedule/forecast\n4. **Consider rollback** if critical functionality is impacted\n\n### üîç Investigation\n\nCheck the workflow artifacts for detailed metrics and diagnostic data.\n\n### üìû Escalation\n\nIf the issue persists or affects user-facing functionality, escalate immediately.\n\n/cc @devops-team @on-call\n\n---\n*This alert was generated automatically by the post-deploy check workflow.*\n`\n\ngithub.rest.issues.create({\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  title: title,\n  body: body,\n  labels: ['deployment', 'failure', 'urgent', 'automated', 'post-deploy']\n})\n"
            }
          }
        ]
      }
    },
    "intent": "Posts scheduled content at configured time",
    "category": "posting"
  },
  {
    "filename": "post-dinner.yml",
    "name": "Post Dinner Content",
    "on": {
      "schedule": [
        {
          "cron": "0 18 * * *"
        }
      ],
      "workflow_dispatch": null
    },
    "jobs": {
      "post-dinner": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Post Dinner Content",
            "run": "echo \"üçΩÔ∏è Posting dinner content...\"\n\n# Store response in variable for error analysis\nRESPONSE=$(curl -s -w \"HTTP_CODE:%{http_code}\" -L -X POST \"${{ secrets.SITE_URL }}/api/admin/posting/post-now\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"immediate\": true}' \\\n  --retry 2 --retry-delay 10 2>&1 || true)\n\n# Extract HTTP code and response body\nHTTP_CODE=$(echo \"$RESPONSE\" | grep -o 'HTTP_CODE:[0-9]*' | cut -d':' -f2)\nRESPONSE_BODY=$(echo \"$RESPONSE\" | sed 's/HTTP_CODE:[0-9]*$//')\n\necho \"HTTP Status: $HTTP_CODE\"\necho \"Response: $RESPONSE_BODY\"\n\n# Check for specific error conditions\nif [[ \"$HTTP_CODE\" == \"401\" ]]; then\n  echo \"‚ùå CRITICAL: Authentication failed - AUTH_TOKEN is invalid or expired\"\n  echo \"This indicates a security breach recovery issue - AUTH_TOKEN needs to be updated in GitHub secrets\"\n  exit 1\nelif [[ \"$HTTP_CODE\" != \"200\" ]]; then\n  echo \"‚ùå FAILED: API returned HTTP $HTTP_CODE\"\n  echo \"Response body: $RESPONSE_BODY\"\n  exit 1\nelse\n  echo \"‚úÖ SUCCESS: Dinner content posted (HTTP $HTTP_CODE)\"\nfi\n"
          },
          {
            "name": "Verify Post Success",
            "if": "success()",
            "run": "echo \"‚úÖ Dinner content posted successfully\"\n"
          },
          {
            "name": "Handle Failure",
            "if": "failure()",
            "run": "echo \"‚ùå CRITICAL: Dinner posting failed - this may indicate:\"\necho \"  1. AUTH_TOKEN is invalid/expired (most common after security breach)\"\necho \"  2. API endpoint is down\"\necho \"  3. No approved content available\"\necho \"  4. Database connectivity issues\"\necho \"\"\necho \"üîß TROUBLESHOOTING STEPS:\"\necho \"  1. Check AUTH_TOKEN in GitHub secrets matches current production token\"\necho \"  2. Verify Vercel deployment is live\"\necho \"  3. Check content queue has approved posts available\"\necho \"  4. Review API logs in Vercel dashboard\"\necho \"\"\necho \"üìû IMMEDIATE ACTIONS REQUIRED:\"\necho \"  - Update AUTH_TOKEN in GitHub secrets if authentication failed\"\necho \"  - Run content approval if queue is empty\"\necho \"  - Check Vercel deployment status\"\n"
          }
        ]
      }
    },
    "intent": "Posts scheduled content at 18:00 ET (dinner slot)",
    "category": "posting"
  },
  {
    "filename": "post-evening.yml",
    "name": "Post Evening Content",
    "on": {
      "schedule": [
        {
          "cron": "0 20 * * *"
        }
      ],
      "workflow_dispatch": null
    },
    "jobs": {
      "post-evening": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Post Evening Content",
            "run": "echo \"üåô Posting evening content...\"\n\n# Store response in variable for error analysis\nRESPONSE=$(curl -s -w \"HTTP_CODE:%{http_code}\" -L -X POST \"${{ secrets.SITE_URL }}/api/admin/posting/post-now\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"immediate\": true}' \\\n  --retry 2 --retry-delay 10 2>&1 || true)\n\n# Extract HTTP code and response body\nHTTP_CODE=$(echo \"$RESPONSE\" | grep -o 'HTTP_CODE:[0-9]*' | cut -d':' -f2)\nRESPONSE_BODY=$(echo \"$RESPONSE\" | sed 's/HTTP_CODE:[0-9]*$//')\n\necho \"HTTP Status: $HTTP_CODE\"\necho \"Response: $RESPONSE_BODY\"\n\n# Check for specific error conditions\nif [[ \"$HTTP_CODE\" == \"401\" ]]; then\n  echo \"‚ùå CRITICAL: Authentication failed - AUTH_TOKEN is invalid or expired\"\n  echo \"This indicates a security breach recovery issue - AUTH_TOKEN needs to be updated in GitHub secrets\"\n  exit 1\nelif [[ \"$HTTP_CODE\" != \"200\" ]]; then\n  echo \"‚ùå FAILED: API returned HTTP $HTTP_CODE\"\n  echo \"Response body: $RESPONSE_BODY\"\n  exit 1\nelse\n  echo \"‚úÖ SUCCESS: Evening content posted (HTTP $HTTP_CODE)\"\nfi\n"
          },
          {
            "name": "Verify Post Success",
            "if": "success()",
            "run": "echo \"‚úÖ Evening content posted successfully\"\n"
          },
          {
            "name": "Handle Failure",
            "if": "failure()",
            "run": "echo \"‚ùå CRITICAL: Evening posting failed - this may indicate:\"\necho \"  1. AUTH_TOKEN is invalid/expired (most common after security breach)\"\necho \"  2. API endpoint is down\"\necho \"  3. No approved content available\"\necho \"  4. Database connectivity issues\"\necho \"\"\necho \"üîß TROUBLESHOOTING STEPS:\"\necho \"  1. Check AUTH_TOKEN in GitHub secrets matches current production token\"\necho \"  2. Verify Vercel deployment is live\"\necho \"  3. Check content queue has approved posts available\"\necho \"  4. Review API logs in Vercel dashboard\"\necho \"\"\necho \"üìû IMMEDIATE ACTIONS REQUIRED:\"\necho \"  - Update AUTH_TOKEN in GitHub secrets if authentication failed\"\necho \"  - Run content approval if queue is empty\"\necho \"  - Check Vercel deployment status\"\n"
          }
        ]
      }
    },
    "intent": "Posts scheduled content at 21:00 ET (evening slot)",
    "category": "posting"
  },
  {
    "filename": "post-late-night.yml",
    "name": "Post Late Night Content",
    "on": {
      "schedule": [
        {
          "cron": "30 22 * * *"
        }
      ],
      "workflow_dispatch": null
    },
    "jobs": {
      "post-late-night": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Post Late Night Content",
            "run": "echo \"üåÉ Posting late night content...\"\n\n# Store response in variable for error analysis\nRESPONSE=$(curl -s -w \"HTTP_CODE:%{http_code}\" -L -X POST \"${{ secrets.SITE_URL }}/api/admin/posting/post-now\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"immediate\": true}' \\\n  --retry 2 --retry-delay 10 2>&1 || true)\n\n# Extract HTTP code and response body\nHTTP_CODE=$(echo \"$RESPONSE\" | grep -o 'HTTP_CODE:[0-9]*' | cut -d':' -f2)\nRESPONSE_BODY=$(echo \"$RESPONSE\" | sed 's/HTTP_CODE:[0-9]*$//')\n\necho \"HTTP Status: $HTTP_CODE\"\necho \"Response: $RESPONSE_BODY\"\n\n# Check for specific error conditions\nif [[ \"$HTTP_CODE\" == \"401\" ]]; then\n  echo \"‚ùå CRITICAL: Authentication failed - AUTH_TOKEN is invalid or expired\"\n  echo \"This indicates a security breach recovery issue - AUTH_TOKEN needs to be updated in GitHub secrets\"\n  exit 1\nelif [[ \"$HTTP_CODE\" != \"200\" ]]; then\n  echo \"‚ùå FAILED: API returned HTTP $HTTP_CODE\"\n  echo \"Response body: $RESPONSE_BODY\"\n  exit 1\nelse\n  echo \"‚úÖ SUCCESS: Late night content posted (HTTP $HTTP_CODE)\"\nfi\n"
          },
          {
            "name": "Verify Post Success",
            "if": "success()",
            "run": "echo \"‚úÖ Late night content posted successfully\"\n"
          },
          {
            "name": "Handle Failure",
            "if": "failure()",
            "run": "echo \"‚ùå CRITICAL: Late night posting failed - this may indicate:\"\necho \"  1. AUTH_TOKEN is invalid/expired (most common after security breach)\"\necho \"  2. API endpoint is down\"\necho \"  3. No approved content available\"\necho \"  4. Database connectivity issues\"\necho \"\"\necho \"üîß TROUBLESHOOTING STEPS:\"\necho \"  1. Check AUTH_TOKEN in GitHub secrets matches current production token\"\necho \"  2. Verify Vercel deployment is live\"\necho \"  3. Check content queue has approved posts available\"\necho \"  4. Review API logs in Vercel dashboard\"\necho \"\"\necho \"üìû IMMEDIATE ACTIONS REQUIRED:\"\necho \"  - Update AUTH_TOKEN in GitHub secrets if authentication failed\"\necho \"  - Run content approval if queue is empty\"\necho \"  - Check Vercel deployment status\"\n"
          }
        ]
      }
    },
    "intent": "Posts scheduled content at 23:30 ET (late night slot)",
    "category": "posting"
  },
  {
    "filename": "post-lunch.yml",
    "name": "Post Lunch Content",
    "on": {
      "schedule": [
        {
          "cron": "0 12 * * *"
        }
      ],
      "workflow_dispatch": null
    },
    "jobs": {
      "post-lunch": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Post Lunch Content",
            "run": "echo \"ü•™ Posting lunch content...\"\n\n# Store response in variable for error analysis\nRESPONSE=$(curl -s -w \"HTTP_CODE:%{http_code}\" -L -X POST \"${{ secrets.SITE_URL }}/api/admin/posting/post-now\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"immediate\": true}' \\\n  --retry 2 --retry-delay 10 2>&1 || true)\n\n# Extract HTTP code and response body\nHTTP_CODE=$(echo \"$RESPONSE\" | grep -o 'HTTP_CODE:[0-9]*' | cut -d':' -f2)\nRESPONSE_BODY=$(echo \"$RESPONSE\" | sed 's/HTTP_CODE:[0-9]*$//')\n\necho \"HTTP Status: $HTTP_CODE\"\necho \"Response: $RESPONSE_BODY\"\n\n# Check for specific error conditions\nif [[ \"$HTTP_CODE\" == \"401\" ]]; then\n  echo \"‚ùå CRITICAL: Authentication failed - AUTH_TOKEN is invalid or expired\"\n  echo \"This indicates a security breach recovery issue - AUTH_TOKEN needs to be updated in GitHub secrets\"\n  exit 1\nelif [[ \"$HTTP_CODE\" != \"200\" ]]; then\n  echo \"‚ùå FAILED: API returned HTTP $HTTP_CODE\"\n  echo \"Response body: $RESPONSE_BODY\"\n  exit 1\nelse\n  echo \"‚úÖ SUCCESS: Lunch content posted (HTTP $HTTP_CODE)\"\nfi\n"
          },
          {
            "name": "Verify Post Success",
            "if": "success()",
            "run": "echo \"‚úÖ Lunch content posted successfully\"\n"
          },
          {
            "name": "Handle Failure",
            "if": "failure()",
            "run": "echo \"‚ùå CRITICAL: Lunch posting failed - this may indicate:\"\necho \"  1. AUTH_TOKEN is invalid/expired (most common after security breach)\"\necho \"  2. API endpoint is down\"\necho \"  3. No approved content available\"\necho \"  4. Database connectivity issues\"\necho \"\"\necho \"üîß TROUBLESHOOTING STEPS:\"\necho \"  1. Check AUTH_TOKEN in GitHub secrets matches current production token\"\necho \"  2. Verify Vercel deployment is live\"\necho \"  3. Check content queue has approved posts available\"\necho \"  4. Review API logs in Vercel dashboard\"\necho \"\"\necho \"üìû IMMEDIATE ACTIONS REQUIRED:\"\necho \"  - Update AUTH_TOKEN in GitHub secrets if authentication failed\"\necho \"  - Run content approval if queue is empty\"\necho \"  - Check Vercel deployment status\"\n"
          }
        ]
      }
    },
    "intent": "Posts scheduled content at 12:00 ET (lunch slot)",
    "category": "posting"
  },
  {
    "filename": "post-remediation-check.yml",
    "name": "Post-Remediation Validation",
    "on": {
      "repository_dispatch": {
        "types": [
          "post-remediation-check"
        ]
      },
      "workflow_dispatch": {
        "inputs": {
          "manual_trigger": {
            "description": "Manually trigger post-remediation validation",
            "required": false,
            "type": "boolean",
            "default": false
          }
        }
      }
    },
    "env": {
      "NODE_OPTIONS": "--max-old-space-size=4096"
    },
    "jobs": {
      "validate-remediation": {
        "name": "Validate Phase 3 Remediation Results",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 10,
        "steps": [
          {
            "name": "Checkout repository",
            "uses": "actions/checkout@v4",
            "with": {
              "token": "${{ secrets.GITHUB_TOKEN }}",
              "fetch-depth": 1
            }
          },
          {
            "name": "üìä Extract Remediation Context",
            "id": "context",
            "run": "echo \"üîç Extracting remediation context from dispatch payload...\"\n\n# Extract data from repository_dispatch payload\nSECURITY_IMPROVEMENT=\"${{ github.event.client_payload.security_improvement }}\"\nBUILD_DIAGNOSTICS=\"${{ github.event.client_payload.build_diagnostics }}\"\nTRIGGERED_BY=\"${{ github.event.client_payload.triggered_by }}\"\nCOMMIT_SHA=\"${{ github.event.client_payload.commit_sha }}\"\nREMEDIATION_TIME=\"${{ github.event.client_payload.remediation_timestamp }}\"\n\n# Set defaults for manual triggers\nif [ \"${{ github.event_name }}\" = \"workflow_dispatch\" ]; then\n  SECURITY_IMPROVEMENT=\"unknown\"\n  BUILD_DIAGNOSTICS=\"unknown\"\n  TRIGGERED_BY=\"manual\"\n  COMMIT_SHA=\"${{ github.sha }}\"\n  REMEDIATION_TIME=\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"\nfi\n\necho \"security_improvement=$SECURITY_IMPROVEMENT\" >> $GITHUB_OUTPUT\necho \"build_diagnostics=$BUILD_DIAGNOSTICS\" >> $GITHUB_OUTPUT\necho \"triggered_by=$TRIGGERED_BY\" >> $GITHUB_OUTPUT\necho \"commit_sha=$COMMIT_SHA\" >> $GITHUB_OUTPUT\necho \"remediation_time=$REMEDIATION_TIME\" >> $GITHUB_OUTPUT\n\necho \"üìã Remediation Context:\"\necho \"- Security Improvement: $SECURITY_IMPROVEMENT\"\necho \"- Build Diagnostics: $BUILD_DIAGNOSTICS\"\necho \"- Triggered By: $TRIGGERED_BY\"\necho \"- Commit SHA: $COMMIT_SHA\"\necho \"- Remediation Time: $REMEDIATION_TIME\"\n"
          },
          {
            "name": "Setup Node.js with caching",
            "uses": "actions/setup-node@v4",
            "with": {
              "node-version": "18",
              "cache": "npm"
            }
          },
          {
            "name": "Install dependencies",
            "run": "npm ci --prefer-offline --no-audit --no-fund\nnpm install --no-save tsx\n"
          },
          {
            "name": "üß™ Re-run CI Health Checks",
            "id": "health-recheck",
            "run": "echo \"üß™ Re-running CI health checks to validate remediation...\"\n\n# Run the critical failure gatekeeper in report-only mode\nnpx tsx scripts/checkCriticalFailures.ts --report-only || true\n\n# Extract health metrics from the report\nif [[ -f \"reports/ci-health-gate.md\" ]]; then\n  echo \"health_report_generated=true\" >> $GITHUB_OUTPUT\n  \n  # Extract confidence score\n  if grep -q \"Confidence Score:\" reports/ci-health-gate.md; then\n    CONFIDENCE=$(grep \"Confidence Score:\" reports/ci-health-gate.md | head -1 | grep -oE '[0-9]+' || echo \"0\")\n    echo \"confidence_score=$CONFIDENCE\" >> $GITHUB_OUTPUT\n    echo \"üìä Current confidence score: $CONFIDENCE/100\"\n  fi\n  \n  # Check CI readiness\n  if grep -q \"CI Readiness: ‚úÖ Ready\" reports/ci-health-gate.md; then\n    echo \"ci_ready=true\" >> $GITHUB_OUTPUT\n    echo \"‚úÖ CI is ready to proceed\"\n  else\n    echo \"ci_ready=false\" >> $GITHUB_OUTPUT\n    echo \"‚ùå CI is still blocked\"\n  fi\n  \n  # Check for remaining blockers\n  BLOCKERS=$(grep -c \"Blocking Issues:\" reports/ci-health-gate.md || echo \"0\")\n  echo \"blocking_issues=$BLOCKERS\" >> $GITHUB_OUTPUT\nelse\n  echo \"health_report_generated=false\" >> $GITHUB_OUTPUT\n  echo \"ci_ready=false\" >> $GITHUB_OUTPUT\n  echo \"confidence_score=0\" >> $GITHUB_OUTPUT\nfi\n"
          },
          {
            "name": "üîç Validate Specific Components",
            "id": "component-validation",
            "run": "echo \"üîç Validating specific components after remediation...\"\n\n# Test lint status\necho \"üìù Testing lint status...\"\nif npm run lint:check 2>/dev/null || true; then\n  echo \"lint_status=pass\" >> $GITHUB_OUTPUT\n  echo \"‚úÖ Lint checks passing\"\nelse\n  echo \"lint_status=fail\" >> $GITHUB_OUTPUT\n  echo \"‚ùå Lint checks still failing\"\nfi\n\n# Test security status (basic check)\necho \"üîí Testing security status...\"\nif npm audit --audit-level=critical 2>/dev/null; then\n  echo \"security_status=pass\" >> $GITHUB_OUTPUT\n  echo \"‚úÖ No critical security vulnerabilities\"\nelse\n  echo \"security_status=fail\" >> $GITHUB_OUTPUT\n  echo \"‚ö†Ô∏è Critical security vulnerabilities remain\"\nfi\n\n# Test build status\necho \"üèóÔ∏è Testing build status...\"\nif timeout 120 npm run build 2>/dev/null; then\n  echo \"build_status=pass\" >> $GITHUB_OUTPUT\n  echo \"‚úÖ Build completed successfully\"\nelse\n  echo \"build_status=fail\" >> $GITHUB_OUTPUT\n  echo \"‚ùå Build still failing\"\nfi\n"
          },
          {
            "name": "üìà Calculate Remediation Effectiveness",
            "id": "effectiveness",
            "run": "echo \"üìà Calculating remediation effectiveness...\"\n\n# Get current health metrics\nCONFIDENCE=\"${{ steps.health-recheck.outputs.confidence_score }}\"\nCI_READY=\"${{ steps.health-recheck.outputs.ci_ready }}\"\nLINT_STATUS=\"${{ steps.component-validation.outputs.lint_status }}\"\nSECURITY_STATUS=\"${{ steps.component-validation.outputs.security_status }}\"\nBUILD_STATUS=\"${{ steps.component-validation.outputs.build_status }}\"\n\n# Calculate effectiveness score\nEFFECTIVENESS=0\n\n# Base score from confidence\nif [[ \"$CONFIDENCE\" =~ ^[0-9]+$ ]]; then\n  EFFECTIVENESS=$CONFIDENCE\nfi\n\n# Bonus points for component health\n[[ \"$LINT_STATUS\" == \"pass\" ]] && EFFECTIVENESS=$((EFFECTIVENESS + 5))\n[[ \"$SECURITY_STATUS\" == \"pass\" ]] && EFFECTIVENESS=$((EFFECTIVENESS + 10))\n[[ \"$BUILD_STATUS\" == \"pass\" ]] && EFFECTIVENESS=$((EFFECTIVENESS + 15))\n[[ \"$CI_READY\" == \"true\" ]] && EFFECTIVENESS=$((EFFECTIVENESS + 20))\n\n# Cap at 100\n[[ $EFFECTIVENESS -gt 100 ]] && EFFECTIVENESS=100\n\necho \"effectiveness_score=$EFFECTIVENESS\" >> $GITHUB_OUTPUT\necho \"üìä Remediation effectiveness: $EFFECTIVENESS/100\"\n\n# Determine overall status\nif [[ $EFFECTIVENESS -ge 80 && \"$CI_READY\" == \"true\" ]]; then\n  echo \"overall_status=success\" >> $GITHUB_OUTPUT\n  echo \"‚úÖ Remediation was highly effective\"\nelif [[ $EFFECTIVENESS -ge 60 ]]; then\n  echo \"overall_status=partial\" >> $GITHUB_OUTPUT\n  echo \"‚ö†Ô∏è Remediation was partially effective\"\nelse\n  echo \"overall_status=failed\" >> $GITHUB_OUTPUT\n  echo \"‚ùå Remediation was not effective\"\nfi\n"
          },
          {
            "name": "üö® Check Rollback Requirements",
            "id": "rollback-check",
            "if": "steps.effectiveness.outputs.effectiveness_score < 30",
            "run": "echo \"üö® Checking if rollback is required...\"\n\nEFFECTIVENESS=\"${{ steps.effectiveness.outputs.effectiveness_score }}\"\nCI_READY=\"${{ steps.health-recheck.outputs.ci_ready }}\"\n\necho \"Current effectiveness: $EFFECTIVENESS/100\"\necho \"CI Ready: $CI_READY\"\n\n# Determine if rollback is needed\nif [[ $EFFECTIVENESS -lt 30 && \"$CI_READY\" == \"false\" ]]; then\n  echo \"rollback_required=true\" >> $GITHUB_OUTPUT\n  echo \"üö® ROLLBACK REQUIRED: Health critically low after remediation\"\n  echo \"rollback_reason=Health score $EFFECTIVENESS < 30 and CI still blocked\" >> $GITHUB_OUTPUT\nelse\n  echo \"rollback_required=false\" >> $GITHUB_OUTPUT\n  echo \"‚úÖ No rollback required\"\nfi\n"
          },
          {
            "name": "üìä Generate Validation Summary",
            "run": "echo \"üìä Post-Remediation Validation Summary\" >> $GITHUB_STEP_SUMMARY\necho \"=======================================\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\necho \"### üéØ Remediation Context\" >> $GITHUB_STEP_SUMMARY\necho \"- **Triggered By:** ${{ steps.context.outputs.triggered_by }}\" >> $GITHUB_STEP_SUMMARY\necho \"- **Commit SHA:** ${{ steps.context.outputs.commit_sha }}\" >> $GITHUB_STEP_SUMMARY\necho \"- **Remediation Time:** ${{ steps.context.outputs.remediation_time }}\" >> $GITHUB_STEP_SUMMARY\necho \"- **Security Improvement:** ${{ steps.context.outputs.security_improvement }}\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\necho \"### üìà Validation Results\" >> $GITHUB_STEP_SUMMARY\necho \"- **Overall Status:** ${{ steps.effectiveness.outputs.overall_status }}\" >> $GITHUB_STEP_SUMMARY\necho \"- **Effectiveness Score:** ${{ steps.effectiveness.outputs.effectiveness_score }}/100\" >> $GITHUB_STEP_SUMMARY\necho \"- **Confidence Score:** ${{ steps.health-recheck.outputs.confidence_score }}/100\" >> $GITHUB_STEP_SUMMARY\necho \"- **CI Ready:** ${{ steps.health-recheck.outputs.ci_ready == 'true' && '‚úÖ Yes' || '‚ùå No' }}\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\necho \"### üîç Component Status\" >> $GITHUB_STEP_SUMMARY\necho \"- **Lint:** ${{ steps.component-validation.outputs.lint_status == 'pass' && '‚úÖ Pass' || '‚ùå Fail' }}\" >> $GITHUB_STEP_SUMMARY\necho \"- **Security:** ${{ steps.component-validation.outputs.security_status == 'pass' && '‚úÖ Pass' || '‚ùå Fail' }}\" >> $GITHUB_STEP_SUMMARY\necho \"- **Build:** ${{ steps.component-validation.outputs.build_status == 'pass' && '‚úÖ Pass' || '‚ùå Fail' }}\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\nif [[ \"${{ steps.rollback-check.outputs.rollback_required }}\" == \"true\" ]]; then\n  echo \"### üö® Rollback Required\" >> $GITHUB_STEP_SUMMARY\n  echo \"**Reason:** ${{ steps.rollback-check.outputs.rollback_reason }}\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\nfi\n\necho \"### üìã Next Actions\" >> $GITHUB_STEP_SUMMARY\nif [[ \"${{ steps.effectiveness.outputs.overall_status }}\" == \"success\" ]]; then\n  echo \"‚úÖ **Remediation successful** - CI can proceed normally\" >> $GITHUB_STEP_SUMMARY\nelif [[ \"${{ steps.effectiveness.outputs.overall_status }}\" == \"partial\" ]]; then\n  echo \"‚ö†Ô∏è **Partial success** - Review remaining issues and consider manual fixes\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"‚ùå **Remediation failed** - Manual intervention required\" >> $GITHUB_STEP_SUMMARY\n  if [[ \"${{ steps.rollback-check.outputs.rollback_required }}\" == \"true\" ]]; then\n    echo \"üö® **Consider rollback** - Health critically low\" >> $GITHUB_STEP_SUMMARY\n  fi\nfi\n"
          },
          {
            "name": "Upload Post-Remediation Reports",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "post-remediation-validation-reports",
              "path": "reports/ci-health-gate.md\nreports/build-diagnostics.md\nreports/security-deep-fix.md\n",
              "retention-days": 30
            }
          },
          {
            "name": "üìù Update Original PR with Validation Results",
            "if": "github.event_name == 'repository_dispatch' && github.event.client_payload.commit_sha",
            "env": {
              "GH_TOKEN": "${{ secrets.GITHUB_TOKEN }}"
            },
            "run": "echo \"üìù Updating original PR with validation results...\"\n\n# Find PR associated with the commit SHA\nCOMMIT_SHA=\"${{ steps.context.outputs.commit_sha }}\"\n\n# Create validation results comment\ncat > validation_comment.md << EOF\n## üß™ Post-Remediation Validation Results\n\n**Validation Time:** $(date -u +%Y-%m-%dT%H:%M:%SZ)  \n**Triggered By:** ${{ steps.context.outputs.triggered_by }}\n\n### üìä Results Summary\n- **Overall Status:** ${{ steps.effectiveness.outputs.overall_status == 'success' && '‚úÖ Success' || steps.effectiveness.outputs.overall_status == 'partial' && '‚ö†Ô∏è Partial' || '‚ùå Failed' }}\n- **Effectiveness Score:** ${{ steps.effectiveness.outputs.effectiveness_score }}/100\n- **CI Ready:** ${{ steps.health-recheck.outputs.ci_ready == 'true' && '‚úÖ Yes' || '‚ùå No' }}\n\n### üîç Component Validation\n- **Lint:** ${{ steps.component-validation.outputs.lint_status == 'pass' && '‚úÖ' || '‚ùå' }}\n- **Security:** ${{ steps.component-validation.outputs.security_status == 'pass' && '‚úÖ' || '‚ùå' }}\n- **Build:** ${{ steps.component-validation.outputs.build_status == 'pass' && '‚úÖ' || '‚ùå' }}\n\n${{ steps.rollback-check.outputs.rollback_required == 'true' && '### üö® Rollback Required\\n**Reason:** Health critically low after remediation\\n' || '' }}\n\n> **Note:** This validation ran independently after Phase 3 auto-healing completed.\nEOF\n\necho \"‚úÖ Validation results prepared for PR update\"\n"
          },
          {
            "name": "üéØ Final Validation Status",
            "run": "echo \"üéØ Post-Remediation Validation Complete\"\necho \"=======================================\"\necho \"\"\necho \"üìä Overall Status: ${{ steps.effectiveness.outputs.overall_status }}\"\necho \"üìà Effectiveness: ${{ steps.effectiveness.outputs.effectiveness_score }}/100\"\necho \"üöÄ CI Ready: ${{ steps.health-recheck.outputs.ci_ready }}\"\necho \"\"\n\nif [[ \"${{ steps.effectiveness.outputs.overall_status }}\" == \"success\" ]]; then\n  echo \"‚úÖ Phase 3 auto-healing was successful!\"\n  echo \"üéâ CI pipeline can proceed normally\"\n  exit 0\nelif [[ \"${{ steps.effectiveness.outputs.overall_status }}\" == \"partial\" ]]; then\n  echo \"‚ö†Ô∏è Phase 3 auto-healing was partially successful\"\n  echo \"üîß Some manual intervention may be needed\"\n  exit 0\nelse\n  echo \"‚ùå Phase 3 auto-healing was not effective\"\n  echo \"üö® Manual intervention required\"\n  if [[ \"${{ steps.rollback-check.outputs.rollback_required }}\" == \"true\" ]]; then\n    echo \"üîÑ Consider rollback due to critically low health\"\n  fi\n  exit 1\nfi\n"
          }
        ]
      }
    },
    "intent": "Posts scheduled content at configured time",
    "category": "posting"
  },
  {
    "filename": "post-snack.yml",
    "name": "Post Afternoon Snack",
    "on": {
      "schedule": [
        {
          "cron": "0 15 * * *"
        }
      ],
      "workflow_dispatch": null
    },
    "jobs": {
      "post-snack": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Post Snack Content",
            "run": "echo \"üçø Posting afternoon snack content...\"\n\n# Store response in variable for error analysis\nRESPONSE=$(curl -s -w \"HTTP_CODE:%{http_code}\" -L -X POST \"${{ secrets.SITE_URL }}/api/admin/posting/post-now\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"immediate\": true}' \\\n  --retry 2 --retry-delay 10 2>&1 || true)\n\n# Extract HTTP code and response body\nHTTP_CODE=$(echo \"$RESPONSE\" | grep -o 'HTTP_CODE:[0-9]*' | cut -d':' -f2)\nRESPONSE_BODY=$(echo \"$RESPONSE\" | sed 's/HTTP_CODE:[0-9]*$//')\n\necho \"HTTP Status: $HTTP_CODE\"\necho \"Response: $RESPONSE_BODY\"\n\n# Check for specific error conditions\nif [[ \"$HTTP_CODE\" == \"401\" ]]; then\n  echo \"‚ùå CRITICAL: Authentication failed - AUTH_TOKEN is invalid or expired\"\n  echo \"This indicates a security breach recovery issue - AUTH_TOKEN needs to be updated in GitHub secrets\"\n  exit 1\nelif [[ \"$HTTP_CODE\" != \"200\" ]]; then\n  echo \"‚ùå FAILED: API returned HTTP $HTTP_CODE\"\n  echo \"Response body: $RESPONSE_BODY\"\n  exit 1\nelse\n  echo \"‚úÖ SUCCESS: Snack content posted (HTTP $HTTP_CODE)\"\nfi\n"
          },
          {
            "name": "Verify Post Success",
            "if": "success()",
            "run": "echo \"‚úÖ Snack content posted successfully\"\n"
          },
          {
            "name": "Handle Failure",
            "if": "failure()",
            "run": "echo \"‚ùå CRITICAL: Snack posting failed - this may indicate:\"\necho \"  1. AUTH_TOKEN is invalid/expired (most common after security breach)\"\necho \"  2. API endpoint is down\"\necho \"  3. No approved content available\"\necho \"  4. Database connectivity issues\"\necho \"\"\necho \"üîß TROUBLESHOOTING STEPS:\"\necho \"  1. Check AUTH_TOKEN in GitHub secrets matches current production token\"\necho \"  2. Verify Vercel deployment is live\"\necho \"  3. Check content queue has approved posts available\"\necho \"  4. Review API logs in Vercel dashboard\"\necho \"\"\necho \"üìû IMMEDIATE ACTIONS REQUIRED:\"\necho \"  - Update AUTH_TOKEN in GitHub secrets if authentication failed\"\necho \"  - Run content approval if queue is empty\"\necho \"  - Check Vercel deployment status\"\n"
          }
        ]
      }
    },
    "intent": "Posts scheduled content at 15:00 ET (snack slot)",
    "category": "posting"
  },
  {
    "filename": "post.yml",
    "name": "Content Posting",
    "on": {
      "schedule": [
        {
          "cron": "0 13 * * *"
        },
        {
          "cron": "0 17 * * *"
        },
        {
          "cron": "0 20 * * *"
        },
        {
          "cron": "0 23 * * *"
        },
        {
          "cron": "0 2 * * *"
        },
        {
          "cron": "30 4 * * *"
        }
      ],
      "workflow_dispatch": {
        "inputs": {
          "slot": {
            "description": "Time slot to post (breakfast, lunch, snack, dinner, evening, late-night, or manual)",
            "type": "choice",
            "options": [
              "manual",
              "breakfast",
              "lunch",
              "snack",
              "dinner",
              "evening",
              "late-night"
            ],
            "default": "manual"
          },
          "dry-run": {
            "description": "Dry run mode (preview only)",
            "type": "boolean",
            "default": false
          }
        }
      },
      "workflow_call": {
        "inputs": {
          "slot": {
            "description": "Time slot to post",
            "type": "string",
            "default": "manual"
          },
          "dry-run": {
            "description": "Dry run mode",
            "type": "boolean",
            "default": false
          }
        }
      }
    },
    "concurrency": {
      "group": "posting-${{ github.ref }}-${{ inputs.slot || 'scheduled' }}",
      "cancel-in-progress": false
    },
    "env": {
      "NODE_ENV": "production",
      "CI": true
    },
    "jobs": {
      "determine-slot": {
        "name": "Determine Posting Slot",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 2,
        "outputs": {
          "slot": "${{ steps.strategy.outputs.slot }}",
          "slot-index": "${{ steps.strategy.outputs.slot-index }}",
          "dry-run": "${{ steps.strategy.outputs.dry-run }}",
          "auth-token": "${{ steps.strategy.outputs.auth-token }}"
        },
        "steps": [
          {
            "name": "Determine posting strategy",
            "id": "strategy",
            "run": "SLOT=\"${{ inputs.slot || 'manual' }}\"\nDRY_RUN=\"${{ inputs.dry-run || 'false' }}\"\nSLOT_INDEX=\"0\"\n\n# Determine slot based on schedule\nif [[ \"${{ github.event_name }}\" == \"schedule\" ]]; then\n  HOUR=$(date +%H)\n  MINUTE=$(date +%M)\n  \n  if [[ $HOUR -eq 13 && $MINUTE -eq 0 ]]; then\n    SLOT=\"breakfast\"\n    SLOT_INDEX=\"0\"\n    echo \"üåÖ Breakfast posting slot (8:00 AM ET)\"\n  elif [[ $HOUR -eq 17 && $MINUTE -eq 0 ]]; then\n    SLOT=\"lunch\"\n    SLOT_INDEX=\"1\"\n    echo \"üçΩÔ∏è Lunch posting slot (12:00 PM ET)\"\n  elif [[ $HOUR -eq 20 && $MINUTE -eq 0 ]]; then\n    SLOT=\"snack\"\n    SLOT_INDEX=\"2\"\n    echo \"üçø Snack posting slot (3:00 PM ET)\"\n  elif [[ $HOUR -eq 23 && $MINUTE -eq 0 ]]; then\n    SLOT=\"dinner\"\n    SLOT_INDEX=\"3\"\n    echo \"üçΩÔ∏è Dinner posting slot (6:00 PM ET)\"\n  elif [[ $HOUR -eq 2 && $MINUTE -eq 0 ]]; then\n    SLOT=\"evening\"\n    SLOT_INDEX=\"4\"\n    echo \"üåÜ Evening posting slot (9:00 PM ET)\"\n  elif [[ $HOUR -eq 4 && $MINUTE -eq 30 ]]; then\n    SLOT=\"late-night\"\n    SLOT_INDEX=\"5\"\n    echo \"üåô Late night posting slot (11:30 PM ET)\"\n  fi\nelse\n  # Map manual slots to indices\n  case \"$SLOT\" in\n    breakfast) SLOT_INDEX=\"0\" ;;\n    lunch) SLOT_INDEX=\"1\" ;;\n    snack) SLOT_INDEX=\"2\" ;;\n    dinner) SLOT_INDEX=\"3\" ;;\n    evening) SLOT_INDEX=\"4\" ;;\n    late-night) SLOT_INDEX=\"5\" ;;\n    manual) SLOT_INDEX=\"6\" ;;\n  esac\nfi\n\necho \"slot=$SLOT\" >> $GITHUB_OUTPUT\necho \"slot-index=$SLOT_INDEX\" >> $GITHUB_OUTPUT\necho \"dry-run=$DRY_RUN\" >> $GITHUB_OUTPUT\necho \"auth-token=${{ secrets.AUTH_TOKEN }}\" >> $GITHUB_OUTPUT\necho \"Selected slot: $SLOT (index: $SLOT_INDEX), dry-run: $DRY_RUN\"\n"
          }
        ]
      },
      "pre-post-check": {
        "name": "Pre-Post Validation",
        "runs-on": "ubuntu-latest",
        "needs": "determine-slot",
        "timeout-minutes": 5,
        "outputs": {
          "can-post": "${{ steps.validation.outputs.can-post }}",
          "content-available": "${{ steps.validation.outputs.content-available }}"
        },
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "20",
              "cache-key-suffix": "post-validation"
            }
          },
          {
            "name": "Setup Supabase environment",
            "uses": "./.github/actions/setup-supabase-rest",
            "with": {
              "supabase-url": "${{ secrets.SUPABASE_URL }}",
              "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}",
              "database-url": "${{ secrets.DATABASE_URL }}"
            }
          },
          {
            "name": "Validate posting conditions",
            "id": "validation",
            "run": "echo \"üîç Validating posting conditions for slot: ${{ needs.determine-slot.outputs.slot }}...\"\n\nAUTH_TOKEN=\"${{ needs.determine-slot.outputs.auth-token }}\"\nSLOT_INDEX=\"${{ needs.determine-slot.outputs.slot-index }}\"\nTODAY=$(date +%Y-%m-%d)\n\n# Check if content is available for this slot\nRESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n  \"${{ secrets.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/admin/schedule/forecast?date=$TODAY\" \\\n  --max-time 30)\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//')\n\nCAN_POST=\"false\"\nCONTENT_AVAILABLE=\"false\"\n\nif [ \"$HTTP_STATUS\" -eq 200 ]; then\n  echo \"‚úÖ Schedule API accessible\"\n  CONTENT_AVAILABLE=\"true\"\n  \n  # Check if this slot hasn't been posted yet today\n  # This is a simplified check - you'd parse the JSON response\n  if [[ \"$BODY\" == *\"upcoming\"* ]]; then\n    CAN_POST=\"true\"\n    echo \"‚úÖ Content ready for posting\"\n  else\n    echo \"‚ö†Ô∏è Content may already be posted for this slot\"\n  fi\nelse\n  echo \"‚ùå Schedule API not accessible (status: $HTTP_STATUS)\"\nfi\n\n# Dry run mode always allows posting for testing\nif [[ \"${{ needs.determine-slot.outputs.dry-run }}\" == \"true\" ]]; then\n  CAN_POST=\"true\"\n  echo \"üß™ Dry run mode - posting validation bypassed\"\nfi\n\necho \"can-post=$CAN_POST\" >> $GITHUB_OUTPUT\necho \"content-available=$CONTENT_AVAILABLE\" >> $GITHUB_OUTPUT\necho \"Validation result: can-post=$CAN_POST, content-available=$CONTENT_AVAILABLE\"\n"
          }
        ]
      },
      "post-content": {
        "name": "Post Content",
        "runs-on": "ubuntu-latest",
        "needs": [
          "determine-slot",
          "pre-post-check"
        ],
        "if": "needs.pre-post-check.outputs.can-post == 'true'",
        "timeout-minutes": 10,
        "outputs": {
          "post-status": "${{ steps.post.outputs.post-status }}",
          "post-id": "${{ steps.post.outputs.post-id }}"
        },
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "20",
              "cache-key-suffix": "post-content"
            }
          },
          {
            "name": "Setup Supabase environment",
            "uses": "./.github/actions/setup-supabase-rest",
            "with": {
              "supabase-url": "${{ secrets.SUPABASE_URL }}",
              "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}",
              "database-url": "${{ secrets.DATABASE_URL }}"
            }
          },
          {
            "name": "Execute content posting",
            "id": "post",
            "run": "echo \"üì§ Posting content for slot: ${{ needs.determine-slot.outputs.slot }}...\"\n\nAUTH_TOKEN=\"${{ needs.determine-slot.outputs.auth-token }}\"\nSLOT=\"${{ needs.determine-slot.outputs.slot }}\"\nSLOT_INDEX=\"${{ needs.determine-slot.outputs.slot-index }}\"\nDRY_RUN=\"${{ needs.determine-slot.outputs.dry-run }}\"\n\n# Prepare the posting request\nJSON_PAYLOAD=$(cat <<EOF\n{\n  \"slot\": \"$SLOT\",\n  \"slotIndex\": $SLOT_INDEX,\n  \"dryRun\": $DRY_RUN\n}\nEOF\n)\n\n# Execute the post\nRESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -X POST \\\n  -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"$JSON_PAYLOAD\" \\\n  \"${{ secrets.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/admin/post\" \\\n  --max-time 300)\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//')\n\necho \"post-status=$HTTP_STATUS\" >> $GITHUB_OUTPUT\n\nif [ \"$HTTP_STATUS\" -eq 200 ]; then\n  if [[ \"$DRY_RUN\" == \"true\" ]]; then\n    echo \"‚úÖ Dry run completed successfully\"\n    echo \"post-id=dry-run-$(date +%s)\" >> $GITHUB_OUTPUT\n  else\n    echo \"‚úÖ Content posted successfully\"\n    # Extract post ID from response if available\n    POST_ID=$(echo \"$BODY\" | grep -o '\"id\":\"[^\"]*\"' | cut -d'\"' -f4 || echo \"posted-$(date +%s)\")\n    echo \"post-id=$POST_ID\" >> $GITHUB_OUTPUT\n  fi\n  \n  echo \"Response: $BODY\"\nelse\n  echo \"‚ùå Content posting failed with status $HTTP_STATUS\"\n  echo \"Response: $BODY\"\n  exit 1\nfi\n"
          },
          {
            "name": "Upload posting logs",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "posting-logs-${{ needs.determine-slot.outputs.slot }}",
              "path": "logs/post-*.log\nreports/post-*.json\n",
              "retention-days": 7,
              "if-no-files-found": "ignore"
            }
          }
        ]
      },
      "post-validation": {
        "name": "Post-Posting Validation",
        "runs-on": "ubuntu-latest",
        "needs": [
          "determine-slot",
          "post-content"
        ],
        "if": "needs.post-content.outputs.post-status == '200'",
        "timeout-minutes": 5,
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "20",
              "cache-key-suffix": "post-validation"
            }
          },
          {
            "name": "Validate post success",
            "run": "echo \"‚úÖ Validating successful post: ${{ needs.post-content.outputs.post-id }}...\"\n\nAUTH_TOKEN=\"${{ needs.determine-slot.outputs.auth-token }}\"\nSLOT=\"${{ needs.determine-slot.outputs.slot }}\"\nDRY_RUN=\"${{ needs.determine-slot.outputs.dry-run }}\"\n\nif [[ \"$DRY_RUN\" == \"true\" ]]; then\n  echo \"üß™ Dry run mode - post validation skipped\"\nelse\n  # Verify the content was actually posted\n  sleep 10  # Give the system time to update\n  \n  # Check the schedule to see if the content is marked as posted\n  TODAY=$(date +%Y-%m-%d)\n  RESPONSE=$(curl -s \\\n    -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n    \"${{ secrets.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/admin/schedule/forecast?date=$TODAY\" \\\n    --max-time 30)\n  \n  if [[ \"$RESPONSE\" == *\"posted\"* ]]; then\n    echo \"‚úÖ Post validation confirmed - content appears in schedule as posted\"\n  else\n    echo \"‚ö†Ô∏è Post validation warning - content may not be marked as posted yet\"\n  fi\nfi\n\necho \"üéâ Posting workflow completed for slot: $SLOT\"\n"
          }
        ]
      },
      "summary": {
        "name": "Posting Summary",
        "runs-on": "ubuntu-latest",
        "needs": [
          "determine-slot",
          "pre-post-check",
          "post-content",
          "post-validation"
        ],
        "if": "always()",
        "steps": [
          {
            "name": "Generate posting summary",
            "run": "echo \"## üì§ Content Posting Results\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Slot:** ${{ needs.determine-slot.outputs.slot }}\" >> $GITHUB_STEP_SUMMARY\necho \"**Slot Index:** ${{ needs.determine-slot.outputs.slot-index }}\" >> $GITHUB_STEP_SUMMARY\necho \"**Dry Run:** ${{ needs.determine-slot.outputs.dry-run }}\" >> $GITHUB_STEP_SUMMARY\necho \"**Trigger:** ${{ github.event_name }}\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"| Step | Status | Details |\" >> $GITHUB_STEP_SUMMARY\necho \"|------|--------|---------|\" >> $GITHUB_STEP_SUMMARY\necho \"| Pre-Check | ${{ needs.pre-post-check.result }} | Can post: ${{ needs.pre-post-check.outputs.can-post }} |\" >> $GITHUB_STEP_SUMMARY\n\nif [[ \"${{ needs.post-content.result }}\" != \"\" ]]; then\n  echo \"| Posting | ${{ needs.post-content.result }} | Status: ${{ needs.post-content.outputs.post-status }} |\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"| Posting | skipped | Pre-checks failed |\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [[ \"${{ needs.post-validation.result }}\" != \"\" ]]; then\n  echo \"| Validation | ${{ needs.post-validation.result }} | Post verification |\" >> $GITHUB_STEP_SUMMARY\nfi\n\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Overall status\nif [[ \"${{ needs.post-content.result }}\" == \"success\" ]]; then\n  if [[ \"${{ needs.determine-slot.outputs.dry-run }}\" == \"true\" ]]; then\n    echo \"## üß™ Dry run completed successfully\" >> $GITHUB_STEP_SUMMARY\n    echo \"Content would have been posted for **${{ needs.determine-slot.outputs.slot }}** slot.\" >> $GITHUB_STEP_SUMMARY\n  else\n    echo \"## ‚úÖ Content posted successfully\" >> $GITHUB_STEP_SUMMARY\n    echo \"**Post ID:** ${{ needs.post-content.outputs.post-id }}\" >> $GITHUB_STEP_SUMMARY\n    echo \"Posted for **${{ needs.determine-slot.outputs.slot }}** slot.\" >> $GITHUB_STEP_SUMMARY\n  fi\nelif [[ \"${{ needs.pre-post-check.outputs.can-post }}\" == \"false\" ]]; then\n  echo \"## ‚ÑπÔ∏è Posting skipped\" >> $GITHUB_STEP_SUMMARY\n  echo \"Content was not available or already posted for this slot.\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"## ‚ùå Posting failed\" >> $GITHUB_STEP_SUMMARY\n  echo \"Check posting logs for details.\" >> $GITHUB_STEP_SUMMARY\nfi\n"
          }
        ]
      }
    },
    "intent": "Scans social media platforms for hotdog-related content",
    "category": "scanning"
  },
  {
    "filename": "production-audit.yml",
    "name": "üîç Production Audit",
    "on": {
      "schedule": [
        {
          "cron": "0 9 * * 2"
        }
      ],
      "workflow_dispatch": {
        "inputs": {
          "force_run": {
            "description": "Force run audit even if recent run exists",
            "required": false,
            "type": "boolean",
            "default": false
          },
          "audit_window_days": {
            "description": "Number of days to audit (default: 7)",
            "required": false,
            "type": "number",
            "default": 7
          }
        }
      }
    },
    "env": {
      "PRODUCTION_URL": "https://hotdog-diaries.vercel.app",
      "GITHUB_REPO": "${{ github.repository }}"
    },
    "jobs": {
      "production-audit": {
        "name": "üîç 7-Day Production Health Audit",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 20,
        "permissions": {
          "contents": "read",
          "actions": "read",
          "issues": "write"
        },
        "steps": [
          {
            "name": "üì• Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "üîß Install dependencies",
            "run": "# Install required tools for audit script\nsudo apt-get update\nsudo apt-get install -y bc jq curl\n"
          },
          {
            "name": "üîê Configure environment",
            "run": "echo \"APP_ORIGIN=${{ env.PRODUCTION_URL }}\" >> $GITHUB_ENV\necho \"GITHUB_REPO=${{ env.GITHUB_REPO }}\" >> $GITHUB_ENV\n\n# Validate required secrets are available\nif [[ -z \"${{ secrets.AUTH_TOKEN }}\" ]]; then\n  echo \"‚ùå AUTH_TOKEN secret not configured\"\n  exit 1\nfi\n\nif [[ -z \"${{ secrets.SUPABASE_URL }}\" ]]; then\n  echo \"‚ùå SUPABASE_URL secret not configured\"  \n  exit 1\nfi\n\nif [[ -z \"${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}\" ]]; then\n  echo \"‚ùå SUPABASE_SERVICE_ROLE_KEY secret not configured\"\n  exit 1\nfi\n\necho \"‚úÖ All required secrets available\"\n"
          },
          {
            "name": "üîç Run production audit",
            "id": "audit",
            "env": {
              "AUTH_TOKEN": "${{ secrets.AUTH_TOKEN }}",
              "SUPABASE_URL": "${{ secrets.SUPABASE_URL }}",
              "SUPABASE_SERVICE_KEY": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}",
              "GITHUB_TOKEN": "${{ secrets.GITHUB_TOKEN }}"
            },
            "run": "echo \"üîç Starting 7-day production audit...\"\necho \"Target: $APP_ORIGIN\"\necho \"Repository: $GITHUB_REPO\"\necho \"\"\n\n# Make audit script executable\nchmod +x scripts/production-audit.sh\n\n# Run audit and capture exit code\nset +e  # Don't exit on failure, we want to capture results\n\nif scripts/production-audit.sh; then\n  AUDIT_EXIT_CODE=0\n  echo \"‚úÖ Production audit completed successfully\"\nelse\n  AUDIT_EXIT_CODE=$?\n  echo \"‚ùå Production audit detected issues (exit code: $AUDIT_EXIT_CODE)\"\nfi\n\necho \"AUDIT_EXIT_CODE=$AUDIT_EXIT_CODE\" >> $GITHUB_ENV\n\n# Check if artifacts were generated\nif [[ -d \"prod_audit_artifacts\" ]]; then\n  echo \"üìä Audit artifacts generated:\"\n  ls -la prod_audit_artifacts/\n  \n  # Extract key metrics for summary\n  if [[ -f \"prod_audit_artifacts/production_audit_$(date +%F).md\" ]]; then\n    echo \"üìã Audit report available\"\n    \n    # Extract summary information if available\n    if grep -q \"Total posts (7d):\" prod_audit_artifacts/production_audit_*.md; then\n      TOTAL_POSTS=$(grep \"Total posts (7d):\" prod_audit_artifacts/production_audit_*.md | grep -o '[0-9]\\+' | head -1)\n      echo \"TOTAL_POSTS=${TOTAL_POSTS:-0}\" >> $GITHUB_ENV\n    fi\n    \n    if grep -q \"Platform diversity:\" prod_audit_artifacts/production_audit_*.md; then\n      PLATFORM_COUNT=$(grep \"Platform diversity:\" prod_audit_artifacts/production_audit_*.md | grep -o '[0-9]\\+' | head -1)\n      echo \"PLATFORM_COUNT=${PLATFORM_COUNT:-0}\" >> $GITHUB_ENV\n    fi\n  fi\nelse\n  echo \"‚ö†Ô∏è No audit artifacts generated\"\nfi\n"
          },
          {
            "name": "üì§ Upload audit artifacts",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "production-audit-${{ github.run_number }}",
              "path": "prod_audit_artifacts/",
              "retention-days": 90,
              "compression-level": 6
            }
          },
          {
            "name": "üì§ Upload latest audit (overwrites)",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "production-audit-latest",
              "path": "prod_audit_artifacts/",
              "retention-days": 365,
              "compression-level": 6
            }
          },
          {
            "name": "üìä Generate audit summary",
            "if": "always()",
            "run": "echo \"## üîç Production Audit Summary\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Date:** $(date -u -Iseconds)\" >> $GITHUB_STEP_SUMMARY\necho \"**Target:** $APP_ORIGIN\" >> $GITHUB_STEP_SUMMARY\necho \"**Audit Window:** 7 days\" >> $GITHUB_STEP_SUMMARY\necho \"**Exit Code:** ${AUDIT_EXIT_CODE:-unknown}\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Status badge\nif [[ \"${AUDIT_EXIT_CODE:-1}\" -eq 0 ]]; then\n  echo \"**Status:** üü¢ **HEALTHY** - No issues detected\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"**Status:** üî¥ **ISSUES DETECTED** - Requires attention\" >> $GITHUB_STEP_SUMMARY\nfi\n\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"### üìä Key Metrics\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"| Metric | Value |\" >> $GITHUB_STEP_SUMMARY\necho \"|--------|-------|\" >> $GITHUB_STEP_SUMMARY\necho \"| Posts (7d) | ${TOTAL_POSTS:-N/A} |\" >> $GITHUB_STEP_SUMMARY\necho \"| Unique Platforms | ${PLATFORM_COUNT:-N/A} |\" >> $GITHUB_STEP_SUMMARY\necho \"| Audit Status | $([ \"${AUDIT_EXIT_CODE:-1}\" -eq 0 ] && echo \"‚úÖ Passed\" || echo \"‚ùå Failed\") |\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Add artifacts info\necho \"### üìÅ Generated Artifacts\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"- **Latest:** \\`production-audit-latest\\`\" >> $GITHUB_STEP_SUMMARY\necho \"- **Versioned:** \\`production-audit-${{ github.run_number }}\\`\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Include audit report summary if available\nif [[ -f prod_audit_artifacts/production_audit_*.md ]]; then\n  echo \"### üìã Audit Report\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"\\`\\`\\`\" >> $GITHUB_STEP_SUMMARY\n  head -20 prod_audit_artifacts/production_audit_*.md | grep -E \"^(#|##|\\*|-|[0-9])\" >> $GITHUB_STEP_SUMMARY || echo \"Report preview not available\" >> $GITHUB_STEP_SUMMARY\n  echo \"\\`\\`\\`\" >> $GITHUB_STEP_SUMMARY\nfi\n"
          },
          {
            "name": "üö® Create issue on critical failure",
            "if": "failure() && env.AUDIT_EXIT_CODE == '1' && github.event_name == 'schedule'",
            "uses": "actions/github-script@v7",
            "with": {
              "script": "const issueTitle = `üö® Production Audit Failed - ${new Date().toISOString().split('T')[0]}`;\nconst issueBody = `# Production Audit Failure Report\n\nThe weekly production audit detected critical issues on ${new Date().toISOString()}.\n\n**Workflow Run:** [#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})\n**Target Environment:** ${{ env.APP_ORIGIN }}\n**Audit Window:** 7 days\n\n## Failure Details\n\n- **Audit Exit Code:** ${{ env.AUDIT_EXIT_CODE }}\n- **Total Posts (7d):** ${{ env.TOTAL_POSTS || 'N/A' }}\n- **Platform Diversity:** ${{ env.PLATFORM_COUNT || 'N/A' }} unique platforms\n\n## Investigation Steps\n\n1. üîç Download audit artifacts: \\`production-audit-${{ github.run_number }}\\`\n2. üìä Review the generated markdown report for detailed findings\n3. üè• Check the [admin dashboard](${{ env.APP_ORIGIN }}/admin)\n4. üìà Review [system metrics](${{ env.APP_ORIGIN }}/api/system/metrics)\n5. üìã Consult the [SRE runbook](./docs/runbook.md) for incident response\n\n## Common Issues to Check\n\n- **Platform Diversity:** Is one platform dominating content (>60% share)?\n- **Content Queue:** Is the queue running low on approved content?\n- **Forecast Integrity:** Are there mismatches between API and database?\n- **CI Health:** Are GitHub Actions workflows failing repeatedly?\n- **Database Issues:** Are there connection or query problems?\n\n## Immediate Actions\n\n- [ ] Download and review audit artifacts\n- [ ] Check admin dashboard for obvious issues\n- [ ] Review recent deployment logs\n- [ ] Verify all API keys and secrets are valid\n- [ ] Run manual smoke tests if needed\n- [ ] Apply fixes and re-run audit to verify resolution\n- [ ] Close this issue when resolved\n\n---\n\nThis issue was automatically created by the Production Audit workflow.\n`;\n\nawait github.rest.issues.create({\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  title: issueTitle,\n  body: issueBody,\n  labels: ['critical', 'production', 'audit-failure', 'auto-created']\n});\n"
            }
          },
          {
            "name": "‚úÖ Success notification",
            "if": "success()",
            "run": "echo \"üéâ Production audit completed successfully!\"\necho \"\"\necho \"üìä Audit summary:\"\necho \"  - Target: $APP_ORIGIN\"\necho \"  - Window: 7 days\"\necho \"  - Posts: ${TOTAL_POSTS:-N/A}\"\necho \"  - Platforms: ${PLATFORM_COUNT:-N/A}\"\necho \"  - Status: ‚úÖ Healthy\"\necho \"\"\necho \"üîç Production system is operating within normal parameters.\"\n"
          }
        ]
      }
    },
    "intent": "General workflow (purpose unclear from name)",
    "category": "maintenance"
  },
  {
    "filename": "queue-monitor-hook.yml",
    "name": "Queue Monitor Hook",
    "on": {
      "workflow_call": {
        "inputs": {
          "trigger_source": {
            "description": "Source that triggered this hook",
            "required": true,
            "type": "string"
          },
          "post_count": {
            "description": "Number of posts just made",
            "required": false,
            "default": "1",
            "type": "string"
          }
        },
        "secrets": {
          "SITE_URL": {
            "required": true
          },
          "AUTH_TOKEN": {
            "required": true
          }
        }
      }
    },
    "jobs": {
      "queue-monitor": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Check Queue Health After Posting",
            "id": "health-check",
            "run": "echo \"ü©∫ Checking queue health after posting from ${{ inputs.trigger_source }}...\"\n\n# Get current queue health\nHEALTH_RESPONSE=$(curl -s -w \"HTTP_CODE:%{http_code}\" -L \\\n  \"${{ secrets.SITE_URL }}/api/admin/queue-health\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  --retry 2 --retry-delay 5 2>&1 || true)\n\n# Extract HTTP code and response body\nHTTP_CODE=$(echo \"$HEALTH_RESPONSE\" | grep -o 'HTTP_CODE:[0-9]*' | cut -d':' -f2)\nRESPONSE_BODY=$(echo \"$HEALTH_RESPONSE\" | sed 's/HTTP_CODE:[0-9]*$//')\n\nif [[ \"$HTTP_CODE\" == \"200\" ]]; then\n  echo \"‚úÖ Queue health check successful\"\n  \n  # Extract key metrics\n  HEALTH_STATUS=$(echo \"$RESPONSE_BODY\" | jq -r '.health.status // \"unknown\"')\n  DAYS_CONTENT=$(echo \"$RESPONSE_BODY\" | jq -r '.queue.daysOfContent // 0')\n  APPROVED_CONTENT=$(echo \"$RESPONSE_BODY\" | jq -r '.queue.totalApproved // 0')\n  NEEDS_SCANNING=$(echo \"$RESPONSE_BODY\" | jq -r '.queue.needsScanning // false')\n  \n  echo \"üìä Queue Status: $HEALTH_STATUS\"\n  echo \"üìÖ Days of Content: $DAYS_CONTENT\"\n  echo \"‚úÖ Approved Content: $APPROVED_CONTENT\"\n  echo \"üîç Needs Scanning: $NEEDS_SCANNING\"\n  \n  # Set outputs for auto-scan decision\n  echo \"health_status=$HEALTH_STATUS\" >> $GITHUB_OUTPUT\n  echo \"days_content=$DAYS_CONTENT\" >> $GITHUB_OUTPUT\n  echo \"approved_content=$APPROVED_CONTENT\" >> $GITHUB_OUTPUT\n  echo \"needs_scanning=$NEEDS_SCANNING\" >> $GITHUB_OUTPUT\n  echo \"should_auto_scan=false\" >> $GITHUB_OUTPUT\n  \n  # Determine if we should trigger auto-scan\n  if [[ \"$HEALTH_STATUS\" == \"critical\" ]] || [[ \"$HEALTH_STATUS\" == \"emergency\" ]]; then\n    echo \"üö® CRITICAL/EMERGENCY: Auto-scan will be triggered\"\n    echo \"should_auto_scan=true\" >> $GITHUB_OUTPUT\n  elif [[ \"$NEEDS_SCANNING\" == \"true\" ]] && [[ \"${DAYS_CONTENT%.*}\" -lt 3 ]]; then\n    echo \"‚ö†Ô∏è WARNING: Low content - auto-scan will be triggered\"\n    echo \"should_auto_scan=true\" >> $GITHUB_OUTPUT\n  else\n    echo \"‚úÖ Queue is healthy - no auto-scan needed\"\n  fi\n  \nelse\n  echo \"‚ùå Queue health check failed with HTTP $HTTP_CODE\"\n  echo \"Response: $RESPONSE_BODY\"\n  # Don't fail the hook, just log the issue\n  echo \"should_auto_scan=false\" >> $GITHUB_OUTPUT\nfi\n"
          },
          {
            "name": "Trigger Auto-Scan if Needed",
            "if": "steps.health-check.outputs.should_auto_scan == 'true'",
            "run": "HEALTH_STATUS=\"${{ steps.health-check.outputs.health_status }}\"\nDAYS_CONTENT=\"${{ steps.health-check.outputs.days_content }}\"\n\necho \"üöÄ Triggering auto-scan due to low queue...\"\necho \"Health Status: $HEALTH_STATUS\"\necho \"Days of Content: $DAYS_CONTENT\"\n\n# Determine scan mode based on urgency\nif [[ \"$HEALTH_STATUS\" == \"emergency\" ]]; then\n  SCAN_MODE=\"emergency\"\n  echo \"üÜò Using EMERGENCY mode\"\nelif [[ \"$HEALTH_STATUS\" == \"critical\" ]]; then\n  SCAN_MODE=\"emergency\"\n  echo \"üö® Using EMERGENCY mode for critical status\"\nelse\n  SCAN_MODE=\"auto\"\n  echo \"ü§ñ Using AUTO mode\"\nfi\n\n# Trigger auto-scan\nAUTO_SCAN_RESPONSE=$(curl -s -w \"HTTP_CODE:%{http_code}\" -L -X POST \\\n  \"${{ secrets.SITE_URL }}/api/admin/auto-scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\\\"mode\\\": \\\"$SCAN_MODE\\\", \\\"triggeredBy\\\": \\\"queue-monitor-hook-${{ inputs.trigger_source }}\\\"}\" \\\n  --retry 2 --retry-delay 10 2>&1 || true)\n\n# Extract results\nAUTO_HTTP_CODE=$(echo \"$AUTO_SCAN_RESPONSE\" | grep -o 'HTTP_CODE:[0-9]*' | cut -d':' -f2)\nAUTO_RESPONSE_BODY=$(echo \"$AUTO_SCAN_RESPONSE\" | sed 's/HTTP_CODE:[0-9]*$//')\n\nif [[ \"$AUTO_HTTP_CODE\" == \"200\" ]]; then\n  echo \"‚úÖ Auto-scan triggered successfully\"\n  \n  # Extract scan results\n  TRIGGERED=$(echo \"$AUTO_RESPONSE_BODY\" | jq -r '.summary.totalTriggered // 0')\n  ERRORS=$(echo \"$AUTO_RESPONSE_BODY\" | jq -r '.summary.totalErrors // 0')\n  \n  echo \"üìä Auto-scan Results:\"\n  echo \"  - Triggered: $TRIGGERED scans\"\n  echo \"  - Errors: $ERRORS\"\n  \n  if [[ \"$TRIGGERED\" -gt 0 ]]; then\n    echo \"üéØ Platforms scanned:\"\n    echo \"$AUTO_RESPONSE_BODY\" | jq -r '.triggeredScans[]? // empty' | sed 's/^/  - /'\n  fi\n  \n  if [[ \"$ERRORS\" -gt 0 ]]; then\n    echo \"‚ö†Ô∏è Scan errors occurred:\"\n    echo \"$AUTO_RESPONSE_BODY\" | jq -r '.errors[]? // empty' | sed 's/^/  - /'\n  fi\n  \nelse\n  echo \"‚ùå Auto-scan failed with HTTP $AUTO_HTTP_CODE\"\n  echo \"Response: $AUTO_RESPONSE_BODY\"\n  # Don't fail the hook - the original posting was successful\nfi\n"
          },
          {
            "name": "Log Hook Completion",
            "if": "always()",
            "run": "HEALTH_STATUS=\"${{ steps.health-check.outputs.health_status || 'unknown' }}\"\nSHOULD_SCAN=\"${{ steps.health-check.outputs.should_auto_scan || 'false' }}\"\n\necho \"üìã Queue Monitor Hook Summary:\"\necho \"  - Triggered by: ${{ inputs.trigger_source }}\"\necho \"  - Posts made: ${{ inputs.post_count }}\"\necho \"  - Queue health: $HEALTH_STATUS\"\necho \"  - Auto-scan triggered: $SHOULD_SCAN\"\necho \"  - Hook completed: $(date -u)\"\n\nif [[ \"$SHOULD_SCAN\" == \"true\" ]]; then\n  echo \"‚úÖ Queue monitoring and auto-replenishment completed\"\nelse\n  echo \"‚úÖ Queue monitoring completed - no action needed\"\nfi\n"
          }
        ]
      }
    },
    "intent": "Monitors system health and performance",
    "category": "monitor"
  },
  {
    "filename": "queue-monitor.yml",
    "name": "Monitor Queue Health & Scan if Needed",
    "on": {
      "schedule": [
        {
          "cron": "0 */3 * * *"
        }
      ],
      "workflow_dispatch": null
    },
    "jobs": {
      "check-queue-health": {
        "runs-on": "ubuntu-latest",
        "outputs": {
          "needs_scan": "${{ steps.check.outputs.needs_scan }}",
          "urgency": "${{ steps.check.outputs.urgency }}",
          "days_left": "${{ steps.check.outputs.days_left }}",
          "approved_count": "${{ steps.check.outputs.approved_count }}"
        },
        "steps": [
          {
            "name": "Check Queue Status",
            "id": "check",
            "run": "echo \"üîç Checking queue health...\"\n\nSTATUS=$(curl -L -s \"${{ secrets.SITE_URL }}/api/admin/schedule\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Accept: application/json\")\n\nif [ $? -ne 0 ]; then\n  echo \"‚ùå Failed to fetch queue status\"\n  echo \"needs_scan=true\" >> $GITHUB_OUTPUT\n  echo \"urgency=critical\" >> $GITHUB_OUTPUT\n  echo \"days_left=0\" >> $GITHUB_OUTPUT\n  echo \"approved_count=0\" >> $GITHUB_OUTPUT\n  exit 1\nfi\n\nAPPROVED_COUNT=$(echo \"$STATUS\" | jq -r '.queueStatus.totalApproved // 0')\nDAYS_OF_CONTENT=$(echo \"scale=1; $APPROVED_COUNT / 6\" | bc -l)\n\necho \"üìä Queue Status: $APPROVED_COUNT approved items = $DAYS_OF_CONTENT days\"\necho \"approved_count=$APPROVED_COUNT\" >> $GITHUB_OUTPUT\necho \"days_left=$DAYS_OF_CONTENT\" >> $GITHUB_OUTPUT\n\n# Determine if we need to scan\nif (( $(echo \"$DAYS_OF_CONTENT < 1\" | bc -l) )); then\n  echo \"üö® CRITICAL: Less than 1 day of content!\"\n  echo \"needs_scan=true\" >> $GITHUB_OUTPUT\n  echo \"urgency=critical\" >> $GITHUB_OUTPUT\nelif (( $(echo \"$DAYS_OF_CONTENT < 3\" | bc -l) )); then\n  echo \"‚ö†Ô∏è WARNING: Less than 3 days of content\"\n  echo \"needs_scan=true\" >> $GITHUB_OUTPUT\n  echo \"urgency=high\" >> $GITHUB_OUTPUT\nelif (( $(echo \"$DAYS_OF_CONTENT < 7\" | bc -l) )); then\n  echo \"üì° Normal scan needed (less than 7 days)\"\n  echo \"needs_scan=true\" >> $GITHUB_OUTPUT\n  echo \"urgency=normal\" >> $GITHUB_OUTPUT\nelse\n  echo \"‚úÖ Queue is healthy ($DAYS_OF_CONTENT days)\"\n  echo \"needs_scan=false\" >> $GITHUB_OUTPUT\n  echo \"urgency=none\" >> $GITHUB_OUTPUT\nfi\n"
          }
        ]
      },
      "scan-platforms": {
        "needs": "check-queue-health",
        "if": "needs.check-queue-health.outputs.needs_scan == 'true'",
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Emergency Scan All Platforms",
            "if": "needs.check-queue-health.outputs.urgency == 'critical'",
            "run": "echo \"üö® EMERGENCY: Scanning all platforms with auto-approval\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/content/emergency-scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"autoApprove\": true, \"maxItems\": 100, \"platforms\": [\"reddit\", \"youtube\", \"giphy\", \"pixabay\", \"bluesky\", \"imgur\", \"lemmy\", \"tumblr\"]}' \\\n  --fail --show-error --retry 2\n"
          },
          {
            "name": "High Priority Scan",
            "if": "needs.check-queue-health.outputs.urgency == 'high'",
            "run": "echo \"‚ö†Ô∏è HIGH PRIORITY: Scanning top platforms\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/content/priority-scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"platforms\": [\"reddit\", \"youtube\", \"giphy\", \"pixabay\"], \"maxItems\": 50}' \\\n  --fail --show-error --retry 2\n"
          },
          {
            "name": "Normal Scan",
            "if": "needs.check-queue-health.outputs.urgency == 'normal'",
            "run": "echo \"üì° Normal scan of select platforms\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/content/normal-scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"platforms\": [\"reddit\", \"giphy\", \"bluesky\"], \"maxItems\": 25}' \\\n  --fail --show-error --retry 2\n"
          },
          {
            "name": "Report Scan Results",
            "if": "always()",
            "run": "echo \"üìä Scan completed for urgency level: ${{ needs.check-queue-health.outputs.urgency }}\"\necho \"Queue had ${{ needs.check-queue-health.outputs.approved_count }} approved items\"\necho \"Estimated ${{ needs.check-queue-health.outputs.days_left }} days of content\"\n"
          }
        ]
      }
    },
    "intent": "Scans social media platforms for hotdog-related content",
    "category": "scanning"
  },
  {
    "filename": "runbook-artifact.yml",
    "name": "üìã Generate Runbook Artifacts",
    "on": {
      "push": {
        "branches": [
          "main"
        ],
        "paths": [
          "docs/runbook.md",
          "scripts/md-to-pdf.ts",
          ".github/workflows/runbook-artifact.yml"
        ]
      },
      "workflow_dispatch": {
        "inputs": {
          "force_regenerate": {
            "description": "Force regenerate all artifacts",
            "required": false,
            "type": "boolean",
            "default": false
          }
        }
      }
    },
    "jobs": {
      "generate-runbook-artifacts": {
        "name": "üîÑ Generate PDF and Upload Artifacts",
        "runs-on": "ubuntu-latest",
        "permissions": {
          "contents": "read",
          "actions": "write"
        },
        "steps": [
          {
            "name": "üì• Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "üìã Setup Node.js",
            "uses": "actions/setup-node@v4",
            "with": {
              "node-version": "18",
              "cache": "npm"
            }
          },
          {
            "name": "üì¶ Install dependencies",
            "run": "npm ci\n# Install additional system dependencies for PDF generation\nsudo apt-get update\nsudo apt-get install -y fonts-liberation fonts-dejavu-core\n"
          },
          {
            "name": "üìÑ Generate PDF from Markdown",
            "run": "echo \"üîÑ Generating runbook PDF...\"\nnpm run runbook:pdf\n\n# Verify PDF was generated\nif [ ! -f \"docs/runbook.pdf\" ]; then\n  echo \"‚ùå PDF generation failed\"\n  exit 1\nfi\n\n# Get file size for logging\nPDF_SIZE=$(du -h docs/runbook.pdf | cut -f1)\necho \"‚úÖ PDF generated successfully ($PDF_SIZE)\"\n"
          },
          {
            "name": "üìä Generate metadata",
            "run": "echo \"üìä Generating artifact metadata...\"\n\ncat > docs/runbook-metadata.json << EOF\n{\n  \"generated_at\": \"$(date -u -Iseconds)\",\n  \"git_commit\": \"${{ github.sha }}\",\n  \"git_ref\": \"${{ github.ref }}\",\n  \"workflow_run_id\": \"${{ github.run_id }}\",\n  \"workflow_run_number\": \"${{ github.run_number }}\",\n  \"triggered_by\": \"${{ github.event_name }}\",\n  \"actor\": \"${{ github.actor }}\",\n  \"repository\": \"${{ github.repository }}\",\n  \"pdf_size_bytes\": $(stat -c%s docs/runbook.pdf),\n  \"pdf_checksum_sha256\": \"$(sha256sum docs/runbook.pdf | cut -d' ' -f1)\",\n  \"source_files\": [\n    \"docs/runbook.md\",\n    \"scripts/md-to-pdf.ts\"\n  ],\n  \"format_version\": \"1.0.0\"\n}\nEOF\n\necho \"‚úÖ Metadata generated\"\ncat docs/runbook-metadata.json\n"
          },
          {
            "name": "üîç Validate artifacts",
            "run": "echo \"üîç Validating generated artifacts...\"\n\n# Check PDF is valid (basic validation)\nif file docs/runbook.pdf | grep -q \"PDF\"; then\n  echo \"‚úÖ PDF format validated\"\nelse\n  echo \"‚ùå Invalid PDF format\"\n  exit 1\nfi\n\n# Check metadata is valid JSON\nif jq . docs/runbook-metadata.json > /dev/null; then\n  echo \"‚úÖ Metadata JSON validated\"\nelse\n  echo \"‚ùå Invalid metadata JSON\"\n  exit 1\nfi\n\n# Check file sizes are reasonable\nPDF_SIZE_BYTES=$(stat -c%s docs/runbook.pdf)\nif [ $PDF_SIZE_BYTES -lt 10000 ]; then\n  echo \"‚ùå PDF suspiciously small ($PDF_SIZE_BYTES bytes)\"\n  exit 1\nelif [ $PDF_SIZE_BYTES -gt 50000000 ]; then\n  echo \"‚ùå PDF suspiciously large ($PDF_SIZE_BYTES bytes)\"\n  exit 1\nelse\n  echo \"‚úÖ PDF size reasonable ($PDF_SIZE_BYTES bytes)\"\nfi\n"
          },
          {
            "name": "üì§ Upload runbook artifacts",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "sre-runbook-${{ github.run_number }}",
              "path": "docs/runbook.pdf\ndocs/runbook.md\ndocs/runbook-metadata.json\n",
              "retention-days": 90,
              "compression-level": 6
            }
          },
          {
            "name": "üì§ Upload latest runbook (overwrites)",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "sre-runbook-latest",
              "path": "docs/runbook.pdf\ndocs/runbook.md\ndocs/runbook-metadata.json\n",
              "retention-days": 365,
              "compression-level": 6
            }
          },
          {
            "name": "üìã Generate summary",
            "run": "echo \"## üìã Runbook Artifacts Generated\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Generated at:** $(date -u -Iseconds)\" >> $GITHUB_STEP_SUMMARY\necho \"**Commit:** \\`${{ github.sha }}\\`\" >> $GITHUB_STEP_SUMMARY\necho \"**Triggered by:** ${{ github.event_name }}\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"### üìÑ Generated Files\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"| File | Size | SHA256 |\" >> $GITHUB_STEP_SUMMARY\necho \"|------|------|---------|\" >> $GITHUB_STEP_SUMMARY\necho \"| \\`docs/runbook.pdf\\` | $(du -h docs/runbook.pdf | cut -f1) | \\`$(sha256sum docs/runbook.pdf | cut -d' ' -f1 | head -c16)...\\` |\" >> $GITHUB_STEP_SUMMARY\necho \"| \\`docs/runbook.md\\` | $(du -h docs/runbook.md | cut -f1) | \\`$(sha256sum docs/runbook.md | cut -d' ' -f1 | head -c16)...\\` |\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"### üì§ Artifact Downloads\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"- **Versioned:** \\`sre-runbook-${{ github.run_number }}\\`\" >> $GITHUB_STEP_SUMMARY\necho \"- **Latest:** \\`sre-runbook-latest\\`\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"‚úÖ All artifacts generated successfully!\" >> $GITHUB_STEP_SUMMARY\n"
          },
          {
            "name": "‚úÖ Complete",
            "run": "echo \"üéâ Runbook artifact generation completed successfully!\"\necho \"\"\necho \"üì¶ Artifacts available:\"\necho \"  - sre-runbook-${{ github.run_number }} (versioned)\"\necho \"  - sre-runbook-latest (always current)\"\necho \"\"\necho \"üìÑ Generated files:\"\nls -la docs/runbook.*\n"
          }
        ]
      }
    },
    "intent": "General workflow (purpose unclear from name)",
    "category": "maintenance"
  },
  {
    "filename": "scan-bluesky.yml",
    "name": "Scan Bluesky for Content",
    "on": {
      "schedule": [
        {
          "cron": "0 1,9,17 * * *"
        }
      ],
      "workflow_dispatch": null
    },
    "jobs": {
      "scan-bluesky": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Scan Bluesky",
            "run": "echo \"ü¶ã Scanning Bluesky for hotdog posts...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/bluesky/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 15}' \\\n  --fail --show-error --retry 2 --retry-delay 5\n\necho \"‚úÖ Bluesky scan completed\"\n"
          },
          {
            "name": "Handle Scan Failure",
            "if": "failure()",
            "run": "echo \"‚ùå Bluesky scanning failed - will retry on next scheduled run\"\n"
          }
        ]
      }
    },
    "intent": "Scans Bluesky for hotdog content",
    "category": "scanning"
  },
  {
    "filename": "scan-giphy.yml",
    "name": "Scan Giphy for Content",
    "on": {
      "schedule": [
        {
          "cron": "0 2,10,18 * * *"
        }
      ],
      "workflow_dispatch": null
    },
    "jobs": {
      "scan-giphy": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Scan Giphy",
            "run": "echo \"üé≠ Scanning Giphy for hotdog GIFs...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/giphy/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 15}' \\\n  --fail --show-error --retry 2 --retry-delay 5\n\necho \"‚úÖ Giphy scan completed\"\n"
          },
          {
            "name": "Handle Scan Failure",
            "if": "failure()",
            "run": "echo \"‚ùå Giphy scanning failed - will retry on next scheduled run\"\n"
          }
        ]
      }
    },
    "intent": "Scans Giphy for hotdog GIFs",
    "category": "scanning"
  },
  {
    "filename": "scan-imgur.yml",
    "name": "Scan Imgur for Content",
    "on": {
      "schedule": [
        {
          "cron": "0 4,12,20 * * *"
        }
      ],
      "workflow_dispatch": null
    },
    "jobs": {
      "scan-imgur": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Scan Imgur",
            "run": "echo \"üñºÔ∏è Scanning Imgur for hotdog images...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/imgur/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 15}' \\\n  --fail --show-error --retry 2 --retry-delay 5\n\necho \"‚úÖ Imgur scan completed\"\n"
          },
          {
            "name": "Handle Scan Failure",
            "if": "failure()",
            "run": "echo \"‚ùå Imgur scanning failed - will retry on next scheduled run\"\n"
          }
        ]
      }
    },
    "intent": "Scans Imgur for hotdog images",
    "category": "scanning"
  },
  {
    "filename": "scan-lemmy.yml",
    "name": "Scan Lemmy for Content",
    "on": {
      "schedule": [
        {
          "cron": "0 5,13,21 * * *"
        }
      ],
      "workflow_dispatch": null
    },
    "jobs": {
      "scan-lemmy": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Scan Lemmy",
            "run": "echo \"üåê Scanning Lemmy for hotdog content...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/lemmy/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 10}' \\\n  --fail --show-error --retry 2 --retry-delay 5 || echo \"‚ö†Ô∏è Lemmy scan failed (continuing)\"\n\necho \"‚úÖ Lemmy scan attempt completed\"\n"
          },
          {
            "name": "Handle Scan Failure",
            "if": "failure()",
            "run": "echo \"‚ö†Ô∏è Lemmy scanning failed - this is expected occasionally\"\n"
          }
        ]
      }
    },
    "intent": "Scans social media platforms for hotdog-related content",
    "category": "scanning"
  },
  {
    "filename": "scan-niche-platforms.yml",
    "name": "Scan Niche Platforms",
    "on": {
      "schedule": [
        {
          "cron": "0 6,14,22 * * *"
        }
      ],
      "workflow_dispatch": null
    },
    "jobs": {
      "scan-lemmy": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Scan Lemmy",
            "run": "echo \"üåê Scanning Lemmy for hotdog content...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/lemmy/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 10}' \\\n  --fail --show-error --retry 2 || echo \"‚ö†Ô∏è Lemmy scan failed (continuing)\"\n\necho \"‚úÖ Lemmy scan attempt completed\"\n"
          }
        ]
      },
      "scan-tumblr": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Scan Tumblr",
            "run": "echo \"üé® Scanning Tumblr for hotdog content...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/tumblr/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 10}' \\\n  --fail --show-error --retry 2 || echo \"‚ö†Ô∏è Tumblr scan failed (continuing)\"\n\necho \"‚úÖ Tumblr scan attempt completed\"\n"
          }
        ]
      }
    },
    "intent": "Scans social media platforms for hotdog-related content",
    "category": "scanning"
  },
  {
    "filename": "scan-pixabay.yml",
    "name": "Scan Pixabay for Content",
    "on": {
      "schedule": [
        {
          "cron": "0 3,11,19 * * *"
        }
      ],
      "workflow_dispatch": null
    },
    "jobs": {
      "scan-pixabay": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Scan Pixabay",
            "run": "echo \"üì∏ Scanning Pixabay for hotdog images...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/pixabay/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 15}' \\\n  --fail --show-error --retry 2 --retry-delay 5\n\necho \"‚úÖ Pixabay scan completed\"\n"
          },
          {
            "name": "Handle Scan Failure",
            "if": "failure()",
            "run": "echo \"‚ùå Pixabay scanning failed - will retry on next scheduled run\"\n"
          }
        ]
      }
    },
    "intent": "Scans social media platforms for hotdog-related content",
    "category": "scanning"
  },
  {
    "filename": "scan-reddit.yml",
    "name": "Scan Reddit for Content",
    "on": {
      "schedule": [
        {
          "cron": "0 2,10,18 * * *"
        }
      ],
      "workflow_dispatch": null
    },
    "jobs": {
      "scan-reddit": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Scan Reddit",
            "run": "echo \"ü§ñ Scanning Reddit for hotdog content...\"\n\nRESULT=$(curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/reddit/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 20}' \\\n  --silent --show-error --retry 2)\n\nif [ $? -eq 0 ]; then\n  FOUND=$(echo \"$RESULT\" | jq -r '.totalFound // 0')\n  PROCESSED=$(echo \"$RESULT\" | jq -r '.processed // 0')\n  echo \"‚úÖ Reddit scan successful: Found $FOUND, processed $PROCESSED\"\nelse\n  echo \"‚ùå Reddit scan failed\"\n  exit 1\nfi\n"
          },
          {
            "name": "Handle Scan Failure",
            "if": "failure()",
            "run": "echo \"‚ùå Reddit scanning failed - will retry on next scheduled run\"\n"
          }
        ]
      }
    },
    "intent": "Scans Reddit for hotdog content",
    "category": "scanning"
  },
  {
    "filename": "scan-social-platforms.yml",
    "name": "Scan Social Platforms",
    "on": {
      "schedule": [
        {
          "cron": "0 1,9,17 * * *"
        }
      ],
      "workflow_dispatch": null
    },
    "jobs": {
      "scan-giphy": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Scan Giphy",
            "run": "echo \"üé≠ Scanning Giphy for hotdog GIFs...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/giphy/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 15}' \\\n  --fail --show-error --retry 2\n\necho \"‚úÖ Giphy scan completed\"\n"
          }
        ]
      },
      "scan-bluesky": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Scan Bluesky",
            "run": "echo \"ü¶ã Scanning Bluesky for hotdog posts...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/bluesky/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 15}' \\\n  --fail --show-error --retry 2\n\necho \"‚úÖ Bluesky scan completed\"\n"
          }
        ]
      },
      "scan-imgur": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Scan Imgur",
            "run": "echo \"üñºÔ∏è Scanning Imgur for hotdog images...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/imgur/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 15}' \\\n  --fail --show-error --retry 2\n\necho \"‚úÖ Imgur scan completed\"\n"
          }
        ]
      },
      "scan-pixabay": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Scan Pixabay",
            "run": "echo \"üì∏ Scanning Pixabay for hotdog images...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/pixabay/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 15}' \\\n  --fail --show-error --retry 2\n\necho \"‚úÖ Pixabay scan completed\"\n"
          }
        ]
      }
    },
    "intent": "Scans social media platforms for hotdog-related content",
    "category": "scanning"
  },
  {
    "filename": "scan-tumblr.yml",
    "name": "Scan Tumblr for Content",
    "on": {
      "schedule": [
        {
          "cron": "0 6,14,22 * * *"
        }
      ],
      "workflow_dispatch": null
    },
    "jobs": {
      "scan-tumblr": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Scan Tumblr",
            "run": "echo \"üé® Scanning Tumblr for hotdog content...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/tumblr/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 10}' \\\n  --fail --show-error --retry 2 --retry-delay 5 || echo \"‚ö†Ô∏è Tumblr scan failed (continuing)\"\n\necho \"‚úÖ Tumblr scan attempt completed\"\n"
          },
          {
            "name": "Handle Scan Failure",
            "if": "failure()",
            "run": "echo \"‚ö†Ô∏è Tumblr scanning failed - this is expected occasionally\"\n"
          }
        ]
      }
    },
    "intent": "Scans social media platforms for hotdog-related content",
    "category": "scanning"
  },
  {
    "filename": "scan-youtube.yml",
    "name": "Scan YouTube for Content",
    "on": {
      "schedule": [
        {
          "cron": "0 4,16 * * *"
        }
      ],
      "workflow_dispatch": null
    },
    "jobs": {
      "scan-youtube": {
        "runs-on": "ubuntu-latest",
        "steps": [
          {
            "name": "Scan YouTube",
            "run": "echo \"üé¨ Scanning YouTube for hotdog content...\"\n\nRESULT=$(curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/youtube/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 10}' \\\n  --silent --show-error --retry 2)\n\nif [ $? -eq 0 ]; then\n  FOUND=$(echo \"$RESULT\" | jq -r '.totalFound // 0')\n  PROCESSED=$(echo \"$RESULT\" | jq -r '.processed // 0')\n  echo \"‚úÖ YouTube scan successful: Found $FOUND, processed $PROCESSED\"\nelse\n  echo \"‚ùå YouTube scan failed (possibly quota exceeded)\"\nfi\n"
          },
          {
            "name": "Handle Quota Issues",
            "if": "failure()",
            "run": "echo \"‚ö†Ô∏è YouTube scan failed - likely quota limit reached\"\necho \"Will retry on next scheduled run\"\n"
          }
        ]
      }
    },
    "intent": "Scans YouTube for hotdog content",
    "category": "scanning"
  },
  {
    "filename": "scanners.yml",
    "name": "Content Scanners",
    "on": {
      "schedule": [
        {
          "cron": "0 */4 * * *"
        },
        {
          "cron": "30 */6 * * *"
        },
        {
          "cron": "15 */8 * * *"
        }
      ],
      "workflow_dispatch": {
        "inputs": {
          "platforms": {
            "description": "Platforms to scan (comma-separated: reddit,youtube,giphy,imgur,bluesky,tumblr,lemmy,pixabay,niche)",
            "type": "string",
            "default": "all"
          },
          "max-posts": {
            "description": "Maximum posts per platform",
            "type": "number",
            "default": 50
          }
        }
      },
      "workflow_call": {
        "inputs": {
          "platforms": {
            "description": "Platforms to scan",
            "type": "string",
            "default": "all"
          },
          "max-posts": {
            "description": "Maximum posts per platform",
            "type": "number",
            "default": 50
          }
        }
      }
    },
    "concurrency": {
      "group": "scanners-${{ github.ref }}",
      "cancel-in-progress": false
    },
    "env": {
      "NODE_ENV": "production",
      "CI": true
    },
    "jobs": {
      "plan-scans": {
        "name": "Plan Demand-Driven Scans",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 5,
        "outputs": {
          "matrix": "${{ steps.plan.outputs.matrix }}",
          "reason": "${{ steps.plan.outputs.reason }}",
          "has_work": "${{ steps.plan.outputs.has_work }}"
        },
        "env": {
          "TARGET_URL": "${{ vars.SITE_URL || secrets.SITE_URL || 'https://hotdog-diaries.vercel.app' }}",
          "SUPABASE_URL": "${{ secrets.SUPABASE_URL }}",
          "SUPABASE_SERVICE_ROLE_KEY": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}",
          "SCAN_MIN_PER_PLATFORM": "${{ vars.SCAN_MIN_PER_PLATFORM || '40' }}",
          "SCAN_MAX_PER_PLATFORM": "${{ vars.SCAN_MAX_PER_PLATFORM || '120' }}",
          "SCAN_GLOBAL_MAX": "${{ vars.SCAN_GLOBAL_MAX || '800' }}",
          "SCAN_COOLDOWN_MIN": "${{ vars.SCAN_COOLDOWN_MIN || '180' }}",
          "MIN_CONF": "${{ vars.MIN_CONF || '0.70' }}",
          "MIN_CANDIDATES": "${{ vars.MIN_CANDIDATES || '20' }}",
          "PLATFORM_ALLOW": "${{ vars.PLATFORM_ALLOW || 'reddit,youtube,giphy,imgur,bluesky,tumblr,lemmy,pixabay' }}"
        },
        "steps": [
          {
            "uses": "actions/checkout@v4"
          },
          {
            "uses": "actions/setup-node@v4",
            "with": {
              "node-version": 20,
              "cache": "npm"
            }
          },
          {
            "name": "Install dependencies",
            "run": "echo \"üì¶ Installing dependencies for planner tests...\"\nnpm ci --prefer-offline --no-audit --no-fund\n"
          },
          {
            "name": "Runtime policy guard",
            "id": "policy-guard",
            "run": "echo \"üõ°Ô∏è Running runtime policy enforcement...\"\n\n# Verify planner tests pass before allowing scans\nif ! npm run test:planner --silent >/dev/null 2>&1; then\n  echo \"‚ùå POLICY VIOLATION: Planner contract tests failed\"\n  echo \"üö® Blocking scan execution for safety\"\n  echo \"policy-violation=true\" >> $GITHUB_OUTPUT\n  exit 1\nfi\n\necho \"‚úÖ Policy guard passed - planner behavior validated\"\necho \"policy-violation=false\" >> $GITHUB_OUTPUT\n"
          },
          {
            "name": "Build scan plan",
            "id": "plan",
            "if": "steps.policy-guard.outputs.policy-violation == 'false'",
            "run": "echo \"üîç Analyzing queue depths and planning scans...\"\nnode scripts/scan-plan.mjs 2>&1 | tee plan_stdout.txt || true\n\n# Check if matrix has any work\nif [ -f scan_matrix.json ]; then\n  matrix=$(cat scan_matrix.json)\n  has_work=$(echo \"$matrix\" | jq -r 'if .include | length > 0 then \"true\" else \"false\" end')\nelse\n  matrix='{\"include\":[]}'\n  has_work='false'\nfi\n\n# Output for next job\necho \"matrix=$matrix\" >> $GITHUB_OUTPUT\necho \"has_work=$has_work\" >> $GITHUB_OUTPUT\n\n# Handle policy guard results\npolicy_status=\"Unknown\"\nif [ \"${{ steps.policy-guard.outputs.policy-violation }}\" = \"true\" ]; then\n  policy_status=\"‚ùå VIOLATION - Scan blocked\"\n  echo \"reason=policy_violation\" >> $GITHUB_OUTPUT\nelif [ \"${{ steps.policy-guard.outputs.policy-violation }}\" = \"false\" ]; then\n  policy_status=\"‚úÖ Passed\"\nfi\n\n# Summary for UI\necho \"## üìä Scan Planning Results\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Policy Guard:** $policy_status\" >> $GITHUB_STEP_SUMMARY\n\nif [ -f scan_plan.json ]; then\n  reason=$(jq -r '.analysis.reason // \"unknown\"' scan_plan.json)\n  echo \"reason=$reason\" >> $GITHUB_OUTPUT\n  echo \"**Reason:** $reason\" >> $GITHUB_STEP_SUMMARY\n  echo \"**Platforms to scan:** $(echo \"$matrix\" | jq -r '.include | length')\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  \n  if [ \"$has_work\" = \"true\" ]; then\n    echo \"### Platforms Needing Content\" >> $GITHUB_STEP_SUMMARY\n    echo \"| Platform | Desired Posts |\" >> $GITHUB_STEP_SUMMARY\n    echo \"|----------|---------------|\" >> $GITHUB_STEP_SUMMARY\n    echo \"$matrix\" | jq -r '.include[] | \"| \\(.platform) | \\(.desired) |\"' >> $GITHUB_STEP_SUMMARY\n  else\n    echo \"‚úÖ All platforms have sufficient content queued!\" >> $GITHUB_STEP_SUMMARY\n  fi\nelse\n  echo \"**Status:** Scan planning blocked due to policy violation\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"‚ö†Ô∏è **SAFETY BLOCK**: Planner contract tests failed. Scans are disabled until issues are resolved.\" >> $GITHUB_STEP_SUMMARY\nfi\n"
          },
          {
            "name": "Upload plan artifacts",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "scan-plan-${{ github.run_number }}",
              "path": "scan_plan.json\nscan_matrix.json\nplan_stdout.txt\n",
              "retention-days": 7,
              "if-no-files-found": "ignore"
            }
          }
        ]
      },
      "determine-platforms": {
        "name": "Determine Scan Strategy (Legacy)",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 2,
        "if": "${{ github.event_name == 'workflow_dispatch' && (inputs.platforms != '' && inputs.platforms != 'demand-driven') }}",
        "outputs": {
          "platforms": "${{ steps.strategy.outputs.platforms }}",
          "schedule-type": "${{ steps.strategy.outputs.schedule-type }}"
        },
        "steps": [
          {
            "name": "Determine scan strategy",
            "id": "strategy",
            "run": "PLATFORMS=\"${{ inputs.platforms || 'all' }}\"\nSCHEDULE_TYPE=\"manual\"\n\n# Determine platforms based on trigger\nif [[ \"${{ github.event_name }}\" == \"schedule\" ]]; then\n  SCHEDULE_TYPE=\"scheduled\"\n  \n  # Determine which platforms based on cron time\n  HOUR=$(date +%H)\n  MINUTE=$(date +%M)\n  \n  if [[ $MINUTE -eq 0 ]]; then\n    # Every 4 hours (0, 4, 8, 12, 16, 20) - high volume\n    PLATFORMS=\"reddit,youtube,giphy\"\n    echo \"üîÑ High-volume scan: reddit,youtube,giphy\"\n  elif [[ $MINUTE -eq 30 ]]; then\n    # Every 6 hours (00:30, 06:30, 12:30, 18:30) - medium volume  \n    PLATFORMS=\"imgur,bluesky,tumblr\"\n    echo \"üîÑ Medium-volume scan: imgur,bluesky,tumblr\"\n  elif [[ $MINUTE -eq 15 ]]; then\n    # Every 8 hours (00:15, 08:15, 16:15) - low volume\n    PLATFORMS=\"lemmy,pixabay,niche\"\n    echo \"üîÑ Low-volume scan: lemmy,pixabay,niche\"\n  fi\nfi\n\nif [[ \"$PLATFORMS\" == \"all\" ]]; then\n  PLATFORMS=\"reddit,youtube,giphy,imgur,bluesky,tumblr,lemmy,pixabay,niche\"\nfi\n\necho \"platforms=$PLATFORMS\" >> $GITHUB_OUTPUT\necho \"schedule-type=$SCHEDULE_TYPE\" >> $GITHUB_OUTPUT\necho \"Selected platforms: $PLATFORMS\"\n"
          }
        ]
      },
      "demand-driven-scan": {
        "name": "Scan Content (Demand-Driven)",
        "runs-on": "ubuntu-latest",
        "needs": "plan-scans",
        "if": "${{ needs.plan-scans.outputs.has_work == 'true' }}",
        "timeout-minutes": 15,
        "strategy": {
          "fail-fast": false,
          "max-parallel": 3,
          "matrix": "${{ fromJSON(needs.plan-scans.outputs.matrix) }}"
        },
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "20",
              "cache-key-suffix": "scanner-${{ matrix.platform }}"
            }
          },
          {
            "name": "Setup Supabase environment",
            "uses": "./.github/actions/setup-supabase-rest",
            "with": {
              "supabase-url": "${{ secrets.SUPABASE_URL }}",
              "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}",
              "database-url": "${{ secrets.DATABASE_URL }}"
            }
          },
          {
            "name": "Platform-specific setup",
            "run": "echo \"üîß Setting up environment for ${{ matrix.platform }}...\"\necho \"üìä Target: ${{ matrix.desired }} posts\"\n\ncase \"${{ matrix.platform }}\" in\n  reddit)\n    echo \"REDDIT_CLIENT_ID=${{ secrets.REDDIT_CLIENT_ID }}\" >> $GITHUB_ENV\n    echo \"REDDIT_CLIENT_SECRET=${{ secrets.REDDIT_CLIENT_SECRET }}\" >> $GITHUB_ENV\n    ;;\n  youtube)\n    echo \"YOUTUBE_API_KEY=${{ secrets.YOUTUBE_API_KEY }}\" >> $GITHUB_ENV\n    ;;\n  giphy)\n    echo \"GIPHY_API_KEY=${{ secrets.GIPHY_API_KEY }}\" >> $GITHUB_ENV\n    ;;\n  imgur)\n    echo \"IMGUR_CLIENT_ID=${{ secrets.IMGUR_CLIENT_ID }}\" >> $GITHUB_ENV\n    ;;\n  bluesky)\n    echo \"BLUESKY_IDENTIFIER=${{ secrets.BLUESKY_IDENTIFIER }}\" >> $GITHUB_ENV\n    echo \"BLUESKY_APP_PASSWORD=${{ secrets.BLUESKY_APP_PASSWORD }}\" >> $GITHUB_ENV\n    ;;\n  pixabay)\n    echo \"PIXABAY_API_KEY=${{ secrets.PIXABAY_API_KEY }}\" >> $GITHUB_ENV\n    ;;\n  tumblr)\n    echo \"TUMBLR_API_KEY=${{ secrets.TUMBLR_API_KEY }}\" >> $GITHUB_ENV\n    ;;\nesac\n"
          },
          {
            "name": "Scan platform content",
            "id": "scan",
            "run": "echo \"üì° Scanning ${{ matrix.platform }} for hotdog content...\"\necho \"üéØ Targeting ${{ matrix.desired }} posts based on queue deficit\"\n\n# Use the desired count from the planning phase\nMAX_POSTS=\"${{ matrix.desired }}\"\n\ncase \"${{ matrix.platform }}\" in\n  reddit)\n    npm run scan:reddit -- --max-posts=$MAX_POSTS || true\n    ;;\n  youtube)\n    npm run scan:youtube -- --max-posts=$MAX_POSTS || true\n    ;;\n  giphy)\n    npm run scan:giphy -- --max-posts=$MAX_POSTS || true\n    ;;\n  imgur)\n    npm run scan:imgur -- --max-posts=$MAX_POSTS || true\n    ;;\n  bluesky)\n    npm run scan:bluesky -- --max-posts=$MAX_POSTS || true\n    ;;\n  tumblr)\n    npm run scan:tumblr -- --max-posts=$MAX_POSTS || true\n    ;;\n  lemmy)\n    npm run scan:lemmy -- --max-posts=$MAX_POSTS || true\n    ;;\n  pixabay)\n    npm run scan:pixabay -- --max-posts=$MAX_POSTS || true\n    ;;\n  *)\n    echo \"‚ùå Unknown platform: ${{ matrix.platform }}\"\n    ;;\nesac\n\necho \"‚úÖ Scan completed for ${{ matrix.platform }}\"\n"
          },
          {
            "name": "Upload scan logs",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "scan-logs-${{ matrix.platform }}-${{ github.run_number }}",
              "path": "logs/scan-*.log\nreports/scan-*.json\n",
              "retention-days": 3,
              "if-no-files-found": "ignore"
            }
          }
        ]
      },
      "scan": {
        "name": "Scan Content (Legacy)",
        "runs-on": "ubuntu-latest",
        "needs": "determine-platforms",
        "if": "${{ needs.determine-platforms.outputs.platforms != '' }}",
        "timeout-minutes": 15,
        "strategy": {
          "fail-fast": false,
          "max-parallel": 3,
          "matrix": {
            "platform": "${{ fromJSON(format('[{0}]', replace(replace(needs.determine-platforms.outputs.platforms, ' ', ''), ',', '\",\"'))) }}"
          }
        },
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "20",
              "cache-key-suffix": "scanner-${{ matrix.platform }}"
            }
          },
          {
            "name": "Setup Supabase environment",
            "uses": "./.github/actions/setup-supabase-rest",
            "with": {
              "supabase-url": "${{ secrets.SUPABASE_URL }}",
              "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}",
              "database-url": "${{ secrets.DATABASE_URL }}"
            }
          },
          {
            "name": "Platform-specific setup",
            "run": "echo \"üîß Setting up environment for ${{ matrix.platform }}...\"\n\ncase \"${{ matrix.platform }}\" in\n  reddit)\n    echo \"REDDIT_CLIENT_ID=${{ secrets.REDDIT_CLIENT_ID }}\" >> $GITHUB_ENV\n    echo \"REDDIT_CLIENT_SECRET=${{ secrets.REDDIT_CLIENT_SECRET }}\" >> $GITHUB_ENV\n    ;;\n  youtube)\n    echo \"YOUTUBE_API_KEY=${{ secrets.YOUTUBE_API_KEY }}\" >> $GITHUB_ENV\n    ;;\n  giphy)\n    echo \"GIPHY_API_KEY=${{ secrets.GIPHY_API_KEY }}\" >> $GITHUB_ENV\n    ;;\n  imgur)\n    echo \"IMGUR_CLIENT_ID=${{ secrets.IMGUR_CLIENT_ID }}\" >> $GITHUB_ENV\n    ;;\n  bluesky)\n    echo \"BLUESKY_IDENTIFIER=${{ secrets.BLUESKY_IDENTIFIER }}\" >> $GITHUB_ENV\n    echo \"BLUESKY_APP_PASSWORD=${{ secrets.BLUESKY_APP_PASSWORD }}\" >> $GITHUB_ENV\n    ;;\n  pixabay)\n    echo \"PIXABAY_API_KEY=${{ secrets.PIXABAY_API_KEY }}\" >> $GITHUB_ENV\n    ;;\n  niche)\n    # Niche platforms may use multiple APIs\n    echo \"LEMMY_INSTANCE_URL=${{ secrets.LEMMY_INSTANCE_URL }}\" >> $GITHUB_ENV\n    echo \"TUMBLR_API_KEY=${{ secrets.TUMBLR_API_KEY }}\" >> $GITHUB_ENV\n    ;;\nesac\n"
          },
          {
            "name": "Scan platform content",
            "id": "scan",
            "run": "echo \"üì° Scanning ${{ matrix.platform }} for hotdog content...\"\n\nMAX_POSTS=\"${{ inputs.max-posts || 50 }}\"\n\ncase \"${{ matrix.platform }}\" in\n  reddit)\n    npm run scan:reddit -- --max-posts=$MAX_POSTS\n    ;;\n  youtube)\n    npm run scan:youtube -- --max-posts=$MAX_POSTS\n    ;;\n  giphy)\n    npm run scan:giphy -- --max-posts=$MAX_POSTS\n    ;;\n  imgur)\n    npm run scan:imgur -- --max-posts=$MAX_POSTS\n    ;;\n  bluesky)\n    npm run scan:bluesky -- --max-posts=$MAX_POSTS\n    ;;\n  tumblr)\n    npm run scan:tumblr -- --max-posts=$MAX_POSTS\n    ;;\n  lemmy)\n    npm run scan:lemmy -- --max-posts=$MAX_POSTS\n    ;;\n  pixabay)\n    npm run scan:pixabay -- --max-posts=$MAX_POSTS\n    ;;\n  niche)\n    # Scan multiple niche platforms with smaller limits\n    npm run scan:niche-platforms -- --max-posts=20\n    ;;\n  *)\n    echo \"‚ùå Unknown platform: ${{ matrix.platform }}\"\n    exit 1\n    ;;\nesac\n\necho \"‚úÖ Scan completed for ${{ matrix.platform }}\"\n"
          },
          {
            "name": "Upload scan logs",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "scan-logs-${{ matrix.platform }}",
              "path": "logs/scan-*.log\nreports/scan-*.json\n",
              "retention-days": 3,
              "if-no-files-found": "ignore"
            }
          }
        ]
      },
      "summary": {
        "name": "Scan Summary",
        "runs-on": "ubuntu-latest",
        "needs": [
          "plan-scans",
          "demand-driven-scan",
          "determine-platforms",
          "scan"
        ],
        "if": "always() && !cancelled()",
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "20",
              "cache-key-suffix": "scanner-summary"
            }
          },
          {
            "name": "Setup Supabase environment",
            "uses": "./.github/actions/setup-supabase-rest",
            "with": {
              "supabase-url": "${{ secrets.SUPABASE_URL }}",
              "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}",
              "database-url": "${{ secrets.DATABASE_URL }}"
            }
          },
          {
            "name": "Generate scan summary",
            "run": "echo \"üìä Generating content scan summary...\"\nnpm run scan:summary || echo \"Summary generation completed\"\n"
          },
          {
            "name": "Create GitHub step summary",
            "run": "echo \"## üì° Content Scanner Results\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Trigger:** ${{ github.event_name }}\" >> $GITHUB_STEP_SUMMARY\n\n# Check if demand-driven or legacy\nif [ \"${{ needs.plan-scans.outputs.reason }}\" != \"\" ]; then\n  echo \"**Mode:** Demand-Driven\" >> $GITHUB_STEP_SUMMARY\n  echo \"**Reason:** ${{ needs.plan-scans.outputs.reason }}\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"**Mode:** Legacy/Manual\" >> $GITHUB_STEP_SUMMARY\n  echo \"**Schedule Type:** ${{ needs.determine-platforms.outputs.schedule-type }}\" >> $GITHUB_STEP_SUMMARY\n  echo \"**Platforms:** ${{ needs.determine-platforms.outputs.platforms }}\" >> $GITHUB_STEP_SUMMARY\nfi\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"| Platform | Status | Notes |\" >> $GITHUB_STEP_SUMMARY\necho \"|----------|--------|-------|\" >> $GITHUB_STEP_SUMMARY\n\n# Create a simplified status report\nPLATFORMS=($(echo \"${{ needs.determine-platforms.outputs.platforms }}\" | tr ',' ' '))\nfor platform in \"${PLATFORMS[@]}\"; do\n  STATUS=\"unknown\"\n  # This is simplified - you'd need to extract actual results\n  echo \"| $platform | $STATUS | Scan attempted |\" >> $GITHUB_STEP_SUMMARY\ndone\n\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"üìà **Next Steps:**\" >> $GITHUB_STEP_SUMMARY\necho \"- Content will be queued for review and scheduling\" >> $GITHUB_STEP_SUMMARY\necho \"- Check admin dashboard for new content\" >> $GITHUB_STEP_SUMMARY\n"
          },
          {
            "name": "Upload summary report",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "scan-summary-report",
              "path": "reports/scan-summary.json\nreports/scan-summary.md\n",
              "retention-days": 7,
              "if-no-files-found": "ignore"
            }
          }
        ]
      },
      "rate-limit-check": {
        "name": "Rate Limit Health Check",
        "runs-on": "ubuntu-latest",
        "needs": "scan",
        "if": "always()",
        "timeout-minutes": 5,
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "20",
              "cache-key-suffix": "rate-limit-check"
            }
          },
          {
            "name": "Check API rate limits",
            "run": "echo \"üîç Checking API rate limit status...\"\n\n# Check rate limits for various APIs\necho \"API Rate Limit Status:\" >> rate-limit-report.md\necho \"=====================\" >> rate-limit-report.md\necho \"\" >> rate-limit-report.md\n\n# This would ideally query actual rate limit status\necho \"- Reddit API: $(date)\" >> rate-limit-report.md\necho \"- YouTube API: $(date)\" >> rate-limit-report.md\necho \"- Giphy API: $(date)\" >> rate-limit-report.md\necho \"- Imgur API: $(date)\" >> rate-limit-report.md\necho \"\" >> rate-limit-report.md\necho \"Generated at: $(date)\" >> rate-limit-report.md\n\necho \"‚úÖ Rate limit check completed\"\n"
          },
          {
            "name": "Upload rate limit report",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "rate-limit-report",
              "path": "rate-limit-report.md",
              "retention-days": 1
            }
          }
        ]
      }
    },
    "intent": "Scans social media platforms for hotdog-related content",
    "category": "scanning"
  },
  {
    "filename": "schedule-reconcile.yml",
    "name": "Schedule Reconciliation",
    "on": {
      "schedule": [
        {
          "cron": "30 6 * * *"
        }
      ],
      "workflow_dispatch": {
        "inputs": {
          "date": {
            "description": "Target date (YYYY-MM-DD, defaults to yesterday)",
            "required": false,
            "type": "string"
          },
          "force_backfill": {
            "description": "Force backfill even if no orphans found",
            "required": false,
            "type": "boolean",
            "default": false
          }
        }
      }
    },
    "env": {
      "NODE_ENV": "production"
    },
    "jobs": {
      "reconcile": {
        "name": "Schedule Reconciliation",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 10,
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js",
            "uses": "actions/setup-node@v4",
            "with": {
              "node-version": "18",
              "cache": "npm"
            }
          },
          {
            "name": "Install dependencies",
            "run": "npm ci --production"
          },
          {
            "name": "Determine target date",
            "id": "date",
            "run": "if [ -n \"${{ github.event.inputs.date }}\" ]; then\n  TARGET_DATE=\"${{ github.event.inputs.date }}\"\nelse\n  # Default to yesterday in ET timezone\n  TARGET_DATE=$(TZ=America/New_York date -d \"yesterday\" +%Y-%m-%d)\nfi\necho \"target_date=$TARGET_DATE\" >> $GITHUB_OUTPUT\necho \"üóìÔ∏è Target date: $TARGET_DATE\"\n"
          },
          {
            "name": "Health Check - Timezone Handling",
            "id": "health_tz",
            "run": "echo \"üïí Checking timezone health...\"\n\nRESPONSE=$(curl -s -w \"%{http_code}\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  \"${{ vars.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/health/schedule-tz?date=${{ steps.date.outputs.target_date }}\")\n\nHTTP_CODE=\"${RESPONSE: -3}\"\nBODY=\"${RESPONSE%???}\"\n\necho \"Response code: $HTTP_CODE\"\necho \"timezone_health_status=$HTTP_CODE\" >> $GITHUB_OUTPUT\n\nif [ \"$HTTP_CODE\" != \"200\" ]; then\n  echo \"‚ùå Timezone health check failed\"\n  echo \"$BODY\" | jq -r '.issues[]?' || echo \"$BODY\"\n  echo \"timezone_issues=true\" >> $GITHUB_OUTPUT\nelse\n  echo \"‚úÖ Timezone health check passed\"\n  echo \"timezone_issues=false\" >> $GITHUB_OUTPUT\nfi\n"
          },
          {
            "name": "Health Check - Posting Source of Truth",
            "id": "health_posting",
            "run": "echo \"üîç Checking posting source of truth health...\"\n\nRESPONSE=$(curl -s -w \"%{http_code}\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  \"${{ vars.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/health/posting-source-of-truth\")\n\nHTTP_CODE=\"${RESPONSE: -3}\"\nBODY=\"${RESPONSE%???}\"\n\necho \"Response code: $HTTP_CODE\"\necho \"posting_health_status=$HTTP_CODE\" >> $GITHUB_OUTPUT\n\nORPHAN_COUNT=$(echo \"$BODY\" | jq -r '.orphan_posts // 0')\nCOMPLIANCE_SCORE=$(echo \"$BODY\" | jq -r '.posting_compliance_score // 0')\n\necho \"orphan_count=$ORPHAN_COUNT\" >> $GITHUB_OUTPUT\necho \"compliance_score=$COMPLIANCE_SCORE\" >> $GITHUB_OUTPUT\n\nif [ \"$HTTP_CODE\" != \"200\" ] || [ \"$ORPHAN_COUNT\" -gt 5 ] || [ \"$COMPLIANCE_SCORE\" -lt 90 ]; then\n  echo \"‚ùå Posting health check indicates issues\"\n  echo \"Orphan posts: $ORPHAN_COUNT, Compliance: $COMPLIANCE_SCORE%\"\n  echo \"$BODY\" | jq -r '.issues[]?' || echo \"$BODY\"\n  echo \"posting_issues=true\" >> $GITHUB_OUTPUT\nelse\n  echo \"‚úÖ Posting health check passed\"\n  echo \"posting_issues=false\" >> $GITHUB_OUTPUT\nfi\n"
          },
          {
            "name": "Run Backfill Job",
            "id": "backfill",
            "if": "steps.health_posting.outputs.orphan_count > 0 || \ngithub.event.inputs.force_backfill == 'true'\n",
            "env": {
              "DATABASE_URL": "${{ secrets.DATABASE_URL }}",
              "SUPABASE_URL": "${{ secrets.SUPABASE_URL }}",
              "SUPABASE_SERVICE_ROLE_KEY": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}"
            },
            "run": "echo \"üîß Running backfill job for orphan posts...\"\necho \"Target date: ${{ steps.date.outputs.target_date }}\"\necho \"Orphan count detected: ${{ steps.health_posting.outputs.orphan_count }}\"\n\n# Run backfill job with write mode\nnpx tsx scripts/ops/backfill-post-links.ts \\\n  --date \"${{ steps.date.outputs.target_date }}\" \\\n  --write \\\n  --verbose\n\n# Check if report was generated\nif [ -f \"ci_audit/backfill-${{ steps.date.outputs.target_date }}.md\" ]; then\n  echo \"backfill_report_generated=true\" >> $GITHUB_OUTPUT\n  echo \"‚úÖ Backfill report generated\"\nelse\n  echo \"backfill_report_generated=false\" >> $GITHUB_OUTPUT\n  echo \"‚ö†Ô∏è No backfill report generated\"\nfi\n"
          },
          {
            "name": "Generate Daily Report",
            "id": "report",
            "run": "echo \"üìä Generating daily reconciliation report...\"\n\n# Create comprehensive report\nREPORT_FILE=\"ci_audit/reconcile-${{ steps.date.outputs.target_date }}.md\"\nmkdir -p ci_audit\n\ncat > \"$REPORT_FILE\" << 'EOF'\n# Schedule Reconciliation Report\n\n**Date:** ${{ steps.date.outputs.target_date }}  \n**Generated:** $(date -u +\"%Y-%m-%d %H:%M:%S UTC\")  \n**Workflow:** ${{ github.workflow }} #${{ github.run_number }}  \n\n## Health Check Results\n\n### Timezone Handling\n- **Status:** ${{ steps.health_tz.outputs.timezone_health_status == '200' && '‚úÖ Healthy' || '‚ùå Issues Detected' }}\n- **HTTP Code:** ${{ steps.health_tz.outputs.timezone_health_status }}\n- **Issues Found:** ${{ steps.health_tz.outputs.timezone_issues }}\n\n### Posting Source of Truth\n- **Status:** ${{ steps.health_posting.outputs.posting_health_status == '200' && '‚úÖ Healthy' || '‚ùå Issues Detected' }}\n- **HTTP Code:** ${{ steps.health_posting.outputs.posting_health_status }}\n- **Orphan Posts:** ${{ steps.health_posting.outputs.orphan_count }}\n- **Compliance Score:** ${{ steps.health_posting.outputs.compliance_score }}%\n- **Issues Found:** ${{ steps.health_posting.outputs.posting_issues }}\n\n## Backfill Results\n\n${{ steps.backfill.outcome == 'success' && '‚úÖ Backfill job completed successfully' || steps.backfill.outcome == 'skipped' && '‚è≠Ô∏è Backfill job skipped (no orphan posts)' || '‚ùå Backfill job failed or was not run' }}\n\n${{ steps.backfill.outputs.backfill_report_generated == 'true' && 'üìÑ Detailed backfill report available: `ci_audit/backfill-${{ steps.date.outputs.target_date }}.md`' || '' }}\n\n## Recommendations\n\n${{ steps.health_tz.outputs.timezone_issues == 'true' && '- üïí **Timezone Issues:** Review timezone conversion logic and DST handling' || '' }}\n${{ steps.health_posting.outputs.posting_issues == 'true' && '- üîç **Posting Issues:** Review scheduled_posts table integrity and posting service configuration' || '' }}\n${{ steps.health_posting.outputs.orphan_count > 5 && '- üîß **High Orphan Count:** Consider running backfill job more frequently or investigating posting service' || '' }}\n${{ steps.health_posting.outputs.compliance_score < 90 && '- üìä **Low Compliance:** Enable ENFORCE_SCHEDULE_SOURCE_OF_TRUTH feature flag and review posting workflows' || '' }}\n\n## Next Steps\n\n- Monitor system health endpoints daily\n- Review any generated backfill reports\n- Ensure ENFORCE_SCHEDULE_SOURCE_OF_TRUTH=true in production\n- Consider manual review if compliance score remains low\n\n---\n*Generated by Schedule Reconciliation workflow*\nEOF\n\necho \"report_file=$REPORT_FILE\" >> $GITHUB_OUTPUT\necho \"‚úÖ Daily report generated: $REPORT_FILE\"\n"
          },
          {
            "name": "Upload Reports",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "reconciliation-reports-${{ steps.date.outputs.target_date }}",
              "path": "ci_audit/",
              "retention-days": 30
            }
          },
          {
            "name": "Determine Workflow Status",
            "id": "status",
            "run": "# Determine overall workflow status\nif [[ \"${{ steps.health_tz.outputs.timezone_issues }}\" == \"true\" ]] || \\\n   [[ \"${{ steps.health_posting.outputs.posting_issues }}\" == \"true\" ]] || \\\n   [[ \"${{ steps.backfill.outcome }}\" == \"failure\" ]]; then\n  echo \"workflow_status=failure\" >> $GITHUB_OUTPUT\n  echo \"‚ùå Workflow completed with issues\"\nelse\n  echo \"workflow_status=success\" >> $GITHUB_OUTPUT\n  echo \"‚úÖ Workflow completed successfully\"\nfi\n"
          },
          {
            "name": "Notify on Failure",
            "if": "steps.status.outputs.workflow_status == 'failure'",
            "run": "echo \"üö® RECONCILIATION ISSUES DETECTED\"\necho \"\"\necho \"Date: ${{ steps.date.outputs.target_date }}\"\necho \"Timezone Health: ${{ steps.health_tz.outputs.timezone_issues == 'true' && 'ISSUES' || 'OK' }}\"\necho \"Posting Health: ${{ steps.health_posting.outputs.posting_issues == 'true' && 'ISSUES' || 'OK' }}\"\necho \"Orphan Posts: ${{ steps.health_posting.outputs.orphan_count }}\"\necho \"Compliance Score: ${{ steps.health_posting.outputs.compliance_score }}%\"\necho \"Backfill Status: ${{ steps.backfill.outcome }}\"\necho \"\"\necho \"Please review the generated reports and take corrective action.\"\necho \"Reports available in workflow artifacts.\"\n\n# Exit with failure to mark workflow as failed\nexit 1\n"
          },
          {
            "name": "Success Summary",
            "if": "steps.status.outputs.workflow_status == 'success'",
            "run": "echo \"‚úÖ RECONCILIATION COMPLETED SUCCESSFULLY\"\necho \"\"\necho \"Date: ${{ steps.date.outputs.target_date }}\"\necho \"All health checks passed\"\necho \"Orphan Posts: ${{ steps.health_posting.outputs.orphan_count }}\"\necho \"Compliance Score: ${{ steps.health_posting.outputs.compliance_score }}%\"\necho \"System operating within expected parameters\"\n"
          }
        ]
      }
    },
    "intent": "Creates scheduled_posts rows for upcoming posting slots",
    "category": "test"
  },
  {
    "filename": "scheduler.yml",
    "name": "Content Scheduler",
    "on": {
      "schedule": [
        {
          "cron": "0 1 * * *"
        },
        {
          "cron": "0 12 * * *"
        },
        {
          "cron": "0 0 * * 0"
        }
      ],
      "workflow_dispatch": {
        "inputs": {
          "operation": {
            "description": "Operation to perform",
            "type": "choice",
            "options": [
              "refill",
              "forecast",
              "reconcile",
              "twoDays"
            ],
            "default": "refill"
          },
          "days": {
            "description": "Number of days (for refill/forecast)",
            "type": "number",
            "default": 2
          }
        }
      },
      "workflow_call": {
        "inputs": {
          "operation": {
            "description": "Operation to perform",
            "type": "string",
            "default": "refill"
          },
          "days": {
            "description": "Number of days",
            "type": "number",
            "default": 2
          }
        }
      }
    },
    "concurrency": {
      "group": "scheduler-${{ github.ref }}-${{ inputs.operation || 'scheduled' }}",
      "cancel-in-progress": true
    },
    "env": {
      "NODE_ENV": "production",
      "CI": true
    },
    "jobs": {
      "determine-operation": {
        "name": "Determine Operation",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 2,
        "outputs": {
          "operation": "${{ steps.strategy.outputs.operation }}",
          "days": "${{ steps.strategy.outputs.days }}",
          "auth-token": "${{ steps.strategy.outputs.auth-token }}"
        },
        "steps": [
          {
            "name": "Determine operation strategy",
            "id": "strategy",
            "run": "OPERATION=\"${{ inputs.operation || 'refill' }}\"\nDAYS=\"${{ inputs.days || 2 }}\"\n\n# Determine operation based on schedule\nif [[ \"${{ github.event_name }}\" == \"schedule\" ]]; then\n  HOUR=$(date +%H)\n  \n  if [[ $HOUR -eq 1 ]]; then\n    OPERATION=\"refill\"\n    DAYS=2\n    echo \"üåÖ Morning refill - scheduling next 2 days\"\n  elif [[ $HOUR -eq 12 ]]; then\n    OPERATION=\"forecast\"\n    DAYS=3\n    echo \"üåû Midday forecast check - analyzing next 3 days\"\n  elif [[ $HOUR -eq 0 ]]; then\n    OPERATION=\"reconcile\"\n    DAYS=7\n    echo \"üåô Weekly reconcile - checking past 7 days\"\n  fi\nfi\n\necho \"operation=$OPERATION\" >> $GITHUB_OUTPUT\necho \"days=$DAYS\" >> $GITHUB_OUTPUT\necho \"auth-token=${{ secrets.AUTH_TOKEN }}\" >> $GITHUB_OUTPUT\necho \"Selected operation: $OPERATION for $DAYS days\"\n"
          }
        ]
      },
      "refill": {
        "name": "Refill Schedule",
        "runs-on": "ubuntu-latest",
        "needs": "determine-operation",
        "if": "needs.determine-operation.outputs.operation == 'refill' || needs.determine-operation.outputs.operation == 'twoDays'",
        "timeout-minutes": 10,
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "20",
              "cache-key-suffix": "scheduler-refill"
            }
          },
          {
            "name": "Setup Supabase environment",
            "uses": "./.github/actions/setup-supabase-rest",
            "with": {
              "supabase-url": "${{ secrets.SUPABASE_URL }}",
              "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}",
              "database-url": "${{ secrets.DATABASE_URL }}"
            }
          },
          {
            "name": "Refill content schedule",
            "id": "refill",
            "run": "echo \"üîÑ Refilling content schedule for ${{ needs.determine-operation.outputs.days }} days...\"\n\nDAYS=\"${{ needs.determine-operation.outputs.days }}\"\nAUTH_TOKEN=\"${{ needs.determine-operation.outputs.auth-token }}\"\n\n# Call the refill API endpoint\nRESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -X POST \\\n  -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\\\"days\\\": $DAYS}\" \\\n  \"${{ secrets.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/admin/schedule/refill\" \\\n  --max-time 300)\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//')\n\necho \"refill-status=$HTTP_STATUS\" >> $GITHUB_OUTPUT\necho \"refill-response<<EOF\" >> $GITHUB_OUTPUT\necho \"$BODY\" >> $GITHUB_OUTPUT\necho \"EOF\" >> $GITHUB_OUTPUT\n\nif [ \"$HTTP_STATUS\" -eq 200 ]; then\n  echo \"‚úÖ Schedule refill completed successfully\"\nelse\n  echo \"‚ùå Schedule refill failed with status $HTTP_STATUS\"\n  echo \"Response: $BODY\"\n  exit 1\nfi\n"
          },
          {
            "name": "Upload refill logs",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "scheduler-refill-logs",
              "path": "logs/scheduler-*.log\nreports/refill-*.json\n",
              "retention-days": 7,
              "if-no-files-found": "ignore"
            }
          }
        ]
      },
      "forecast": {
        "name": "Generate Forecast",
        "runs-on": "ubuntu-latest",
        "needs": "determine-operation",
        "if": "needs.determine-operation.outputs.operation == 'forecast'",
        "timeout-minutes": 8,
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "20",
              "cache-key-suffix": "scheduler-forecast"
            }
          },
          {
            "name": "Setup Supabase environment",
            "uses": "./.github/actions/setup-supabase-rest",
            "with": {
              "supabase-url": "${{ secrets.SUPABASE_URL }}",
              "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}",
              "database-url": "${{ secrets.DATABASE_URL }}"
            }
          },
          {
            "name": "Generate schedule forecast",
            "id": "forecast",
            "run": "echo \"üîÆ Generating schedule forecast for ${{ needs.determine-operation.outputs.days }} days...\"\n\nDAYS=\"${{ needs.determine-operation.outputs.days }}\"\nAUTH_TOKEN=\"${{ needs.determine-operation.outputs.auth-token }}\"\n\n# Get forecast for multiple days\nfor i in $(seq 0 $((DAYS-1))); do\n  DATE=$(date -d \"+$i days\" +%Y-%m-%d)\n  echo \"üìÖ Generating forecast for $DATE...\"\n  \n  RESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n    -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n    \"${{ secrets.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/admin/schedule/forecast?date=$DATE\" \\\n    --max-time 60)\n  \n  HTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\n  BODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//')\n  \n  if [ \"$HTTP_STATUS\" -eq 200 ]; then\n    echo \"‚úÖ Forecast for $DATE generated successfully\"\n    echo \"$BODY\" > \"forecast-$DATE.json\"\n  else\n    echo \"‚ö†Ô∏è Forecast for $DATE failed with status $HTTP_STATUS\"\n    echo \"Response: $BODY\"\n  fi\ndone\n\necho \"‚úÖ Forecast generation completed\"\n"
          },
          {
            "name": "Upload forecast reports",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "scheduler-forecast-reports",
              "path": "forecast-*.json\nlogs/forecast-*.log\n",
              "retention-days": 7,
              "if-no-files-found": "ignore"
            }
          }
        ]
      },
      "reconcile": {
        "name": "Reconcile Content",
        "runs-on": "ubuntu-latest",
        "needs": "determine-operation",
        "if": "needs.determine-operation.outputs.operation == 'reconcile'",
        "timeout-minutes": 15,
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "20",
              "cache-key-suffix": "scheduler-reconcile"
            }
          },
          {
            "name": "Setup Supabase environment",
            "uses": "./.github/actions/setup-supabase-rest",
            "with": {
              "supabase-url": "${{ secrets.SUPABASE_URL }}",
              "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}",
              "database-url": "${{ secrets.DATABASE_URL }}"
            }
          },
          {
            "name": "Reconcile content and schedule",
            "id": "reconcile",
            "run": "echo \"üîÑ Reconciling content and schedule for past ${{ needs.determine-operation.outputs.days }} days...\"\n\nDAYS=\"${{ needs.determine-operation.outputs.days }}\"\nAUTH_TOKEN=\"${{ needs.determine-operation.outputs.auth-token }}\"\n\n# Call the reconcile API endpoint\nRESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -X POST \\\n  -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\\\"days\\\": $DAYS}\" \\\n  \"${{ secrets.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/admin/schedule/reconcile\" \\\n  --max-time 600)\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//')\n\necho \"reconcile-status=$HTTP_STATUS\" >> $GITHUB_OUTPUT\necho \"reconcile-response<<EOF\" >> $GITHUB_OUTPUT\necho \"$BODY\" >> $GITHUB_OUTPUT\necho \"EOF\" >> $GITHUB_OUTPUT\n\nif [ \"$HTTP_STATUS\" -eq 200 ]; then\n  echo \"‚úÖ Content reconciliation completed successfully\"\nelse\n  echo \"‚ùå Content reconciliation failed with status $HTTP_STATUS\"\n  echo \"Response: $BODY\"\n  exit 1\nfi\n"
          },
          {
            "name": "Upload reconcile logs",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "scheduler-reconcile-logs",
              "path": "logs/reconcile-*.log\nreports/reconcile-*.json\n",
              "retention-days": 14,
              "if-no-files-found": "ignore"
            }
          }
        ]
      },
      "queue-health": {
        "name": "Queue Health Check",
        "runs-on": "ubuntu-latest",
        "needs": [
          "determine-operation",
          "refill",
          "forecast",
          "reconcile"
        ],
        "if": "always()",
        "timeout-minutes": 5,
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "20",
              "cache-key-suffix": "scheduler-health"
            }
          },
          {
            "name": "Setup Supabase environment",
            "uses": "./.github/actions/setup-supabase-rest",
            "with": {
              "supabase-url": "${{ secrets.SUPABASE_URL }}",
              "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}",
              "database-url": "${{ secrets.DATABASE_URL }}"
            }
          },
          {
            "name": "Check queue health",
            "run": "echo \"üè• Checking content queue health...\"\n\nAUTH_TOKEN=\"${{ needs.determine-operation.outputs.auth-token }}\"\n\n# Get queue metrics\nRESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n  \"${{ secrets.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/admin/metrics\" \\\n  --max-time 30)\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//')\n\nif [ \"$HTTP_STATUS\" -eq 200 ]; then\n  echo \"‚úÖ Queue health check completed\"\n  echo \"$BODY\" > queue-health.json\nelse\n  echo \"‚ö†Ô∏è Queue health check failed with status $HTTP_STATUS\"\n  echo \"Response: $BODY\"\nfi\n"
          },
          {
            "name": "Upload health report",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "queue-health-report",
              "path": "queue-health.json",
              "retention-days": 3,
              "if-no-files-found": "ignore"
            }
          }
        ]
      },
      "summary": {
        "name": "Scheduler Summary",
        "runs-on": "ubuntu-latest",
        "needs": [
          "determine-operation",
          "refill",
          "forecast",
          "reconcile",
          "queue-health"
        ],
        "if": "always()",
        "steps": [
          {
            "name": "Generate scheduler summary",
            "run": "echo \"## üìÖ Content Scheduler Results\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Operation:** ${{ needs.determine-operation.outputs.operation }}\" >> $GITHUB_STEP_SUMMARY\necho \"**Days:** ${{ needs.determine-operation.outputs.days }}\" >> $GITHUB_STEP_SUMMARY\necho \"**Trigger:** ${{ github.event_name }}\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"| Job | Status | Notes |\" >> $GITHUB_STEP_SUMMARY\necho \"|-----|--------|-------|\" >> $GITHUB_STEP_SUMMARY\n\n# Add job statuses\nif [[ \"${{ needs.refill.result }}\" != \"\" ]]; then\n  echo \"| Refill | ${{ needs.refill.result }} | Schedule refill operation |\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [[ \"${{ needs.forecast.result }}\" != \"\" ]]; then\n  echo \"| Forecast | ${{ needs.forecast.result }} | Schedule forecast generation |\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [[ \"${{ needs.reconcile.result }}\" != \"\" ]]; then\n  echo \"| Reconcile | ${{ needs.reconcile.result }} | Content reconciliation |\" >> $GITHUB_STEP_SUMMARY\nfi\n\necho \"| Queue Health | ${{ needs.queue-health.result }} | Content queue health check |\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Overall status\nOVERALL_SUCCESS=true\n\nif [[ \"${{ needs.refill.result }}\" == \"failure\" ]] || [[ \"${{ needs.forecast.result }}\" == \"failure\" ]] || [[ \"${{ needs.reconcile.result }}\" == \"failure\" ]]; then\n  OVERALL_SUCCESS=false\nfi\n\nif [[ \"$OVERALL_SUCCESS\" == \"true\" ]]; then\n  echo \"## ‚úÖ Scheduler operations completed successfully\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"## ‚ùå Some scheduler operations failed\" >> $GITHUB_STEP_SUMMARY\n  echo \"Check individual job logs for details.\" >> $GITHUB_STEP_SUMMARY\nfi\n\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"üìà **Next Steps:**\" >> $GITHUB_STEP_SUMMARY\necho \"- Content is scheduled and ready for posting\" >> $GITHUB_STEP_SUMMARY\necho \"- Check admin dashboard for schedule overview\" >> $GITHUB_STEP_SUMMARY\n"
          }
        ]
      }
    },
    "intent": "Creates scheduled_posts rows for upcoming posting slots",
    "category": "scanning"
  },
  {
    "filename": "secret-validation.yml",
    "name": "Secret Validation",
    "on": {
      "push": {
        "branches": [
          "main",
          "develop"
        ]
      },
      "pull_request": {
        "branches": [
          "main",
          "develop"
        ]
      },
      "schedule": [
        {
          "cron": "0 9 * * 1"
        }
      ]
    },
    "jobs": {
      "validate-secrets": {
        "runs-on": "ubuntu-latest",
        "name": "Validate Secret Strength & Environment Variables",
        "steps": [
          {
            "name": "Checkout",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js",
            "uses": "actions/setup-node@v4",
            "with": {
              "node-version": "18",
              "cache": "npm"
            }
          },
          {
            "name": "Install dependencies",
            "run": "npm ci"
          },
          {
            "name": "Validate secret strength",
            "env": {
              "JWT_SECRET": "${{ secrets.JWT_SECRET }}",
              "AUTH_TOKEN": "${{ secrets.AUTH_TOKEN }}",
              "CRON_TOKEN": "${{ secrets.CRON_TOKEN }}",
              "ADMIN_PASSWORD": "${{ secrets.ADMIN_PASSWORD }}"
            },
            "run": "echo \"üîç Running secret validation...\"\nnpm run validate-secrets -- --verbose\n"
          },
          {
            "name": "Validate in strict mode",
            "env": {
              "JWT_SECRET": "${{ secrets.JWT_SECRET }}",
              "AUTH_TOKEN": "${{ secrets.AUTH_TOKEN }}",
              "CRON_TOKEN": "${{ secrets.CRON_TOKEN }}",
              "ADMIN_PASSWORD": "${{ secrets.ADMIN_PASSWORD }}"
            },
            "run": "echo \"üîí Running strict validation (warnings = errors)...\"\nnpm run validate-secrets -- --strict\n"
          },
          {
            "name": "Comment on PR (if failed)",
            "if": "failure() && github.event_name == 'pull_request'",
            "uses": "actions/github-script@v7",
            "with": {
              "script": "github.rest.issues.createComment({\n  issue_number: context.issue.number,\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  body: `## ‚ùå Secret Validation Failed\n  \n  The secret validation check failed. This could be due to:\n  \n  - **Weak tokens**: Tokens that don't meet security requirements (length < 32, weak patterns)\n  - **Missing environment variables**: Variables used in code but not documented in .env.example\n  - **Format violations**: Invalid hex/base64/alphanumeric formats\n  \n  ### Next Steps\n  \n  1. Check the workflow logs for specific validation errors\n  2. Use \\`npm run validate-secrets -- --verbose\\` locally to debug\n  3. For token issues, use \\`npm run rotate-tokens <TOKEN_NAME>\\` to generate new tokens\n  4. For missing env vars, add them to .env.example\n  \n  ### Security Note\n  \n  This validation ensures all secrets meet security standards. Do not bypass these checks without security team approval.\n  `\n})\n"
            }
          }
        ]
      },
      "environment-completeness": {
        "runs-on": "ubuntu-latest",
        "name": "Environment Variable Completeness Check",
        "steps": [
          {
            "name": "Checkout",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js",
            "uses": "actions/setup-node@v4",
            "with": {
              "node-version": "18",
              "cache": "npm"
            }
          },
          {
            "name": "Install dependencies",
            "run": "npm ci"
          },
          {
            "name": "Check environment completeness",
            "run": "echo \"üìã Checking environment variable completeness...\"\n\n# Extract all process.env references from codebase\necho \"Variables referenced in codebase:\"\ngrep -r \"process\\.env\\.\" app/ lib/ components/ scripts/ middleware.* next.config.* 2>/dev/null \\\n  | grep -oE 'process\\.env\\.([A-Z_][A-Z0-9_]*)' \\\n  | sed 's/process\\.env\\.//' \\\n  | sort -u \\\n  | tee /tmp/codebase_vars.txt\n\necho \"\"\necho \"Variables in .env.example:\"\nif [ -f .env.example ]; then\n  grep -E '^[A-Z_][A-Z0-9_]*=' .env.example | cut -d= -f1 | sort | tee /tmp/env_example_vars.txt\nelse\n  echo \"‚ö†Ô∏è .env.example not found\"\n  touch /tmp/env_example_vars.txt\nfi\n\necho \"\"\necho \"Missing from .env.example:\"\ncomm -23 /tmp/codebase_vars.txt /tmp/env_example_vars.txt | tee /tmp/missing_vars.txt\n\n# Filter out system variables that don't need to be in .env.example\nSYSTEM_VARS=\"NODE_ENV PORT PWD PATH HOME USER VERCEL VERCEL_ENV VERCEL_URL VERCEL_REGION GITHUB_ACTIONS GITHUB_SHA GITHUB_REF CI BUILD_ID NEXT_RUNTIME\"\n\nfor var in $SYSTEM_VARS; do\n  sed -i \"/^$var$/d\" /tmp/missing_vars.txt 2>/dev/null || true\ndone\n\nif [ -s /tmp/missing_vars.txt ]; then\n  echo \"‚ùå Missing variables found:\"\n  cat /tmp/missing_vars.txt\n  echo \"\"\n  echo \"Please add these variables to .env.example with appropriate example values.\"\n  exit 1\nelse\n  echo \"‚úÖ All codebase variables are documented in .env.example\"\nfi\n"
          }
        ]
      },
      "token-strength-check": {
        "runs-on": "ubuntu-latest",
        "name": "Token Strength Verification",
        "if": "github.event_name == 'schedule'",
        "steps": [
          {
            "name": "Checkout",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js",
            "uses": "actions/setup-node@v4",
            "with": {
              "node-version": "18",
              "cache": "npm"
            }
          },
          {
            "name": "Install dependencies",
            "run": "npm ci"
          },
          {
            "name": "Check token rotation schedule",
            "env": {
              "JWT_SECRET": "${{ secrets.JWT_SECRET }}",
              "AUTH_TOKEN": "${{ secrets.AUTH_TOKEN }}",
              "CRON_TOKEN": "${{ secrets.CRON_TOKEN }}",
              "ADMIN_PASSWORD": "${{ secrets.ADMIN_PASSWORD }}"
            },
            "run": "echo \"üóìÔ∏è Checking token rotation schedule...\"\n\n# Check if any tokens need rotation based on docs/secrets.md\n# This is a simplified check - in production you might want to store\n# rotation dates in a database or separate tracking system\n\nCURRENT_DATE=$(date +%Y-%m-%d)\necho \"Current date: $CURRENT_DATE\"\n\n# Check docs/secrets.md for last rotation dates\nif [ -f docs/secrets.md ]; then\n  echo \"Last rotation dates from docs/secrets.md:\"\n  grep -E \"JWT_SECRET|AUTH_TOKEN|CRON_TOKEN|ADMIN_PASSWORD\" docs/secrets.md || true\nfi\n\n# Run token validation to ensure current tokens are still strong\nnpm run validate-secrets -- --verbose\n\necho \"üìä Token strength verification completed\"\n"
          },
          {
            "name": "Create rotation issue (if needed)",
            "if": "failure()",
            "uses": "actions/github-script@v7",
            "with": {
              "script": "const title = `üîê Token Rotation Required - ${new Date().toISOString().split('T')[0]}`\nconst body = `## Token Rotation Required\n\nThe weekly secret audit has detected tokens that may need rotation.\n\n### Action Required\n\n1. Review token rotation schedule in \\`docs/secrets.md\\`\n2. Identify tokens due for rotation\n3. Use rotation script: \\`npm run rotate-tokens <TOKEN_NAME>\\`\n4. Update all storage locations\n5. Test functionality with new tokens\n6. Update rotation log\n\n### Automation\n\nThis issue was created automatically by the secret validation workflow.\n\n/cc @security-team\n`\n\ngithub.rest.issues.create({\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  title: title,\n  body: body,\n  labels: ['security', 'rotation', 'automated']\n})\n"
            }
          }
        ]
      }
    },
    "intent": "General workflow (purpose unclear from name)",
    "category": "maintenance"
  },
  {
    "filename": "spec-drift.yml",
    "name": "OpenAPI Spec Drift Detection",
    "on": {
      "pull_request": {
        "paths": [
          "app/api/**/*.ts",
          "docs/openapi.yaml",
          "docs/openapi.ignore.json",
          "scripts/route-inventory.ts"
        ]
      },
      "push": {
        "branches": [
          "main"
        ],
        "paths": [
          "app/api/**/*.ts",
          "docs/openapi.yaml",
          "docs/openapi.ignore.json"
        ]
      },
      "workflow_dispatch": null,
      "schedule": [
        {
          "cron": "0 6 * * 1"
        }
      ]
    },
    "concurrency": {
      "group": "spec-drift-${{ github.ref }}",
      "cancel-in-progress": true
    },
    "env": {
      "NODE_ENV": "test",
      "CI": true
    },
    "jobs": {
      "validate-openapi-spec": {
        "name": "Validate OpenAPI Specification",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 5,
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "20",
              "cache-key-suffix": "openapi-validation"
            }
          },
          {
            "name": "Install OpenAPI validation tools",
            "run": "npm install -g @redocly/cli swagger-parser\n"
          },
          {
            "name": "Validate OpenAPI specification syntax",
            "run": "echo \"üîç Validating OpenAPI specification syntax...\"\n\n# Check if openapi.yaml exists\nif [ ! -f \"docs/openapi.yaml\" ]; then\n  echo \"‚ùå OpenAPI specification not found at docs/openapi.yaml\"\n  exit 1\nfi\n\n# Validate with Redocly CLI\necho \"üìã Running Redocly validation...\"\nredocly lint docs/openapi.yaml --skip-rule=no-unused-components\n\n# Validate with swagger-parser (additional validation)\necho \"üìã Running swagger-parser validation...\"\nnpx swagger-parser validate docs/openapi.yaml\n\necho \"‚úÖ OpenAPI specification is valid\"\n"
          },
          {
            "name": "Check specification completeness",
            "run": "echo \"üîç Checking OpenAPI specification completeness...\"\n\n# Check for required sections\nREQUIRED_SECTIONS=(\n  \"openapi\"\n  \"info\"\n  \"servers\"\n  \"paths\"\n  \"components.schemas\"\n  \"components.securitySchemes\"\n)\n\nfor section in \"${REQUIRED_SECTIONS[@]}\"; do\n  if ! yq eval \"has(\\\"${section//./\\\".\\\"}\\\"))\" docs/openapi.yaml | grep -q \"true\"; then\n    echo \"‚ùå Missing required section: $section\"\n    exit 1\n  fi\n  echo \"‚úÖ Found section: $section\"\ndone\n\n# Check for security schemes\nif ! yq eval '.components.securitySchemes | has(\"AdminToken\")' docs/openapi.yaml | grep -q \"true\"; then\n  echo \"‚ùå Missing AdminToken security scheme\"\n  exit 1\nfi\n\necho \"‚úÖ OpenAPI specification is complete\"\n"
          }
        ]
      },
      "detect-api-drift": {
        "name": "Detect API Route Drift",
        "runs-on": "ubuntu-latest",
        "needs": "validate-openapi-spec",
        "timeout-minutes": 10,
        "outputs": {
          "drift-detected": "${{ steps.drift-check.outputs.drift-detected }}",
          "missing-routes": "${{ steps.drift-check.outputs.missing-routes }}"
        },
        "steps": [
          {
            "name": "Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Setup Node.js with cache",
            "uses": "./.github/actions/setup-node",
            "with": {
              "node-version": "20",
              "cache-key-suffix": "drift-detection"
            }
          },
          {
            "name": "Generate current route inventory",
            "run": "echo \"üîç Generating current API route inventory...\"\nnpx tsx scripts/route-inventory.ts\n\n# Show inventory summary\necho \"üìä Current route inventory:\"\ncat docs/api-inventory.json | jq '.totalRoutes, .adminRoutes, .publicRoutes'\n"
          },
          {
            "name": "Load ignore list",
            "id": "ignore-list",
            "run": "echo \"üìã Loading route ignore list...\"\n\nif [ -f \"docs/openapi.ignore.json\" ]; then\n  echo \"‚úÖ Found ignore list with $(jq '.ignoredRoutes | length' docs/openapi.ignore.json) entries\"\nelse\n  echo \"‚ö†Ô∏è No ignore list found, creating empty one\"\n  echo '{\"ignoredRoutes\": [], \"ignorePatterns\": []}' > docs/openapi.ignore.json\nfi\n"
          },
          {
            "name": "Extract routes from OpenAPI spec",
            "run": "echo \"üîç Extracting documented routes from OpenAPI spec...\"\n\n# Extract paths from OpenAPI spec\nyq eval '.paths | keys' docs/openapi.yaml | yq eval '.[]' - > documented-routes.txt\n\n# Prefix with /api for comparison\nsed 's|^|/api|' documented-routes.txt > documented-api-routes.txt\n\necho \"üìã Documented routes:\"\ncat documented-api-routes.txt\n"
          },
          {
            "name": "Check for drift",
            "id": "drift-check",
            "run": "echo \"üîç Checking for API route drift...\"\n\n# Extract actual routes from inventory\njq -r '.routes[].path' docs/api-inventory.json > actual-routes.txt\n\n# Extract ignored routes (exact matches)\njq -r '.ignoredRoutes[].path' docs/openapi.ignore.json > ignored-routes.txt\n\n# Extract ignore patterns\njq -r '.ignorePatterns[].pattern' docs/openapi.ignore.json > ignore-patterns.txt\n\n# Function to check if route matches any ignore pattern\nis_ignored() {\n  local route=\"$1\"\n  \n  # Check exact matches\n  if grep -Fxq \"$route\" ignored-routes.txt 2>/dev/null; then\n    return 0\n  fi\n  \n  # Check patterns\n  while IFS= read -r pattern; do\n    if [[ \"$route\" =~ $pattern ]]; then\n      return 0\n    fi\n  done < ignore-patterns.txt 2>/dev/null\n  \n  return 1\n}\n\n# Find missing routes (in code but not in spec and not ignored)\nMISSING_ROUTES=()\n\nwhile IFS= read -r route; do\n  # Skip empty lines\n  [ -z \"$route\" ] && continue\n  \n  # Check if route is documented or ignored\n  if ! grep -Fxq \"$route\" documented-api-routes.txt && ! is_ignored \"$route\"; then\n    MISSING_ROUTES+=(\"$route\")\n    echo \"‚ùå Missing from OpenAPI spec: $route\"\n  fi\ndone < actual-routes.txt\n\n# Check results\nif [ ${#MISSING_ROUTES[@]} -eq 0 ]; then\n  echo \"‚úÖ No API drift detected - all routes are documented or ignored\"\n  echo \"drift-detected=false\" >> $GITHUB_OUTPUT\n  echo \"missing-routes=\" >> $GITHUB_OUTPUT\nelse\n  echo \"‚ö†Ô∏è API drift detected! ${#MISSING_ROUTES[@]} undocumented routes found\"\n  \n  # Format missing routes for output\n  MISSING_JSON=$(printf '%s\\n' \"${MISSING_ROUTES[@]}\" | jq -R . | jq -s .)\n  echo \"drift-detected=true\" >> $GITHUB_OUTPUT\n  echo \"missing-routes=$MISSING_JSON\" >> $GITHUB_OUTPUT\n  \n  # Generate suggestions\n  echo \"\"\n  echo \"üîß To fix this drift, either:\"\n  echo \"1. Add the missing routes to docs/openapi.yaml, or\"\n  echo \"2. Add them to docs/openapi.ignore.json with a reason\"\n  echo \"\"\n  echo \"Missing routes:\"\n  printf '  - %s\\n' \"${MISSING_ROUTES[@]}\"\nfi\n"
          },
          {
            "name": "Upload drift report",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "api-drift-report",
              "path": "docs/api-inventory.json\ndocumented-routes.txt\nactual-routes.txt\nignored-routes.txt\n",
              "retention-days": 14
            }
          }
        ]
      },
      "generate-drift-summary": {
        "name": "Generate Drift Summary",
        "runs-on": "ubuntu-latest",
        "needs": "detect-api-drift",
        "if": "always()",
        "steps": [
          {
            "name": "Generate summary",
            "run": "echo \"## üìã OpenAPI Spec Drift Detection\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\nif [ \"${{ needs.detect-api-drift.outputs.drift-detected }}\" = \"true\" ]; then\n  echo \"### ‚ö†Ô∏è Drift Detected\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"The following API routes exist in code but are not documented in the OpenAPI specification:\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  \n  # Parse missing routes JSON\n  MISSING_ROUTES='${{ needs.detect-api-drift.outputs.missing-routes }}'\n  echo \"$MISSING_ROUTES\" | jq -r '.[]' | while read -r route; do\n    echo \"- \\`$route\\`\" >> $GITHUB_STEP_SUMMARY\n  done\n  \n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"### üîß Resolution Options\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"1. **Document the routes**: Add them to \\`docs/openapi.yaml\\` with proper schemas\" >> $GITHUB_STEP_SUMMARY\n  echo \"2. **Ignore the routes**: Add them to \\`docs/openapi.ignore.json\\` with clear reasons\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"### üìñ Documentation\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"- [OpenAPI Specification](./docs/openapi.yaml)\" >> $GITHUB_STEP_SUMMARY\n  echo \"- [Ignore List](./docs/openapi.ignore.json)\" >> $GITHUB_STEP_SUMMARY\n  echo \"- [Route Inventory Script](./scripts/route-inventory.ts)\" >> $GITHUB_STEP_SUMMARY\n  \nelse\n  echo \"### ‚úÖ No Drift Detected\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"All API routes are properly documented or ignored. The OpenAPI specification is up to date!\" >> $GITHUB_STEP_SUMMARY\nfi\n\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Workflow**: \\`spec-drift.yml\\`\" >> $GITHUB_STEP_SUMMARY\necho \"**Timestamp**: $(date -Iseconds)\" >> $GITHUB_STEP_SUMMARY\n"
          }
        ]
      },
      "fail-on-drift": {
        "name": "Fail on Drift (PR only)",
        "runs-on": "ubuntu-latest",
        "needs": "detect-api-drift",
        "if": "github.event_name == 'pull_request' && needs.detect-api-drift.outputs.drift-detected == 'true'",
        "steps": [
          {
            "name": "Fail PR on drift",
            "run": "echo \"‚ùå PR blocked due to API specification drift\"\necho \"\"\necho \"The following routes are not documented in the OpenAPI specification:\"\necho '${{ needs.detect-api-drift.outputs.missing-routes }}' | jq -r '.[]'\necho \"\"\necho \"Please update docs/openapi.yaml or docs/openapi.ignore.json before merging.\"\nexit 1\n"
          }
        ]
      }
    },
    "intent": "General workflow (purpose unclear from name)",
    "category": "maintenance"
  },
  {
    "filename": "token-refresh.yml",
    "name": "Token Refresh (Reusable)",
    "on": {
      "workflow_call": {
        "outputs": {
          "auth_token": {
            "description": "Refreshed authentication token",
            "value": "${{ jobs.refresh.outputs.auth_token }}"
          }
        },
        "secrets": {
          "SITE_URL": {
            "required": true
          },
          "SERVICE_ACCOUNT_SECRET": {
            "required": false
          },
          "REFRESH_TOKEN": {
            "required": false
          },
          "AUTH_TOKEN": {
            "required": false
          }
        }
      }
    },
    "jobs": {
      "refresh": {
        "runs-on": "ubuntu-latest",
        "outputs": {
          "auth_token": "${{ steps.refresh.outputs.auth_token }}"
        },
        "steps": [
          {
            "name": "Checkout repository",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "Install jq",
            "run": "sudo apt-get update\nsudo apt-get install -y jq\n"
          },
          {
            "name": "Refresh Authentication Token",
            "id": "refresh",
            "env": {
              "SITE_URL": "${{ secrets.SITE_URL }}",
              "SERVICE_ACCOUNT_SECRET": "${{ secrets.SERVICE_ACCOUNT_SECRET }}",
              "REFRESH_TOKEN": "${{ secrets.REFRESH_TOKEN }}",
              "AUTH_TOKEN": "${{ secrets.AUTH_TOKEN }}"
            },
            "run": "echo \"üîê Starting token refresh process...\"\n\n# Initialize variables\nACCESS_TOKEN=\"\"\nTOKEN_REFRESHED=false\n\n# Try service account first\nif [ -n \"$SERVICE_ACCOUNT_SECRET\" ]; then\n  echo \"Attempting service account authentication...\"\n  RESPONSE=$(curl -s -X POST \"$SITE_URL/api/auth/refresh\" \\\n    -H \"Content-Type: application/json\" \\\n    -d \"{\\\"serviceAccount\\\": \\\"$SERVICE_ACCOUNT_SECRET\\\"}\")\n  \n  ACCESS_TOKEN=$(echo \"$RESPONSE\" | jq -r '.accessToken // empty')\n  \n  if [ -n \"$ACCESS_TOKEN\" ]; then\n    echo \"‚úÖ Service account token obtained\"\n    TOKEN_REFRESHED=true\n  else\n    echo \"‚ö†Ô∏è Service account authentication failed\"\n  fi\nfi\n\n# Try refresh token if service account failed\nif [ \"$TOKEN_REFRESHED\" = false ] && [ -n \"$REFRESH_TOKEN\" ]; then\n  echo \"Attempting refresh token authentication...\"\n  RESPONSE=$(curl -s -X POST \"$SITE_URL/api/auth/refresh\" \\\n    -H \"Content-Type: application/json\" \\\n    -d \"{\\\"refreshToken\\\": \\\"$REFRESH_TOKEN\\\"}\")\n  \n  ACCESS_TOKEN=$(echo \"$RESPONSE\" | jq -r '.accessToken // empty')\n  NEW_REFRESH_TOKEN=$(echo \"$RESPONSE\" | jq -r '.refreshToken // empty')\n  \n  if [ -n \"$ACCESS_TOKEN\" ]; then\n    echo \"‚úÖ Refresh token authentication successful\"\n    TOKEN_REFRESHED=true\n    \n    # Store new refresh token if provided\n    if [ -n \"$NEW_REFRESH_TOKEN\" ]; then\n      echo \"üìù New refresh token received (update GitHub secret manually)\"\n    fi\n  else\n    echo \"‚ö†Ô∏è Refresh token authentication failed\"\n  fi\nfi\n\n# Validate existing token as last resort\nif [ \"$TOKEN_REFRESHED\" = false ] && [ -n \"$AUTH_TOKEN\" ]; then\n  echo \"Testing existing AUTH_TOKEN...\"\n  HTTP_CODE=$(curl -s -o /dev/null -w \"%{http_code}\" \\\n    -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n    \"$SITE_URL/api/admin/queue/status\")\n  \n  if [ \"$HTTP_CODE\" = \"200\" ]; then\n    echo \"‚úÖ Existing token is still valid\"\n    ACCESS_TOKEN=\"$AUTH_TOKEN\"\n    TOKEN_REFRESHED=true\n  else\n    echo \"‚ö†Ô∏è Existing token is invalid (HTTP $HTTP_CODE)\"\n  fi\nfi\n\n# Output final status\nif [ \"$TOKEN_REFRESHED\" = true ]; then\n  echo \"‚úÖ Authentication successful\"\n  echo \"::add-mask::$ACCESS_TOKEN\"\n  echo \"auth_token=$ACCESS_TOKEN\" >> $GITHUB_OUTPUT\nelse\n  echo \"‚ùå All authentication methods failed\"\n  echo \"::error::Token refresh failed - manual intervention required\"\n  exit 1\nfi\n"
          },
          {
            "name": "Verify Token",
            "if": "success()",
            "env": {
              "SITE_URL": "${{ secrets.SITE_URL }}",
              "AUTH_TOKEN": "${{ steps.refresh.outputs.auth_token }}"
            },
            "run": "echo \"üîç Verifying refreshed token...\"\nHTTP_CODE=$(curl -s -o /dev/null -w \"%{http_code}\" \\\n  -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n  \"$SITE_URL/api/admin/queue/status\")\n\nif [ \"$HTTP_CODE\" = \"200\" ]; then\n  echo \"‚úÖ Token verification successful\"\nelse\n  echo \"‚ùå Token verification failed (HTTP $HTTP_CODE)\"\n  exit 1\nfi\n"
          }
        ]
      }
    },
    "intent": "General workflow (purpose unclear from name)",
    "category": "maintenance"
  },
  {
    "filename": "weekly-smoke-test.yml",
    "name": "üî• Weekly Smoke Test",
    "on": {
      "schedule": [
        {
          "cron": "0 8 * * 1"
        }
      ],
      "workflow_dispatch": {
        "inputs": {
          "target_environment": {
            "description": "Target environment to test",
            "required": false,
            "type": "choice",
            "options": [
              "production",
              "development"
            ],
            "default": "production"
          },
          "include_load_tests": {
            "description": "Include load testing",
            "required": false,
            "type": "boolean",
            "default": false
          }
        }
      }
    },
    "env": {
      "PRODUCTION_URL": "https://hotdog-diaries.vercel.app",
      "DEVELOPMENT_URL": "http://localhost:3000"
    },
    "jobs": {
      "weekly-smoke-test": {
        "name": "üè• Production Health Check",
        "runs-on": "ubuntu-latest",
        "timeout-minutes": 15,
        "permissions": {
          "contents": "read",
          "actions": "write",
          "issues": "write"
        },
        "steps": [
          {
            "name": "üì• Checkout code",
            "uses": "actions/checkout@v4"
          },
          {
            "name": "üìã Setup Node.js",
            "uses": "actions/setup-node@v4",
            "with": {
              "node-version": "18",
              "cache": "npm"
            }
          },
          {
            "name": "üì¶ Install dependencies",
            "run": "npm ci\n\n# Install additional system dependencies for smoke tests\nsudo apt-get update\nsudo apt-get install -y bc jq curl\n"
          },
          {
            "name": "üîß Configure test environment",
            "run": "# Determine target URL\nif [ \"${{ github.event.inputs.target_environment }}\" = \"development\" ]; then\n  echo \"BASE_URL_OVERRIDE=${{ env.DEVELOPMENT_URL }}\" >> $GITHUB_ENV\n  echo \"üß™ Testing development environment\"\nelse\n  echo \"BASE_URL_OVERRIDE=${{ env.PRODUCTION_URL }}\" >> $GITHUB_ENV\n  echo \"üåê Testing production environment\"\nfi\n\n# Configure load testing\nif [ \"${{ github.event.inputs.include_load_tests }}\" = \"true\" ]; then\n  echo \"LOAD_TEST_FLAG=--load-test\" >> $GITHUB_ENV\n  echo \"üöÄ Load testing enabled\"\nelse\n  echo \"LOAD_TEST_FLAG=\" >> $GITHUB_ENV\n  echo \"üìä Standard testing only\"\nfi\n"
          },
          {
            "name": "üî• Run smoke tests",
            "id": "smoke_tests",
            "run": "echo \"üî• Starting comprehensive smoke tests...\"\necho \"Target: $BASE_URL_OVERRIDE\"\necho \"\"\n\n# Make smoke test script executable\nchmod +x scripts/smoke.sh\n\n# Create results directory\nmkdir -p test-results\n\n# Run smoke tests with JSON output for parsing\nset +e  # Don't exit on failure, we want to capture results\n\n# Run public endpoint tests (no auth required)\necho \"üìä Testing public endpoints...\"\nif scripts/smoke.sh --json $LOAD_TEST_FLAG > test-results/smoke-results.json; then\n  SMOKE_EXIT_CODE=0\n  echo \"‚úÖ Public smoke tests passed\"\nelse\n  SMOKE_EXIT_CODE=$?\n  echo \"‚ùå Public smoke tests failed (exit code: $SMOKE_EXIT_CODE)\"\nfi\n\n# Also run with human-readable output for logs\necho \"\"\necho \"üìã Detailed test results:\"\nscripts/smoke.sh $LOAD_TEST_FLAG || true\n\n# Save exit code for later steps\necho \"SMOKE_EXIT_CODE=$SMOKE_EXIT_CODE\" >> $GITHUB_ENV\n\n# Parse results if JSON exists\nif [ -f \"test-results/smoke-results.json\" ]; then\n  echo \"üìä Parsing test results...\"\n  \n  TOTAL_TESTS=$(jq -r '.summary.total // 0' test-results/smoke-results.json)\n  PASSED_TESTS=$(jq -r '.summary.passed // 0' test-results/smoke-results.json)\n  FAILED_TESTS=$(jq -r '.summary.failed // 0' test-results/smoke-results.json)\n  SUCCESS_RATE=$(jq -r '.summary.success_rate // 0' test-results/smoke-results.json)\n  \n  echo \"TOTAL_TESTS=$TOTAL_TESTS\" >> $GITHUB_ENV\n  echo \"PASSED_TESTS=$PASSED_TESTS\" >> $GITHUB_ENV\n  echo \"FAILED_TESTS=$FAILED_TESTS\" >> $GITHUB_ENV\n  echo \"SUCCESS_RATE=$SUCCESS_RATE\" >> $GITHUB_ENV\n  \n  echo \"üìà Test Summary:\"\n  echo \"  Total: $TOTAL_TESTS\"\n  echo \"  Passed: $PASSED_TESTS\"\n  echo \"  Failed: $FAILED_TESTS\"\n  echo \"  Success Rate: $SUCCESS_RATE%\"\nfi\n"
          },
          {
            "name": "üè• Quick admin health check",
            "id": "admin_health",
            "if": "always()",
            "run": "echo \"üè• Running quick admin health check...\"\n\n# Make poke-admin script executable\nchmod +x scripts/poke-admin.sh\n\n# Run admin health check (will skip auth tests without token)\nset +e\nif scripts/poke-admin.sh > test-results/admin-health.log 2>&1; then\n  ADMIN_HEALTH_CODE=0\n  echo \"‚úÖ Admin health check passed\"\nelse\n  ADMIN_HEALTH_CODE=$?\n  echo \"‚ùå Admin health check failed (exit code: $ADMIN_HEALTH_CODE)\"\nfi\n\necho \"ADMIN_HEALTH_CODE=$ADMIN_HEALTH_CODE\" >> $GITHUB_ENV\n\necho \"üìã Admin health results:\"\ncat test-results/admin-health.log\n"
          },
          {
            "name": "üìä Generate test report",
            "if": "always()",
            "run": "echo \"üìä Generating comprehensive test report...\"\n\n# Create report file\nREPORT_FILE=\"test-results/weekly-smoke-report.md\"\n\ncat > \"$REPORT_FILE\" << EOF\n# üî• Weekly Smoke Test Report\n\n**Date:** $(date -u -Iseconds)  \n**Environment:** $BASE_URL_OVERRIDE  \n**Triggered by:** ${{ github.event_name }}  \n**Workflow Run:** [\\#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})  \n\n## üìà Test Results Summary\n\n| Metric | Value | Status |\n|--------|-------|--------|\n| **Total Tests** | ${TOTAL_TESTS:-N/A} | - |\n| **Passed Tests** | ${PASSED_TESTS:-N/A} | $([ \"${PASSED_TESTS:-0}\" -eq \"${TOTAL_TESTS:-1}\" ] && echo \"‚úÖ\" || echo \"‚ùå\") |\n| **Failed Tests** | ${FAILED_TESTS:-N/A} | $([ \"${FAILED_TESTS:-1}\" -eq \"0\" ] && echo \"‚úÖ\" || echo \"‚ùå\") |\n| **Success Rate** | ${SUCCESS_RATE:-N/A}% | $([ \"${SUCCESS_RATE:-0}\" = \"100\" ] && echo \"‚úÖ\" || echo \"‚ùå\") |\n| **Admin Health** | Exit Code ${ADMIN_HEALTH_CODE:-N/A} | $([ \"${ADMIN_HEALTH_CODE:-1}\" -eq \"0\" ] && echo \"‚úÖ\" || echo \"‚ùå\") |\n\n## üéØ Overall Status\n\nEOF\n\n# Determine overall status\nif [ \"${SMOKE_EXIT_CODE:-1}\" -eq \"0\" ] && [ \"${ADMIN_HEALTH_CODE:-1}\" -eq \"0\" ]; then\n  echo \"**üéâ ALL TESTS PASSED** - System is healthy\" >> \"$REPORT_FILE\"\n  echo \"OVERALL_STATUS=success\" >> $GITHUB_ENV\nelse\n  echo \"**üö® TESTS FAILED** - Issues detected that require attention\" >> \"$REPORT_FILE\"\n  echo \"OVERALL_STATUS=failure\" >> $GITHUB_ENV\nfi\n\ncat >> \"$REPORT_FILE\" << EOF\n\n## üìã Detailed Results\n\n### üî• Smoke Test Details\n\\`\\`\\`json\n$(cat test-results/smoke-results.json 2>/dev/null || echo '{\"error\": \"No smoke test results available\"}')\n\\`\\`\\`\n\n### üè• Admin Health Check\n\\`\\`\\`\n$(cat test-results/admin-health.log 2>/dev/null || echo \"No admin health results available\")\n\\`\\`\\`\n\n## üîó Quick Links\n\n- [Production Site](${{ env.PRODUCTION_URL }})\n- [Admin Dashboard](${{ env.PRODUCTION_URL }}/admin)\n- [API Documentation](${{ env.PRODUCTION_URL }}/admin/docs)\n- [System Metrics](${{ env.PRODUCTION_URL }}/api/system/metrics)\n\n---\n\nGenerated by [Weekly Smoke Test Workflow](${{ github.server_url }}/${{ github.repository }}/actions/workflows/weekly-smoke-test.yml)\nEOF\n\necho \"‚úÖ Test report generated\"\n"
          },
          {
            "name": "üì§ Upload test results",
            "if": "always()",
            "uses": "actions/upload-artifact@v4",
            "with": {
              "name": "weekly-smoke-test-${{ github.run_number }}",
              "path": "test-results/\n",
              "retention-days": 30,
              "compression-level": 6
            }
          },
          {
            "name": "üìù Add to job summary",
            "if": "always()",
            "run": "echo \"## üî• Weekly Smoke Test Summary\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Add report content to job summary\ncat test-results/weekly-smoke-report.md >> $GITHUB_STEP_SUMMARY\n"
          },
          {
            "name": "üö® Create issue on failure",
            "if": "failure() && github.event_name == 'schedule'",
            "uses": "actions/github-script@v7",
            "with": {
              "script": "const issueTitle = `üö® Weekly Smoke Test Failed - ${new Date().toISOString().split('T')[0]}`;\nconst issueBody = `# Weekly Smoke Test Failure Report\n\nThe weekly smoke test failed on ${new Date().toISOString()}.\n\n**Workflow Run:** [#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})\n**Environment:** ${{ env.BASE_URL_OVERRIDE }}\n\n## Failure Details\n\n- **Smoke Tests Exit Code:** ${{ env.SMOKE_EXIT_CODE }}\n- **Admin Health Exit Code:** ${{ env.ADMIN_HEALTH_CODE }}\n- **Overall Status:** ${{ env.OVERALL_STATUS }}\n\n## Next Steps\n\n1. üîç Review the [workflow logs](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})\n2. üè• Check the [admin dashboard](${{ env.PRODUCTION_URL }}/admin)\n3. üìä Review [system metrics](${{ env.PRODUCTION_URL }}/api/system/metrics)\n4. üìã Consult the [SRE runbook](./docs/runbook.md) for troubleshooting\n\n## Auto-Generated Actions\n\n- [ ] Investigate root cause\n- [ ] Apply fixes if needed\n- [ ] Re-run smoke tests to verify\n- [ ] Close this issue when resolved\n\n---\n\nThis issue was automatically created by the Weekly Smoke Test workflow.\n`;\n\nawait github.rest.issues.create({\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  title: issueTitle,\n  body: issueBody,\n  labels: ['bug', 'monitoring', 'auto-created']\n});\n"
            }
          },
          {
            "name": "‚úÖ Success notification",
            "if": "success()",
            "run": "echo \"üéâ Weekly smoke test completed successfully!\"\necho \"\"\necho \"üìä Results summary:\"\necho \"  - Total tests: ${TOTAL_TESTS:-N/A}\"\necho \"  - Passed: ${PASSED_TESTS:-N/A}\"\necho \"  - Failed: ${FAILED_TESTS:-N/A}\"\necho \"  - Success rate: ${SUCCESS_RATE:-N/A}%\"\necho \"  - Admin health: $([ \"${ADMIN_HEALTH_CODE:-1}\" -eq \"0\" ] && echo \"‚úÖ Healthy\" || echo \"‚ùå Issues\")\"\necho \"\"\necho \"üåê System is healthy and operating normally.\"\n"
          }
        ]
      }
    },
    "intent": "Runs automated tests and builds",
    "category": "test"
  }
]