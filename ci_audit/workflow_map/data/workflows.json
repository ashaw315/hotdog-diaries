{
  "scan_timestamp": "2025-10-21T23:49:10.053Z",
  "workflows": [
    {
      "path": ".github/workflows/_posting-guard.yml",
      "filename": "_posting-guard.yml",
      "name": "Posting Guard (Reusable)",
      "triggers": [
        {
          "event": "workflow_call",
          "config": {
            "inputs": {
              "tz": {
                "required": false,
                "type": "string",
                "default": "America/New_York"
              },
              "min_slots": {
                "required": false,
                "type": "number",
                "default": 6
              },
              "date": {
                "required": false,
                "type": "string",
                "default": "today"
              }
            }
          }
        }
      ],
      "jobs": {
        "guard": {
          "permissions": {
            "contents": "read"
          },
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 5,
          "concurrency": {
            "group": "posting-guard-${{ github.ref }}",
            "cancel-in-progress": true
          },
          "steps": [
            {
              "uses": "actions/checkout@v4"
            },
            {
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "20",
                "cache": "pnpm"
              }
            },
            {
              "run": "pnpm install --frozen-lockfile || pnpm install"
            },
            {
              "name": "Verify schedule ready",
              "run": "pnpm tsx scripts/ops/assert-schedule-ready.ts --tz \"${{ inputs.tz }}\" --date \"${{ inputs.date }}\" --min \"${{ inputs.min_slots }}\""
            }
          ]
        }
      },
      "secrets_refs": [],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/auto-approve.yml",
      "filename": "auto-approve.yml",
      "name": "Auto-Approve Content",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 */6 * * *"
            }
          ],
          "cron": [
            "0 */6 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": {
            "inputs": {
              "force_approval": {
                "description": "Force approval of more content",
                "required": false,
                "default": "false",
                "type": "boolean"
              },
              "max_items": {
                "description": "Maximum items to approve",
                "required": false,
                "default": "200",
                "type": "string"
              },
              "min_confidence": {
                "description": "Minimum confidence score",
                "required": false,
                "default": "0.4",
                "type": "string"
              }
            }
          }
        }
      ],
      "jobs": {
        "auto-approve": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Check Auto-Approval Status",
              "id": "check-status",
              "run": "echo \"üîç Checking auto-approval candidates...\"\n\nresponse=$(curl -L -s -w \"%{http_code}\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  \"${{ secrets.SITE_URL }}/api/admin/auto-approve\")\n\nhttp_code=\"${response: -3}\"\nbody=\"${response%???}\"\n\nif [ \"$http_code\" = \"200\" ]; then\n  echo \"‚úÖ Status check successful\"\n  echo \"$body\" | jq -r '.estimation.recommendedAction // \"No action needed\"'\n  \n  # Extract candidate count for decision making\n  candidates=$(echo \"$body\" | jq -r '.totalCandidates // 0')\n  echo \"candidates=$candidates\" >> $GITHUB_OUTPUT\n  \n  echo \"üìä Found $candidates approval candidates\"\nelse\n  echo \"‚ùå Status check failed with code $http_code\"\n  echo \"$body\"\n  exit 1\nfi\n"
            },
            {
              "name": "Run Auto-Approval",
              "if": "steps.check-status.outputs.candidates > 20 || github.event_name == 'workflow_dispatch'",
              "run": "echo \"ü§ñ Running auto-approval process...\"\n\n# Set parameters from workflow inputs or defaults\nFORCE_APPROVAL=\"${{ github.event.inputs.force_approval || 'false' }}\"\nMAX_ITEMS=\"${{ github.event.inputs.max_items || '200' }}\"\nMIN_CONFIDENCE=\"${{ github.event.inputs.min_confidence || '0.4' }}\"\n\necho \"‚öôÔ∏è Parameters: force=$FORCE_APPROVAL, max=$MAX_ITEMS, min_confidence=$MIN_CONFIDENCE\"\n\nresponse=$(curl -L -s -w \"%{http_code}\" \\\n  -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -d \"{\n    \\\"forceApproval\\\": $FORCE_APPROVAL,\n    \\\"maxItems\\\": $MAX_ITEMS,\n    \\\"minConfidenceScore\\\": $MIN_CONFIDENCE\n  }\" \\\n  \"${{ secrets.SITE_URL }}/api/admin/auto-approve\")\n\nhttp_code=\"${response: -3}\"\nbody=\"${response%???}\"\n\nif [ \"$http_code\" = \"200\" ]; then\n  echo \"‚úÖ Auto-approval completed successfully\"\n  echo \"$body\" | jq -r '.message'\n  \n  # Extract results for reporting\n  total_approved=$(echo \"$body\" | jq -r '.approvalResults.total // 0')\n  days_of_content=$(echo \"$body\" | jq -r '.updatedStats.daysOfContent // 0')\n  \n  echo \"üìä Results:\"\n  echo \"  ‚Ä¢ Total approved: $total_approved items\"\n  echo \"  ‚Ä¢ Days of content remaining: $days_of_content days\"\n  echo \"  ‚Ä¢ Breakdown: $(echo \"$body\" | jq -c '.approvalResults')\"\n  \n  # Check if we need alerts\n  if [ \"$days_of_content\" -lt 7 ]; then\n    echo \"‚ö†Ô∏è WARNING: Only $days_of_content days of content remaining!\"\n    echo \"::warning title=Low Content Buffer::Only $days_of_content days of content remaining. Consider running emergency scan.\"\n  fi\nelse\n  echo \"‚ùå Auto-approval failed with code $http_code\"\n  echo \"$body\"\n  echo \"::error title=Auto-Approval Failed::Auto-approval process failed with HTTP $http_code\"\n  exit 1\nfi\n"
            },
            {
              "name": "Skip Auto-Approval",
              "if": "steps.check-status.outputs.candidates <= 20 && github.event_name != 'workflow_dispatch'",
              "run": "echo \"‚úÖ Auto-approval not needed\"\necho \"üìä Only ${{ steps.check-status.outputs.candidates }} candidates found (threshold: 20)\"\necho \"üéØ System is healthy - maintaining current approval levels\"\n"
            },
            {
              "name": "Verify Content Pipeline Health",
              "run": "echo \"üè• Checking overall content pipeline health...\"\n\nresponse=$(curl -L -s -w \"%{http_code}\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  \"${{ secrets.SITE_URL }}/api/admin/automation-health\")\n\nhttp_code=\"${response: -3}\"\nbody=\"${response%???}\"\n\nif [ \"$http_code\" = \"200\" ]; then\n  overall_health=$(echo \"$body\" | jq -r '.overallHealth // \"unknown\"')\n  days_remaining=$(echo \"$body\" | jq -r '.contentSummary.daysRemaining // 0')\n  \n  echo \"üìä Overall Health: $overall_health\"\n  echo \"üìÖ Days Remaining: $days_remaining\"\n  \n  # Set workflow status based on health\n  case \"$overall_health\" in\n    \"critical\")\n      echo \"::error title=Critical Content Health::Content pipeline is in critical state\"\n      ;;\n    \"warning\") \n      echo \"::warning title=Content Health Warning::Content pipeline needs attention\"\n      ;;\n    \"healthy\")\n      echo \"‚úÖ Content pipeline is healthy\"\n      ;;\n  esac\n  \n  # Output recommendations\n  recommendations=$(echo \"$body\" | jq -r '.recommendations[]?' 2>/dev/null || echo \"No specific recommendations\")\n  if [ \"$recommendations\" != \"No specific recommendations\" ]; then\n    echo \"üí° Recommendations:\"\n    echo \"$body\" | jq -r '.recommendations[]' | sed 's/^/  ‚Ä¢ /'\n  fi\nelse\n  echo \"‚ö†Ô∏è Could not verify pipeline health (HTTP $http_code)\"\nfi\n"
            },
            {
              "name": "Report Summary",
              "run": "echo \"üìã Auto-Approval Workflow Summary\"\necho \"=================================\"\necho \"‚è∞ Triggered: $(date -u)\"\necho \"üéØ Trigger: ${{ github.event_name }}\"\necho \"üèóÔ∏è Environment: $([ '${{ github.event_name }}' = 'workflow_dispatch' ] && echo 'Manual' || echo 'Scheduled')\"\necho \"üìä Candidates checked: ${{ steps.check-status.outputs.candidates || 'N/A' }}\"\necho \"‚úÖ Workflow completed successfully\"\n"
            }
          ]
        }
      },
      "secrets_refs": [
        "AUTH_TOKEN",
        "SITE_URL"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/auto-pr-ci-shepherd.yml",
      "filename": "auto-pr-ci-shepherd.yml",
      "name": "Auto PR CI Shepherd",
      "triggers": [
        {
          "event": "push",
          "config": {
            "branches": [
              "main"
            ]
          }
        },
        {
          "event": "workflow_dispatch",
          "config": {
            "inputs": {
              "pr_mode": {
                "description": "Use PR mode instead of direct push",
                "required": false,
                "type": "boolean",
                "default": false
              }
            }
          }
        },
        {
          "event": "pull_request",
          "config": {
            "branches": [
              "main"
            ],
            "types": [
              "opened",
              "synchronize",
              "reopened"
            ]
          }
        }
      ],
      "jobs": {
        "safety-check": {
          "runs-on": "ubuntu-latest",
          "name": "Fork & Permission Safety Check",
          "outputs": {
            "is_fork": "${{ steps.safety.outputs.is_fork }}",
            "is_trusted": "${{ steps.safety.outputs.is_trusted }}",
            "safe_to_proceed": "${{ steps.safety.outputs.safe_to_proceed }}",
            "execution_mode": "${{ steps.safety.outputs.execution_mode }}"
          },
          "steps": [
            {
              "name": "Analyze fork and permission safety",
              "id": "safety",
              "run": "echo \"üîç Analyzing repository and permission safety...\"\n\n# Detect if this is a fork\nIS_FORK=\"false\"\nif [ \"${{ github.event.pull_request.head.repo.full_name }}\" != \"${{ github.repository }}\" ]; then\n  IS_FORK=\"true\"\nfi\n\n# Determine if actor is trusted (repository member or collaborator)\nIS_TRUSTED=\"false\"\nif [ \"${{ github.actor }}\" = \"${{ github.repository_owner }}\" ] || [ \"${{ github.event_name }}\" = \"push\" ]; then\n  IS_TRUSTED=\"true\"\nfi\n\n# Additional trust checks for known bot actors\ncase \"${{ github.actor }}\" in\n  \"dependabot[bot]\"|\"github-actions[bot]\"|\"renovate[bot]\")\n    IS_TRUSTED=\"true\"\n    ;;\nesac\n\n# Determine execution mode based on safety analysis\nEXECUTION_MODE=\"restricted\"\nSAFE_TO_PROCEED=\"false\"\n\nif [ \"$IS_FORK\" = \"false\" ] && [ \"$IS_TRUSTED\" = \"true\" ]; then\n  EXECUTION_MODE=\"full\"\n  SAFE_TO_PROCEED=\"true\"\nelif [ \"$IS_FORK\" = \"true\" ] && [ \"$IS_TRUSTED\" = \"false\" ]; then\n  EXECUTION_MODE=\"fork-restricted\"\n  SAFE_TO_PROCEED=\"false\"\nelif [ \"$IS_TRUSTED\" = \"true\" ]; then\n  EXECUTION_MODE=\"trusted-limited\"\n  SAFE_TO_PROCEED=\"true\"\nfi\n\necho \"üìä Safety Analysis Results:\"\necho \"  - Is Fork: $IS_FORK\"\necho \"  - Is Trusted: $IS_TRUSTED\"\necho \"  - Actor: ${{ github.actor }}\"\necho \"  - Event: ${{ github.event_name }}\"\necho \"  - Execution Mode: $EXECUTION_MODE\"\necho \"  - Safe to Proceed: $SAFE_TO_PROCEED\"\n\n# Set outputs\necho \"is_fork=$IS_FORK\" >> $GITHUB_OUTPUT\necho \"is_trusted=$IS_TRUSTED\" >> $GITHUB_OUTPUT\necho \"safe_to_proceed=$SAFE_TO_PROCEED\" >> $GITHUB_OUTPUT\necho \"execution_mode=$EXECUTION_MODE\" >> $GITHUB_OUTPUT\n\n# Write safety summary\ncat >> $GITHUB_STEP_SUMMARY << 'EOF'\n## üîí Fork & Permission Safety Analysis\n\n| Field | Value |\n|-------|-------|\n| **Is Fork** | $IS_FORK |\n| **Is Trusted Actor** | $IS_TRUSTED |\n| **Actor** | `${{ github.actor }}` |\n| **Event Type** | `${{ github.event_name }}` |\n| **Execution Mode** | `$EXECUTION_MODE` |\n| **Safe to Proceed** | $SAFE_TO_PROCEED |\n\n### üõ°Ô∏è Security Policy\n\n- **Full Mode**: Repository members, push events - all shepherd functions available\n- **Trusted Limited**: Known bots - limited shepherd functions, no auto-merge\n- **Fork Restricted**: External forks - CI checks only, no repository modifications\n\nEOF\n"
            },
            {
              "name": "Block unsafe fork execution",
              "if": "steps.safety.outputs.execution_mode == 'fork-restricted'",
              "run": "echo \"üö® FORK EXECUTION BLOCKED\"\necho \"=========================\"\necho \"This workflow cannot execute shepherd operations from external forks for security reasons.\"\necho \"\"\necho \"For external contributors:\"\necho \"1. Standard CI checks will run instead\"\necho \"2. Repository maintainers can manually trigger shepherd operations\"\necho \"3. Consider contributing through repository collaboration\"\necho \"\"\necho \"This is a security measure to prevent unauthorized repository modifications.\"\nexit 0\n"
            }
          ]
        },
        "shepherd": {
          "needs": "safety-check",
          "if": "needs.safety-check.outputs.safe_to_proceed == 'true'",
          "permissions": {
            "contents": "write",
            "pull-requests": "write",
            "checks": "write",
            "issues": "write",
            "actions": "write"
          },
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 30,
          "concurrency": {
            "group": "ci-shepherd-${{ github.ref }}",
            "cancel-in-progress": true
          },
          "env": {
            "GH_TOKEN": "${{ secrets.GITHUB_TOKEN }}",
            "EXECUTION_MODE": "${{ needs.safety-check.outputs.execution_mode }}"
          },
          "steps": [
            {
              "uses": "actions/checkout@v4",
              "with": {
                "token": "${{ secrets.GITHUB_TOKEN }}"
              }
            },
            {
              "uses": "pnpm/action-setup@v4"
            },
            {
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "20",
                "cache": "pnpm"
              }
            },
            {
              "run": "pnpm install --frozen-lockfile || pnpm install"
            },
            {
              "name": "Security: Validate execution mode",
              "run": "echo \"üîí Execution Mode: $EXECUTION_MODE\"\ncase \"$EXECUTION_MODE\" in\n  \"full\")\n    echo \"‚úÖ Full shepherd operations authorized\"\n    ;;\n  \"trusted-limited\")\n    echo \"‚ö†Ô∏è Limited shepherd operations (trusted actor)\"\n    echo \"Note: Auto-merge and auto-revert operations may be restricted\"\n    ;;\n  \"fork-restricted\")\n    echo \"üö® This should not execute - fork safety failed\"\n    exit 1\n    ;;\n  *)\n    echo \"‚ùå Unknown execution mode: $EXECUTION_MODE\"\n    exit 1\n    ;;\nesac\n"
            },
            {
              "name": "Direct: Preflight watchdog check",
              "if": "${{ inputs.pr_mode != true && env.EXECUTION_MODE != 'fork-restricted' }}",
              "run": "echo \"üîç Running preflight production watchdog...\"\n\n# Check if this is a fork PR and skip workflow dispatch\nif [ \"${{ github.event_name }}\" = \"pull_request\" ] && [ \"${{ github.event.pull_request.head.repo.full_name }}\" != \"${{ github.repository }}\" ]; then\n  echo \"‚ö†Ô∏è Fork PR detected - skipping workflow dispatch for security\"\n  echo \"‚úÖ Fork validation completed successfully\"\n  exit 0\nfi\n\n# Check if this is ANY PR context and skip watchdog (secrets not available)\nif [ \"${{ github.event_name }}\" = \"pull_request\" ]; then\n  echo \"‚ö†Ô∏è PR context detected - prod-watchdog requires secrets not available in PR contexts\"\n  echo \"üîç Production watchdog checks will run on merge to main branch\"\n  echo \"‚úÖ PR validation completed - watchdog skipped for security\"\n  exit 0\nfi\n\n# Handle permission errors gracefully for non-PR contexts\nif ! pnpm tsx scripts/ops/ci-shepherd/run-and-wait-workflow.ts \\\n  --workflow .github/workflows/prod-watchdog.yml \\\n  --timeoutSec 900; then\n  EXIT_CODE=$?\n  if [ $EXIT_CODE -eq 78 ]; then\n    echo \"‚è∏Ô∏è Watchdog concluded neutrally - this is expected\"\n    exit 0\n  else\n    echo \"‚ùå Watchdog failed with exit code $EXIT_CODE\"\n    exit $EXIT_CODE\n  fi\nfi\n"
            },
            {
              "name": "Direct: Auto-remedy on preflight failure",
              "if": "${{ inputs.pr_mode != true && failure() && env.EXECUTION_MODE == 'full' }}",
              "run": "echo \"üîß Preflight watchdog failed, attempting auto-remedy...\"\npnpm tsx scripts/ops/ci-shepherd/auto-remedy.ts\n"
            },
            {
              "name": "Direct: Push to main",
              "if": "${{ inputs.pr_mode != true && success() && env.EXECUTION_MODE == 'full' }}",
              "run": "echo \"‚úÖ Preflight passed, pushing to main\"\n# Add actual push logic here\n"
            },
            {
              "name": "Direct: Post-push watchdog",
              "if": "${{ inputs.pr_mode != true && success() && env.EXECUTION_MODE != 'fork-restricted' }}",
              "run": "echo \"üîç Running post-push production watchdog...\"\n\n# Check if this is a fork PR and skip workflow dispatch\nif [ \"${{ github.event_name }}\" = \"pull_request\" ] && [ \"${{ github.event.pull_request.head.repo.full_name }}\" != \"${{ github.repository }}\" ]; then\n  echo \"‚ö†Ô∏è Fork PR detected - skipping workflow dispatch for security\"\n  echo \"‚úÖ Fork validation completed successfully\"\n  exit 0\nfi\n\n# Check if this is ANY PR context and skip watchdog (secrets not available)\nif [ \"${{ github.event_name }}\" = \"pull_request\" ]; then\n  echo \"‚ö†Ô∏è PR context detected - prod-watchdog requires secrets not available in PR contexts\"\n  echo \"üîç Production watchdog checks will run on merge to main branch\"\n  echo \"‚úÖ PR validation completed - watchdog skipped for security\"\n  exit 0\nfi\n\n# Handle permission errors gracefully for non-PR contexts\nif ! pnpm tsx scripts/ops/ci-shepherd/run-and-wait-workflow.ts \\\n  --workflow .github/workflows/prod-watchdog.yml \\\n  --timeoutSec 900; then\n  EXIT_CODE=$?\n  if [ $EXIT_CODE -eq 78 ]; then\n    echo \"‚è∏Ô∏è Watchdog concluded neutrally - this is expected\"\n    exit 0\n  else\n    echo \"‚ùå Watchdog failed with exit code $EXIT_CODE\"\n    exit $EXIT_CODE\n  fi\nfi\n"
            },
            {
              "name": "Direct: Auto-remedy on post-push failure",
              "if": "${{ inputs.pr_mode != true && failure() && env.EXECUTION_MODE == 'full' }}",
              "run": "echo \"üîß Post-push watchdog failed, attempting auto-remedy...\"\npnpm tsx scripts/ops/ci-shepherd/auto-remedy.ts\n"
            },
            {
              "name": "Direct: Auto-revert if still failing",
              "if": "${{ inputs.pr_mode != true && failure() && env.EXECUTION_MODE == 'full' }}",
              "run": "echo \"‚ùå Auto-remedy failed, triggering auto-revert\"\n# Add auto-revert logic here\n# pnpm tsx scripts/ops/ci-shepherd/auto-revert.ts --target main\n"
            },
            {
              "name": "Direct: Limited mode notification",
              "if": "${{ inputs.pr_mode != true && env.EXECUTION_MODE == 'trusted-limited' }}",
              "run": "echo \"‚ö†Ô∏è LIMITED EXECUTION MODE\"\necho \"=========================\"\necho \"Direct push operations are restricted for trusted-limited mode.\"\necho \"Only watchdog checks are permitted.\"\necho \"Repository modifications require full authorization.\"\n"
            },
            {
              "name": "PR: Create and monitor",
              "if": "${{ inputs.pr_mode == true && env.EXECUTION_MODE == 'full' }}",
              "run": "echo \"üìù Creating PR and setting up monitoring\"\n# Add PR creation logic here\n"
            },
            {
              "name": "PR: Pre-merge watchdog check",
              "if": "${{ inputs.pr_mode == true && success() && env.EXECUTION_MODE != 'fork-restricted' }}",
              "run": "echo \"üîç Running pre-merge production watchdog...\"\n\n# Check if this is ANY PR context and skip watchdog (secrets not available)\nif [ \"${{ github.event_name }}\" = \"pull_request\" ]; then\n  echo \"‚ö†Ô∏è PR context detected - prod-watchdog requires secrets not available in PR contexts\"\n  echo \"üîç Production watchdog checks will run on merge to main branch\"\n  echo \"‚úÖ PR validation completed - watchdog skipped for security\"\n  exit 0\nfi\n\n# Handle permission errors gracefully for non-PR contexts\nif ! pnpm tsx scripts/ops/ci-shepherd/run-and-wait-workflow.ts \\\n  --workflow .github/workflows/prod-watchdog.yml \\\n  --timeoutSec 900; then\n  EXIT_CODE=$?\n  if [ $EXIT_CODE -eq 78 ]; then\n    echo \"‚è∏Ô∏è Watchdog concluded neutrally - this is expected\"\n    exit 0\n  else\n    echo \"‚ùå Watchdog failed with exit code $EXIT_CODE\"\n    exit $EXIT_CODE\n  fi\nfi\n"
            },
            {
              "name": "PR: Auto-remedy on pre-merge failure",
              "if": "${{ inputs.pr_mode == true && failure() && env.EXECUTION_MODE == 'full' }}",
              "run": "echo \"üîß Pre-merge watchdog failed, attempting auto-remedy...\"\npnpm tsx scripts/ops/ci-shepherd/auto-remedy.ts\n"
            },
            {
              "name": "PR: Block merge if still failing",
              "if": "${{ inputs.pr_mode == true && failure() && env.EXECUTION_MODE != 'fork-restricted' }}",
              "run": "echo \"‚ùå Auto-remedy failed, blocking merge\"\n# Only comment on PRs from the same repository (not forks)\nif [ \"${{ github.event.pull_request.head.repo.full_name }}\" = \"${{ github.repository }}\" ]; then\n  gh pr comment ${{ github.event.number }} --body \"üö® **Pre-merge watchdog failure** - auto-remedy attempted but merge still blocked\"\n  gh pr edit ${{ github.event.number }} --add-label \"watchdog-red\"\nelse\n  echo \"Fork PR detected - skipping PR comment operations for security\"\nfi\nexit 1\n"
            },
            {
              "name": "PR: Proceed to merge",
              "if": "${{ inputs.pr_mode == true && success() && env.EXECUTION_MODE == 'full' }}",
              "run": "echo \"‚úÖ Pre-merge watchdog passed, proceeding to merge\"\n# Add merge logic here\n"
            },
            {
              "name": "PR: Post-merge watchdog",
              "if": "${{ inputs.pr_mode == true && success() && env.EXECUTION_MODE != 'fork-restricted' }}",
              "run": "echo \"üîç Running post-merge production watchdog...\"\n\n# Check if this is ANY PR context and skip watchdog (secrets not available)\nif [ \"${{ github.event_name }}\" = \"pull_request\" ]; then\n  echo \"‚ö†Ô∏è PR context detected - prod-watchdog requires secrets not available in PR contexts\"\n  echo \"üîç Production watchdog checks will run on merge to main branch\"\n  echo \"‚úÖ PR validation completed - watchdog skipped for security\"\n  exit 0\nfi\n\n# Handle permission errors gracefully for non-PR contexts\nif ! pnpm tsx scripts/ops/ci-shepherd/run-and-wait-workflow.ts \\\n  --workflow .github/workflows/prod-watchdog.yml \\\n  --timeoutSec 900; then\n  EXIT_CODE=$?\n  if [ $EXIT_CODE -eq 78 ]; then\n    echo \"‚è∏Ô∏è Watchdog concluded neutrally - this is expected\"\n    exit 0\n  else\n    echo \"‚ùå Watchdog failed with exit code $EXIT_CODE\"\n    exit $EXIT_CODE\n  fi\nfi\n"
            },
            {
              "name": "PR: Auto-remedy on post-merge failure",
              "if": "${{ inputs.pr_mode == true && failure() && env.EXECUTION_MODE == 'full' }}",
              "run": "echo \"üîß Post-merge watchdog failed, attempting auto-remedy...\"\npnpm tsx scripts/ops/ci-shepherd/auto-remedy.ts\n"
            },
            {
              "name": "PR: Auto-revert if still failing",
              "if": "${{ inputs.pr_mode == true && failure() && env.EXECUTION_MODE == 'full' }}",
              "run": "echo \"‚ùå Auto-remedy failed, triggering auto-revert\"\n# Add auto-revert logic here  \n# pnpm tsx scripts/ops/ci-shepherd/auto-revert.ts --target main\n"
            },
            {
              "name": "PR: Limited mode notification",
              "if": "${{ inputs.pr_mode == true && env.EXECUTION_MODE == 'trusted-limited' }}",
              "run": "echo \"‚ö†Ô∏è LIMITED EXECUTION MODE\"\necho \"=========================\"\necho \"PR operations are restricted for trusted-limited mode.\"\necho \"Only watchdog checks are permitted.\"\necho \"Merge and revert operations require full authorization.\"\n"
            }
          ]
        },
        "fork-ci": {
          "needs": "safety-check",
          "if": "needs.safety-check.outputs.execution_mode == 'fork-restricted'",
          "runs-on": "ubuntu-latest",
          "name": "Fork PR - Standard CI",
          "permissions": {
            "contents": "read",
            "checks": "write"
          },
          "steps": [
            {
              "uses": "actions/checkout@v4"
            },
            {
              "uses": "pnpm/action-setup@v4"
            },
            {
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "20",
                "cache": "pnpm"
              }
            },
            {
              "run": "pnpm install --frozen-lockfile || pnpm install"
            },
            {
              "name": "Fork CI: Standard checks",
              "run": "echo \"üîç Running standard CI checks for fork PR...\"\necho \"Repository: ${{ github.event.pull_request.head.repo.full_name }}\"\necho \"Branch: ${{ github.event.pull_request.head.ref }}\"\necho \"Actor: ${{ github.actor }}\"\necho \"\"\necho \"Available checks:\"\necho \"- Build verification\"\necho \"- Test execution\"\necho \"- Lint checks\"\necho \"- Type checking\"\necho \"\"\necho \"Note: Shepherd operations are disabled for security\"\necho \"Repository maintainers can manually trigger shepherd operations if needed\"\n"
            },
            {
              "name": "Fork CI: Build check",
              "run": "echo \"üî® Checking build...\"\n# Add build validation here\necho \"Build check completed\"\n"
            },
            {
              "name": "Fork CI: Test check",
              "run": "echo \"üß™ Running tests...\"\n# Add test execution here\necho \"Test check completed\"\n"
            }
          ]
        },
        "neutralize": {
          "needs": "safety-check",
          "if": "needs.safety-check.outputs.safe_to_proceed != 'true' && needs.safety-check.outputs.execution_mode != 'fork-restricted'",
          "runs-on": "ubuntu-latest",
          "name": "Neutralize CI Shepherd",
          "permissions": {
            "contents": "read",
            "actions": "read"
          },
          "steps": [
            {
              "name": "Checkout for neutralize action",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Neutralize with summary",
              "uses": "./.github/actions/neutralize",
              "with": {
                "reason": "CI Shepherd execution blocked due to security or permission restrictions",
                "deploy_state": "${{ needs.safety-check.outputs.execution_mode }}",
                "deploy_reason": "Actor: ${{ github.actor }}, Fork: ${{ needs.safety-check.outputs.is_fork }}, Trusted: ${{ needs.safety-check.outputs.is_trusted }}"
              }
            }
          ]
        }
      },
      "secrets_refs": [
        "GITHUB_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [
        "./.github/actions/neutralize"
      ],
      "reusable_workflows": [],
      "job_count": 4
    },
    {
      "path": ".github/workflows/auto-queue-manager.yml",
      "filename": "auto-queue-manager.yml",
      "name": "Auto Queue Manager",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 */6 * * *"
            }
          ],
          "cron": [
            "0 */6 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": {
            "inputs": {
              "mode": {
                "description": "Scan mode",
                "required": false,
                "default": "auto",
                "type": "choice",
                "options": [
                  "auto",
                  "emergency",
                  "status-only"
                ]
              },
              "force": {
                "description": "Force scan regardless of recommendations",
                "required": false,
                "default": false,
                "type": "boolean"
              }
            }
          }
        }
      ],
      "jobs": {
        "refresh-token": {
          "uses": "./.github/workflows/token-refresh.yml",
          "secrets": {
            "SITE_URL": "${{ secrets.SITE_URL }}",
            "SERVICE_ACCOUNT_SECRET": "${{ secrets.SERVICE_ACCOUNT_SECRET }}",
            "REFRESH_TOKEN": "${{ secrets.REFRESH_TOKEN }}",
            "AUTH_TOKEN": "${{ secrets.AUTH_TOKEN }}"
          }
        },
        "auto-queue-manager": {
          "needs": "refresh-token",
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Check Queue Health",
              "id": "queue-health",
              "env": {
                "AUTH_TOKEN": "${{ needs.refresh-token.outputs.auth_token }}"
              },
              "run": "echo \"ü©∫ Checking queue health...\"\n\n# Get current queue health status\nHEALTH_RESPONSE=$(curl -s -w \"HTTP_CODE:%{http_code}\" -L \\\n  \"${{ secrets.SITE_URL }}/api/admin/queue-health\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  --retry 2 --retry-delay 5 2>&1 || true)\n\n# Extract HTTP code and response body\nHTTP_CODE=$(echo \"$HEALTH_RESPONSE\" | grep -o 'HTTP_CODE:[0-9]*' | cut -d':' -f2)\nRESPONSE_BODY=$(echo \"$HEALTH_RESPONSE\" | sed 's/HTTP_CODE:[0-9]*$//')\n\necho \"Queue Health HTTP Status: $HTTP_CODE\"\n\nif [[ \"$HTTP_CODE\" == \"200\" ]]; then\n  echo \"‚úÖ Queue health check successful\"\n  \n  # Extract key metrics using jq\n  HEALTH_STATUS=$(echo \"$RESPONSE_BODY\" | jq -r '.health.status // \"unknown\"')\n  DAYS_CONTENT=$(echo \"$RESPONSE_BODY\" | jq -r '.queue.daysOfContent // 0')\n  APPROVED_CONTENT=$(echo \"$RESPONSE_BODY\" | jq -r '.queue.totalApproved // 0')\n  \n  echo \"üìä Queue Status: $HEALTH_STATUS\"\n  echo \"üìÖ Days of Content: $DAYS_CONTENT\"\n  echo \"‚úÖ Approved Content: $APPROVED_CONTENT\"\n  \n  # Set outputs for next step\n  echo \"health_status=$HEALTH_STATUS\" >> $GITHUB_OUTPUT\n  echo \"days_content=$DAYS_CONTENT\" >> $GITHUB_OUTPUT\n  echo \"approved_content=$APPROVED_CONTENT\" >> $GITHUB_OUTPUT\n  echo \"health_check_success=true\" >> $GITHUB_OUTPUT\nelse\n  echo \"‚ùå Queue health check failed with HTTP $HTTP_CODE\"\n  echo \"Response: $RESPONSE_BODY\"\n  echo \"health_check_success=false\" >> $GITHUB_OUTPUT\nfi\n"
            },
            {
              "name": "Determine Scan Mode",
              "id": "scan-mode",
              "run": "# Get input mode or default to auto\nINPUT_MODE=\"${{ github.event.inputs.mode || 'auto' }}\"\nFORCE_SCAN=\"${{ github.event.inputs.force || 'false' }}\"\n\n# Get health metrics from previous step\nHEALTH_STATUS=\"${{ steps.queue-health.outputs.health_status }}\"\nDAYS_CONTENT=\"${{ steps.queue-health.outputs.days_content }}\"\nAPPROVED_CONTENT=\"${{ steps.queue-health.outputs.approved_content }}\"\n\necho \"üéØ Determining scan mode...\"\necho \"Input mode: $INPUT_MODE\"\necho \"Health status: $HEALTH_STATUS\"\necho \"Days of content: $DAYS_CONTENT\"\necho \"Force scan: $FORCE_SCAN\"\n\n# Determine final scan mode based on health\nFINAL_MODE=\"$INPUT_MODE\"\n\nif [[ \"$HEALTH_STATUS\" == \"emergency\" ]] || [[ \"${APPROVED_CONTENT%.*}\" -eq 0 ]]; then\n  FINAL_MODE=\"emergency\"\n  echo \"üö® EMERGENCY: Queue is empty - forcing emergency mode\"\nelif [[ \"$HEALTH_STATUS\" == \"critical\" ]] && [[ \"$INPUT_MODE\" == \"auto\" ]]; then\n  FINAL_MODE=\"emergency\" \n  echo \"‚ö†Ô∏è CRITICAL: Queue is very low - upgrading to emergency mode\"\nelif [[ \"$FORCE_SCAN\" == \"true\" ]]; then\n  echo \"üîß Force scan enabled - proceeding with requested mode\"\nfi\n\necho \"final_mode=$FINAL_MODE\" >> $GITHUB_OUTPUT\necho \"should_scan=true\" >> $GITHUB_OUTPUT\necho \"üìã Final scan mode: $FINAL_MODE\"\n"
            },
            {
              "name": "Execute Auto-Scan",
              "if": "steps.scan-mode.outputs.should_scan == 'true'",
              "env": {
                "AUTH_TOKEN": "${{ needs.refresh-token.outputs.auth_token }}"
              },
              "run": "SCAN_MODE=\"${{ steps.scan-mode.outputs.final_mode }}\"\n\necho \"üöÄ Executing auto-scan with mode: $SCAN_MODE\"\n\n# Build request payload\nif [[ \"$SCAN_MODE\" == \"emergency\" ]]; then\n  PAYLOAD='{\"mode\": \"emergency\"}'\nelif [[ \"$SCAN_MODE\" == \"status-only\" ]]; then\n  PAYLOAD='{\"mode\": \"status-only\"}'\nelse\n  PAYLOAD='{\"mode\": \"auto\"}'\nfi\n\necho \"üì§ Request payload: $PAYLOAD\"\n\n# Execute auto-scan\nSCAN_RESPONSE=$(curl -s -w \"HTTP_CODE:%{http_code}\" -L -X POST \\\n  \"${{ secrets.SITE_URL }}/api/admin/auto-scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"$PAYLOAD\" \\\n  --retry 3 --retry-delay 10 2>&1 || true)\n\n# Extract results\nHTTP_CODE=$(echo \"$SCAN_RESPONSE\" | grep -o 'HTTP_CODE:[0-9]*' | cut -d':' -f2)\nRESPONSE_BODY=$(echo \"$SCAN_RESPONSE\" | sed 's/HTTP_CODE:[0-9]*$//')\n\necho \"Auto-Scan HTTP Status: $HTTP_CODE\"\n\nif [[ \"$HTTP_CODE\" == \"200\" ]]; then\n  echo \"‚úÖ Auto-scan completed successfully\"\n  \n  # Extract scan results\n  TRIGGERED=$(echo \"$RESPONSE_BODY\" | jq -r '.summary.totalTriggered // 0')\n  SKIPPED=$(echo \"$RESPONSE_BODY\" | jq -r '.summary.totalSkipped // 0') \n  ERRORS=$(echo \"$RESPONSE_BODY\" | jq -r '.summary.totalErrors // 0')\n  DAYS_AFTER=$(echo \"$RESPONSE_BODY\" | jq -r '.summary.queueDaysAfter // 0')\n  \n  echo \"üìä Scan Results:\"\n  echo \"  - Triggered: $TRIGGERED scans\"\n  echo \"  - Skipped: $SKIPPED scans\"\n  echo \"  - Errors: $ERRORS\"\n  echo \"  - Days of content after: $DAYS_AFTER\"\n  \n  # Show triggered scans\n  TRIGGERED_SCANS=$(echo \"$RESPONSE_BODY\" | jq -r '.triggeredScans[]? // empty')\n  if [[ -n \"$TRIGGERED_SCANS\" ]]; then\n    echo \"üéØ Triggered scans:\"\n    echo \"$TRIGGERED_SCANS\" | sed 's/^/  - /'\n  fi\n  \n  # Show any errors\n  SCAN_ERRORS=$(echo \"$RESPONSE_BODY\" | jq -r '.errors[]? // empty')\n  if [[ -n \"$SCAN_ERRORS\" ]]; then\n    echo \"‚ö†Ô∏è Scan errors:\"\n    echo \"$SCAN_ERRORS\" | sed 's/^/  - /'\n  fi\n  \nelse\n  echo \"‚ùå Auto-scan failed with HTTP $HTTP_CODE\"\n  echo \"Response: $RESPONSE_BODY\"\n  exit 1\nfi\n"
            },
            {
              "name": "Final Health Check",
              "if": "success()",
              "env": {
                "AUTH_TOKEN": "${{ needs.refresh-token.outputs.auth_token }}"
              },
              "run": "echo \"üîÑ Performing final health check...\"\n\n# Brief delay to allow scan results to process\nsleep 30\n\n# Get updated queue health\nFINAL_HEALTH=$(curl -s -L \\\n  \"${{ secrets.SITE_URL }}/api/admin/queue-health\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  --retry 2 2>/dev/null | jq -r '.health.status // \"unknown\"')\n\nFINAL_DAYS=$(curl -s -L \\\n  \"${{ secrets.SITE_URL }}/api/admin/queue-health\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  --retry 2 2>/dev/null | jq -r '.queue.daysOfContent // 0')\n\necho \"üìà Final queue status: $FINAL_HEALTH\"\necho \"üìÖ Final days of content: $FINAL_DAYS\"\n\nif [[ \"$FINAL_HEALTH\" == \"healthy\" ]] || [[ \"$FINAL_HEALTH\" == \"warning\" ]]; then\n  echo \"‚úÖ Queue is in acceptable condition\"\nelse\n  echo \"‚ö†Ô∏è Queue still needs attention: $FINAL_HEALTH\"\nfi\n"
            },
            {
              "name": "Handle Failure",
              "if": "failure()",
              "run": "echo \"‚ùå CRITICAL: Auto queue manager failed\"\necho \"\"\necho \"üîß TROUBLESHOOTING STEPS:\"\necho \"  1. Check AUTH_TOKEN in GitHub secrets\"\necho \"  2. Verify Vercel deployment status\"\necho \"  3. Check API endpoint availability\"\necho \"  4. Review platform API credentials\"\necho \"\"\necho \"üìû IMMEDIATE ACTIONS:\"\necho \"  - Manual content approval may be needed\"\necho \"  - Consider running emergency replenishment\"\necho \"  - Check individual platform scan workflows\"\n"
            }
          ]
        }
      },
      "secrets_refs": [
        "SITE_URL",
        "SERVICE_ACCOUNT_SECRET",
        "REFRESH_TOKEN",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [
        "./.github/workflows/token-refresh.yml"
      ],
      "job_count": 2
    },
    {
      "path": ".github/workflows/ci-failure-drilldown.yml",
      "filename": "ci-failure-drilldown.yml",
      "name": "CI Failure Drilldown (Read-Only)",
      "triggers": [
        {
          "event": "workflow_dispatch",
          "config": {
            "inputs": {
              "lookback_runs": {
                "description": "How many recent runs per workflow to analyze (<=10)",
                "required": false,
                "type": "number",
                "default": 10
              },
              "only_failed": {
                "description": "Only download logs for failed/neutral/skipped",
                "required": false,
                "type": "boolean",
                "default": true
              }
            }
          }
        }
      ],
      "jobs": {
        "drilldown": {
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 15,
          "steps": [
            {
              "uses": "actions/checkout@v4"
            },
            {
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "20"
              }
            },
            {
              "name": "Setup pnpm",
              "uses": "pnpm/action-setup@v4",
              "with": {
                "version": "10.18.3"
              }
            },
            {
              "run": "pnpm install --frozen-lockfile || pnpm install"
            },
            {
              "name": "Fetch runs & logs",
              "env": {
                "LOOKBACK": "${{ inputs.lookback_runs }}",
                "ONLY_FAILED": "${{ inputs.only_failed }}",
                "GH_TOKEN": "${{ secrets.GITHUB_TOKEN }}"
              },
              "run": "pnpm tsx scripts/ci/failure-drilldown/fetch-runs-and-logs.ts --lookback \"$LOOKBACK\" --onlyFailed \"$ONLY_FAILED\""
            },
            {
              "name": "Classify failures",
              "run": "pnpm tsx scripts/ci/failure-drilldown/classify-failures.ts"
            },
            {
              "name": "Assess usefulness",
              "run": "pnpm tsx scripts/ci/failure-drilldown/assess-usefulness.ts"
            },
            {
              "name": "Emit drilldown report",
              "run": "pnpm tsx scripts/ci/failure-drilldown/emit-drilldown-report.ts"
            },
            {
              "name": "Upload artifacts",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "CI_FAILURE_DRILLDOWN",
                "path": "ci_audit/failure_drilldown/",
                "retention-days": 14
              }
            },
            {
              "name": "Open/Update issue with report",
              "env": {
                "GH_TOKEN": "${{ secrets.GITHUB_TOKEN }}"
              },
              "run": "TITLE=\"CI Failure Drilldown (read-only)\"\nBODY=\"$(cat ci_audit/failure_drilldown/DRILLDOWN_REPORT.md)\"\nNUM=$(gh issue list --search \"$TITLE\" --state open --json number -q '.[0].number' || true)\nif [ -n \"$NUM\" ]; then\n  gh issue comment \"$NUM\" --body \"$BODY\"\nelse\n  gh issue create --title \"$TITLE\" --body \"$BODY\" --label \"ci-forensics\"\nfi\n"
            }
          ]
        }
      },
      "permissions": {
        "contents": "read",
        "actions": "read",
        "checks": "read",
        "issues": "write"
      },
      "secrets_refs": [
        "GITHUB_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/ci-new.yml",
      "filename": "ci-new.yml",
      "name": "CI Test",
      "triggers": [
        {
          "event": "push",
          "config": {
            "branches": [
              "main"
            ]
          }
        }
      ],
      "jobs": {
        "test": {
          "name": "Test Job",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 5,
          "steps": [
            {
              "name": "Checkout repository",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js",
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "20",
                "cache": "npm"
              }
            },
            {
              "name": "Cache node_modules",
              "id": "node-modules-cache",
              "uses": "actions/cache@v4",
              "with": {
                "path": "node_modules",
                "key": "node-modules-${{ runner.os }}-${{ hashFiles('package-lock.json') }}",
                "restore-keys": "node-modules-${{ runner.os }}-\n"
              }
            },
            {
              "name": "Install dependencies",
              "if": "steps.node-modules-cache.outputs.cache-hit != 'true'",
              "run": "npm ci"
            },
            {
              "name": "Simple test",
              "run": "echo \"‚úÖ CI workflow syntax is working!\""
            }
          ]
        }
      },
      "secrets_refs": [],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/ci-test.yml",
      "filename": "ci-test.yml",
      "name": "CI Test",
      "triggers": [
        {
          "event": "push",
          "config": {
            "branches": [
              "main"
            ]
          }
        }
      ],
      "jobs": {
        "test": {
          "name": "Test Job",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 5,
          "steps": [
            {
              "name": "Checkout repository",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js",
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "20",
                "cache": "npm"
              }
            },
            {
              "name": "Cache node_modules",
              "id": "node-modules-cache",
              "uses": "actions/cache@v4",
              "with": {
                "path": "node_modules",
                "key": "node-modules-${{ runner.os }}-${{ hashFiles('package-lock.json') }}",
                "restore-keys": "node-modules-${{ runner.os }}-\n"
              }
            },
            {
              "name": "Install dependencies",
              "if": "steps.node-modules-cache.outputs.cache-hit != 'true'",
              "run": "npm ci"
            },
            {
              "name": "Simple test",
              "run": "echo \"‚úÖ CI workflow syntax is working!\""
            }
          ]
        }
      },
      "secrets_refs": [],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/ci.yml",
      "filename": "ci.yml",
      "name": "CI",
      "triggers": [
        {
          "event": "push",
          "config": {
            "branches": [
              "main",
              "develop"
            ]
          }
        },
        {
          "event": "pull_request",
          "config": {
            "branches": [
              "main"
            ]
          }
        },
        {
          "event": "workflow_call",
          "config": {
            "inputs": {
              "node-version": {
                "description": "Node.js version to use",
                "type": "string",
                "default": "20"
              },
              "cache-strategy": {
                "description": "Cache strategy for dependencies",
                "type": "string",
                "default": "aggressive"
              }
            },
            "outputs": {
              "test-results": {
                "description": "Test results summary",
                "value": "${{ jobs.test.outputs.results }}"
              }
            }
          }
        }
      ],
      "jobs": {
        "lint": {
          "name": "Lint & Format Check",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 10,
          "if": "!contains(github.event.head_commit.message, '[skip ci]')",
          "outputs": {
            "lint-status": "${{ steps.lint.outcome }}"
          },
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "${{ inputs.node-version || '20' }}",
                "cache-key-suffix": "lint"
              }
            },
            {
              "name": "Install tsx for scripts",
              "run": "npm install --no-save tsx"
            },
            {
              "name": "Patch legacy path resolver",
              "run": "npx tsx scripts/patchPathResolver.ts"
            },
            {
              "name": "Validate route patterns",
              "run": "npx tsx scripts/validateRoutes.ts"
            },
            {
              "name": "Run ESLint",
              "id": "lint",
              "run": "echo \"üîç Running ESLint...\"\n# Run ESLint but don't fail the build on warnings - capture exit code separately\nnpx next lint --max-warnings=2000 2>&1 | tee lint-results.txt\nLINT_EXIT_CODE=${PIPESTATUS[0]}\n\n# Count warnings for reporting\nWARNING_COUNT=$(grep -c \"Warning:\" lint-results.txt || echo \"0\")\necho \"Found $WARNING_COUNT ESLint warnings\"\n\n# Only capture first 100 lines to avoid HEREDOC issues\necho \"lint-output<<EOF\" >> $GITHUB_OUTPUT\nhead -n 100 lint-results.txt >> $GITHUB_OUTPUT\necho \"EOF\" >> $GITHUB_OUTPUT\n\necho \"warning-count=$WARNING_COUNT\" >> $GITHUB_OUTPUT\n\n# Only fail if there are actual ESLint errors (not warnings)\nif [ $LINT_EXIT_CODE -ne 0 ] && grep -q \"Error:\" lint-results.txt; then\n  echo \"‚ùå ESLint found errors (not just warnings)\"\n  exit 1\nelse\n  echo \"‚úÖ ESLint completed (warnings: $WARNING_COUNT)\"\n  exit 0\nfi\n"
            },
            {
              "name": "Check Prettier formatting",
              "run": "echo \"üé® Checking code formatting...\"\nnpm run format:check || true\n"
            },
            {
              "name": "Upload lint results",
              "if": "failure()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "lint-results",
                "path": "lint-results.txt",
                "retention-days": 7
              }
            }
          ]
        },
        "typecheck": {
          "name": "TypeScript Type Check",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 8,
          "outputs": {
            "typecheck-status": "${{ steps.typecheck.outcome }}"
          },
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "${{ inputs.node-version || '20' }}",
                "cache-key-suffix": "typecheck"
              }
            },
            {
              "name": "Run TypeScript compiler",
              "id": "typecheck",
              "run": "echo \"üîç Running TypeScript type checking...\"\nnpm run type-check 2>&1 | tee typecheck-results.txt\necho \"typecheck-output<<EOF\" >> $GITHUB_OUTPUT\ncat typecheck-results.txt >> $GITHUB_OUTPUT\necho \"EOF\" >> $GITHUB_OUTPUT\n"
            },
            {
              "name": "Upload typecheck results",
              "if": "failure()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "typecheck-results",
                "path": "typecheck-results.txt",
                "retention-days": 7
              }
            }
          ]
        },
        "test": {
          "name": "Unit Tests",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 15,
          "env": {
            "DATABASE_URL_SQLITE": "./test_hotdog_diaries.db",
            "JWT_SECRET": "test-jwt-secret-for-ci"
          },
          "outputs": {
            "results": "${{ steps.test.outputs.results }}",
            "coverage": "${{ steps.test.outputs.coverage }}"
          },
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "${{ inputs.node-version || '20' }}",
                "cache-key-suffix": "test"
              }
            },
            {
              "name": "Initialize test database",
              "run": "echo \"üóÑÔ∏è Setting up test database...\"\nNODE_ENV=test npx tsx scripts/init-sqlite.ts || echo \"Database init attempted\"\n"
            },
            {
              "name": "Run unit tests",
              "id": "test",
              "run": "echo \"üß™ Running unit tests...\"\n# Run tests but don't fail CI on test failures\nnpm test 2>&1 | tee test-results.txt || true\n\n# Extract test summary\nTESTS_PASSED=$(grep -o \"Tests:.*passed\" test-results.txt | head -1 || echo \"Tests: unknown\")\nTESTS_FAILED=$(grep -o \"Tests:.*failed\" test-results.txt | head -1 || echo \"Tests: unknown\")\nTEST_SUITES=$(grep -o \"Test Suites:.*\" test-results.txt | head -1 || echo \"Test Suites: unknown\")\n\necho \"results=${TESTS_PASSED}\" >> $GITHUB_OUTPUT\necho \"failures=${TESTS_FAILED}\" >> $GITHUB_OUTPUT\necho \"suites=${TEST_SUITES}\" >> $GITHUB_OUTPUT\n\necho \"‚ÑπÔ∏è Test results: ${TEST_SUITES}\"\necho \"‚ÑπÔ∏è Individual tests: ${TESTS_PASSED}, ${TESTS_FAILED}\"\n"
            },
            {
              "name": "Run unit tests with coverage",
              "run": "echo \"üìä Running tests with coverage...\"\n# Run coverage but don't fail CI on coverage issues\nnpm run test:coverage || echo \"‚ö†Ô∏è Coverage completed with issues\"\n"
            },
            {
              "name": "Upload test results",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "test-results",
                "path": "test-results.txt\ncoverage/\n",
                "retention-days": 7
              }
            }
          ]
        },
        "planner-tests": {
          "name": "Planner Contract Tests",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 5,
          "env": {
            "SCAN_MIN_PER_PLATFORM": 40,
            "SCAN_MAX_PER_PLATFORM": 120,
            "SCAN_GLOBAL_MAX": 800,
            "SCAN_COOLDOWN_MIN": 180,
            "MIN_CONF": 0.7,
            "MIN_CANDIDATES": 20,
            "PLATFORM_ALLOW": "reddit,youtube,giphy,imgur,bluesky,tumblr,lemmy,pixabay"
          },
          "outputs": {
            "planner-status": "${{ steps.planner-test.outcome }}"
          },
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "${{ inputs.node-version || '20' }}",
                "cache-key-suffix": "planner"
              }
            },
            {
              "name": "Run planner contract tests",
              "id": "planner-test",
              "run": "echo \"üß™ Running demand-driven scanner planner tests...\"\nnpm run test:planner 2>&1 | tee planner-test-results.txt\n\n# Extract test summary\nif grep -q \"tests passed\" planner-test-results.txt; then\n  echo \"‚úÖ Planner tests passed\"\n  echo \"planner-results=passed\" >> $GITHUB_OUTPUT\nelse\n  echo \"‚ùå Planner tests failed\"\n  echo \"planner-results=failed\" >> $GITHUB_OUTPUT\nfi\n"
            },
            {
              "name": "Upload planner test results",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "planner-test-results",
                "path": "planner-test-results.txt\nscan_plan.json\nscan_matrix.json\n",
                "retention-days": 7,
                "if-no-files-found": "ignore"
              }
            }
          ]
        },
        "security": {
          "name": "Security & Dependency Audit",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 10,
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "${{ inputs.node-version || '20' }}",
                "cache-key-suffix": "security"
              }
            },
            {
              "name": "Install tsx for scripts",
              "run": "npm install --no-save tsx"
            },
            {
              "name": "Audit for legacy dependencies",
              "run": "echo \"üîç Scanning for problematic legacy dependencies...\"\nnpx tsx scripts/findOffender.ts || (echo \"‚ùå Legacy modules found\" && exit 1)\necho \"‚úÖ Dependency audit passed\"\n"
            },
            {
              "name": "Run security audit",
              "run": "echo \"üîí Running npm security audit...\"\nnpm audit --audit-level=high || echo \"‚ö†Ô∏è Vulnerabilities found - review required\"\n"
            }
          ]
        },
        "build": {
          "name": "Build Check",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 10,
          "needs": [
            "lint",
            "typecheck",
            "planner-tests"
          ],
          "if": "always() && !cancelled()",
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "${{ inputs.node-version || '20' }}",
                "cache-key-suffix": "build"
              }
            },
            {
              "name": "Install tsx for scripts",
              "run": "npm install --no-save tsx"
            },
            {
              "name": "Patch legacy path resolver (build phase)",
              "run": "npx tsx scripts/patchPathResolver.ts"
            },
            {
              "name": "Validate route patterns (build phase)",
              "run": "npx tsx scripts/validateRoutes.ts"
            },
            {
              "name": "Cache Next.js build",
              "uses": "actions/cache@v4",
              "with": {
                "path": ".next/cache\n",
                "key": "nextjs-${{ runner.os }}-${{ hashFiles('package-lock.json') }}-${{ hashFiles('**/*.js', '**/*.jsx', '**/*.ts', '**/*.tsx') }}",
                "restore-keys": "nextjs-${{ runner.os }}-${{ hashFiles('package-lock.json') }}-\n"
              }
            },
            {
              "name": "Build application",
              "env": {
                "DATABASE_URL_SQLITE": "./test_build.db",
                "NODE_ENV": "test"
              },
              "run": "echo \"üèóÔ∏è Building application...\"\nnpm run build\n"
            },
            {
              "name": "Verify build artifacts",
              "run": "echo \"‚úÖ Verifying build output...\"\nls -la .next/\necho \"Build completed successfully\"\n"
            }
          ]
        },
        "auto-healing": {
          "name": "Auto-Healing Gate",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 10,
          "needs": [
            "lint",
            "typecheck",
            "test",
            "planner-tests",
            "security",
            "build"
          ],
          "if": "always() && contains(needs.*.result, 'failure')",
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4",
              "with": {
                "token": "${{ secrets.GITHUB_TOKEN }}",
                "fetch-depth": 0
              }
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "${{ inputs.node-version || '20' }}",
                "cache-key-suffix": "healing"
              }
            },
            {
              "name": "Install tsx for scripts",
              "run": "npm install --no-save tsx"
            },
            {
              "name": "Run critical failure gatekeeper",
              "id": "gatekeeper",
              "run": "echo \"üõ°Ô∏è Running CI Auto-Healing System...\"\nnpx tsx scripts/checkCriticalFailures.ts || GATEKEEPER_EXIT=$?\n\n# Check if auto-fixes were applied\nif [ -f \"reports/ci-health-gate.md\" ]; then\n  AUTO_FIXES=$(grep -c \"Auto-Fixes Applied\" reports/ci-health-gate.md || echo \"0\")\n  echo \"auto-fixes-applied=$AUTO_FIXES\" >> $GITHUB_OUTPUT\n  echo \"‚úÖ Auto-healing analysis completed\"\nelse\n  echo \"auto-fixes-applied=0\" >> $GITHUB_OUTPUT\nfi\n\nexit ${GATEKEEPER_EXIT:-0}\n"
            },
            {
              "name": "Upload healing reports",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "ci-healing-reports",
                "path": "reports/ci-health-gate.md\nreports/lint-auto-fix.md\nreports/security-audit.md\n",
                "retention-days": 30
              }
            },
            {
              "name": "Commit auto-fixes",
              "if": "steps.gatekeeper.outputs.auto-fixes-applied != '0'",
              "run": "echo \"üîß Auto-fixes applied, committing changes...\"\ngit config --local user.email \"action@github.com\"\ngit config --local user.name \"GitHub Action Auto-Fix\"\ngit add -A\n\nif ! git diff --staged --quiet; then\n  git commit -m \"fix: auto-fix CI issues\n  \n  - Applied ${{ steps.gatekeeper.outputs.auto-fixes-applied }} automatic fixes\n  - Generated by CI Auto-Healing System\n  \n  [skip ci]\"\n  git push\n  echo \"‚úÖ Auto-fixes committed and pushed\"\nfi\n"
            }
          ]
        },
        "summary": {
          "name": "CI Summary",
          "runs-on": "ubuntu-latest",
          "needs": [
            "lint",
            "typecheck",
            "test",
            "planner-tests",
            "security",
            "build"
          ],
          "if": "always()",
          "steps": [
            {
              "name": "Generate CI summary",
              "run": "echo \"## üöÄ CI Results Summary\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"| Job | Status | Details |\" >> $GITHUB_STEP_SUMMARY\necho \"|-----|--------|---------|\" >> $GITHUB_STEP_SUMMARY\necho \"| Lint | ${{ needs.lint.result }} | ${{ needs.lint.outputs.lint-status }} |\" >> $GITHUB_STEP_SUMMARY\necho \"| TypeCheck | ${{ needs.typecheck.result }} | ${{ needs.typecheck.outputs.typecheck-status }} |\" >> $GITHUB_STEP_SUMMARY\necho \"| Unit Tests | ${{ needs.test.result }} | ${{ needs.test.outputs.results }} |\" >> $GITHUB_STEP_SUMMARY\necho \"| Planner Tests | ${{ needs.planner-tests.result }} | ${{ needs.planner-tests.outputs.planner-status }} |\" >> $GITHUB_STEP_SUMMARY\necho \"| Security | ${{ needs.security.result }} | Dependency audit |\" >> $GITHUB_STEP_SUMMARY\necho \"| Build | ${{ needs.build.result }} | Build verification |\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Overall status\nif [[ \"${{ needs.lint.result }}\" == \"success\" && \"${{ needs.typecheck.result }}\" == \"success\" && \"${{ needs.test.result }}\" == \"success\" && \"${{ needs.planner-tests.result }}\" == \"success\" && \"${{ needs.security.result }}\" == \"success\" && \"${{ needs.build.result }}\" == \"success\" ]]; then\n  echo \"## ‚úÖ All CI checks passed!\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"## ‚ùå Some CI checks failed - Auto-healing may have been triggered\" >> $GITHUB_STEP_SUMMARY\nfi\n"
            }
          ]
        }
      },
      "env": {
        "NODE_ENV": "test",
        "CI": true
      },
      "concurrency": {
        "group": "ci-${{ github.ref }}",
        "cancel-in-progress": true
      },
      "secrets_refs": [
        "GITHUB_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [
        "./.github/actions/setup-node"
      ],
      "reusable_workflows": [],
      "job_count": 8
    },
    {
      "path": ".github/workflows/cleanup-duplicates.yml",
      "filename": "cleanup-duplicates.yml",
      "name": "Database Cleanup",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 6 * * *"
            }
          ],
          "cron": [
            "0 6 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": {
            "inputs": {
              "dry_run": {
                "description": "Dry run (no actual deletion)",
                "required": false,
                "type": "boolean",
                "default": false
              }
            }
          }
        }
      ],
      "jobs": {
        "cleanup-duplicates": {
          "runs-on": "ubuntu-latest",
          "name": "Remove Duplicate Content",
          "steps": [
            {
              "name": "Cleanup Duplicates",
              "run": "echo \"üßπ Starting duplicate cleanup...\"\n\n# Call the cleanup API endpoint\nresponse=$(curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/cleanup-duplicates\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"dry_run\": ${{ github.event.inputs.dry_run || false }}}' \\\n  --fail-with-body \\\n  -w \"\\n%{http_code}\")\n\n# Extract HTTP status code\nhttp_code=$(echo \"$response\" | tail -n 1)\nbody=$(echo \"$response\" | head -n -1)\n\necho \"Response: $body\"\n\nif [ \"$http_code\" -ne 200 ]; then\n  echo \"‚ùå Cleanup failed with status $http_code\"\n  exit 1\nfi\n\necho \"‚úÖ Cleanup completed successfully\"\n"
            },
            {
              "name": "Check Duplicate Status",
              "if": "success()",
              "run": "echo \"üìä Checking for remaining duplicates...\"\n\n# Call the monitor endpoint\nresponse=$(curl -L -X GET \"${{ secrets.SITE_URL }}/api/admin/monitor/duplicates\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  --fail-with-body)\n\necho \"Duplicate status: $response\"\n\n# Parse response and check if duplicates exist\nif echo \"$response\" | grep -q '\"duplicates_found\":0'; then\n  echo \"‚úÖ No duplicates found\"\nelse\n  echo \"‚ö†Ô∏è  Some duplicates may still exist\"\nfi\n"
            },
            {
              "name": "Log Failure Details",
              "if": "failure()",
              "run": "echo \"üö® Duplicate cleanup job failed at $(date)\"\necho \"Check the workflow logs for details\"\necho \"Workflow run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"\n"
            }
          ]
        },
        "sync-posted-flags": {
          "runs-on": "ubuntu-latest",
          "name": "Sync Posted Flags",
          "needs": "cleanup-duplicates",
          "steps": [
            {
              "name": "Sync is_posted Flags",
              "run": "echo \"üîÑ Syncing is_posted flags...\"\n\nresponse=$(curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/sync/posted-flags\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  --fail-with-body)\n\necho \"Sync response: $response\"\necho \"‚úÖ Sync completed\"\n"
            }
          ]
        }
      },
      "permissions": {
        "contents": "read"
      },
      "secrets_refs": [
        "SITE_URL",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 2
    },
    {
      "path": ".github/workflows/daily-ingestion-report.yml",
      "filename": "daily-ingestion-report.yml",
      "name": "Daily Ingestion Balance Report",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 9 * * *"
            }
          ],
          "cron": [
            "0 9 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": {
            "inputs": {
              "format": {
                "description": "Output format",
                "required": false,
                "default": "table",
                "type": "choice",
                "options": [
                  "table",
                  "json"
                ]
              }
            }
          }
        }
      ],
      "jobs": {
        "ingestion-report": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "uses": "actions/checkout@v4"
            },
            {
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "18",
                "cache": "npm"
              }
            },
            {
              "run": "npm ci"
            },
            {
              "name": "Generate Ingestion Balance Report",
              "env": {
                "SUPABASE_URL": "${{ secrets.SUPABASE_URL }}",
                "SUPABASE_SERVICE_ROLE_KEY": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY_V2 != '' && secrets.SUPABASE_SERVICE_ROLE_KEY_V2 || secrets.SUPABASE_SERVICE_ROLE_KEY }}"
              },
              "run": "if [ \"${{ github.event.inputs.format }}\" = \"json\" ]; then\n  npx tsx scripts/ingestion-balance-report.ts --format=json\nelse\n  npx tsx scripts/ingestion-balance-report.ts\nfi\n"
            },
            {
              "name": "Check Alert Thresholds",
              "env": {
                "SUPABASE_URL": "${{ secrets.SUPABASE_URL }}",
                "SUPABASE_SERVICE_ROLE_KEY": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY_V2 != '' && secrets.SUPABASE_SERVICE_ROLE_KEY_V2 || secrets.SUPABASE_SERVICE_ROLE_KEY }}"
              },
              "run": "npx tsx scripts/ingestion-balance-report.ts --alert-thresholds",
              "continue-on-error": true,
              "id": "alerts"
            },
            {
              "name": "Create Issue on Alerts",
              "if": "steps.alerts.outcome == 'failure'",
              "uses": "actions/github-script@v7",
              "with": {
                "script": "github.rest.issues.create({\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  title: 'Ingestion Balance Alert - ' + new Date().toISOString().split('T')[0],\n  body: 'Automated ingestion balance monitoring detected issues. Check the workflow logs for details.',\n  labels: ['ingestion', 'alert', 'automated']\n})\n"
              }
            }
          ]
        }
      },
      "secrets_refs": [
        "SUPABASE_URL",
        "SUPABASE_SERVICE_ROLE_KEY_V2"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/daily-report.yml",
      "filename": "daily-report.yml",
      "name": "Daily Summary Report",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 0 * * *"
            }
          ],
          "cron": [
            "0 0 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": null
        }
      ],
      "jobs": {
        "generate-report": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Generate Daily Report",
              "run": "echo \"üìä Generating daily summary report...\"\n\n# Get comprehensive stats from schedule endpoint\nSTATS=$(curl -s \"${{ secrets.SITE_URL }}/api/admin/schedule\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Accept: application/json\")\n\nif [ $? -ne 0 ]; then\n  echo \"‚ùå Failed to fetch daily stats\"\n  exit 1\nfi\n\n# Extract key metrics\nPOSTS_TODAY=$(echo \"$STATS\" | jq -r '.stats.todaysPosts // 0')\nQUEUE_SIZE=$(echo \"$STATS\" | jq -r '.queueStatus.totalPosted // 0')\nAPPROVED=$(echo \"$STATS\" | jq -r '.queueStatus.totalApproved // 0')\nDAYS_LEFT=$(echo \"scale=1; $APPROVED / 6\" | bc -l)\n\n# Get posting schedule status\nSCHEDULE=$(curl -s \"${{ secrets.SITE_URL }}/api/admin/schedule\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\")\n\nMISSED_POSTS=$(echo \"$SCHEDULE\" | jq -r '[.schedule.todaysSchedule[] | select(.posted == false and (.time | strptime(\"%H:%M\") | mktime) < now)] | length')\n\necho \"üìà Daily Report Generated:\"\necho \"  Posts Today: $POSTS_TODAY\"\necho \"  Missed Posts: $MISSED_POSTS\"\necho \"  Queue Size: $QUEUE_SIZE\"\necho \"  Approved: $APPROVED\"\necho \"  Days of Content: $DAYS_LEFT\"\n\n# Save report to file\ncat << EOF > daily-report.md\n# Hotdog Diaries Daily Report\n**Date:** $(date -u +\"%Y-%m-%d\")\n\n## üìä Performance Metrics\n- **Posts Today:** $POSTS_TODAY / 6 expected\n- **Missed Posts:** $MISSED_POSTS\n- **Success Rate:** $(echo \"scale=1; $POSTS_TODAY * 100 / 6\" | bc -l)%\n\n## üì¶ Content Queue Status\n- **Total Items:** $QUEUE_SIZE\n- **Approved & Ready:** $APPROVED\n- **Days of Content:** $DAYS_LEFT days\n- **Health Status:** $([ $(echo \"$DAYS_LEFT > 3\" | bc -l) -eq 1 ] && echo \"‚úÖ Healthy\" || echo \"‚ö†Ô∏è Low\")\n\n## üîß Recommended Actions\n$(if [ $MISSED_POSTS -gt 0 ]; then echo \"- üö® **URGENT:** Run catch-up posting for $MISSED_POSTS missed meals\"; fi)\n$(if [ $(echo \"$DAYS_LEFT < 3\" | bc -l) -eq 1 ]; then echo \"- üì° **HIGH PRIORITY:** Emergency content scanning needed\"; fi)\n$(if [ $(echo \"$DAYS_LEFT < 7\" | bc -l) -eq 1 ]; then echo \"- üîç **NORMAL:** Schedule regular content scanning\"; fi)\n\n---\n*Generated by GitHub Actions at $(date -u)*\nEOF\n\ncat daily-report.md\n"
            },
            {
              "name": "Check for Critical Issues",
              "run": "echo \"üö® Checking for critical issues...\"\n\nSTATS=$(curl -s \"${{ secrets.SITE_URL }}/api/admin/schedule\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\")\n\nAPPROVED=$(echo \"$STATS\" | jq -r '.queueStatus.totalApproved // 0')\nDAYS_LEFT=$(echo \"scale=1; $APPROVED / 6\" | bc -l)\n\n# Check if we need immediate action\nif [ $(echo \"$DAYS_LEFT < 1\" | bc -l) -eq 1 ]; then\n  echo \"üö® CRITICAL ALERT: Less than 1 day of content remaining!\"\n  echo \"Triggering emergency scan workflow...\"\n  \n  # Could trigger emergency scan here\n  curl -X POST \\\n    -H \"Authorization: token ${{ secrets.GITHUB_TOKEN }}\" \\\n    -H \"Accept: application/vnd.github.v3+json\" \\\n    \"https://api.github.com/repos/${{ github.repository }}/actions/workflows/queue-monitor.yml/dispatches\" \\\n    -d '{\"ref\":\"main\"}'\n  \nelif [ $(echo \"$DAYS_LEFT < 3\" | bc -l) -eq 1 ]; then\n  echo \"‚ö†Ô∏è WARNING: Less than 3 days of content remaining\"\n  \nelse\n  echo \"‚úÖ Queue status is acceptable ($DAYS_LEFT days)\"\nfi\n"
            },
            {
              "name": "Archive Report",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "daily-report-${{ github.run_number }}",
                "path": "daily-report.md",
                "retention-days": 30
              }
            }
          ]
        }
      },
      "secrets_refs": [
        "SITE_URL",
        "AUTH_TOKEN",
        "GITHUB_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/deploy-gate.yml",
      "filename": "deploy-gate.yml",
      "name": "Deploy Gate",
      "triggers": [
        {
          "event": "deployment_status",
          "config": null
        },
        {
          "event": "push",
          "config": {
            "branches": [
              "main"
            ]
          }
        },
        {
          "event": "workflow_run",
          "config": {
            "workflows": [
              "Vercel Production Deployment"
            ],
            "types": [
              "completed"
            ]
          }
        }
      ],
      "jobs": {
        "context": {
          "runs-on": "ubuntu-latest",
          "name": "Deployment Context Analysis",
          "outputs": {
            "state": "${{ steps.ctx.outputs.state }}",
            "url": "${{ steps.ctx.outputs.url }}",
            "commit": "${{ steps.ctx.outputs.commit }}",
            "reason": "${{ steps.ctx.outputs.reason }}",
            "proceed": "${{ steps.ctx.outputs.proceed }}",
            "environment": "${{ steps.ctx.outputs.environment }}"
          },
          "steps": [
            {
              "name": "Checkout",
              "uses": "actions/checkout@v4"
            },
            {
              "uses": "pnpm/action-setup@v4"
            },
            {
              "name": "Setup Node.js with pnpm",
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "20",
                "cache": "pnpm"
              }
            },
            {
              "name": "Install dependencies",
              "run": "pnpm install --frozen-lockfile"
            },
            {
              "name": "Analyze deployment context",
              "id": "ctx",
              "run": "echo \"üîç Analyzing deployment context for gate decisions...\"\necho \"Event: ${{ github.event_name }}\"\n\n# For deployment_status events, check the deployment state first\nif [ \"${{ github.event_name }}\" = \"deployment_status\" ]; then\n  DEPLOY_STATE=\"${{ github.event.deployment_status.state }}\"\n  echo \"üìå Deployment status event with state: $DEPLOY_STATE\"\n  \n  # Only proceed if deployment was successful\n  if [ \"$DEPLOY_STATE\" != \"success\" ]; then\n    echo \"‚è∏Ô∏è Deployment state is not success ($DEPLOY_STATE) - neutralizing\"\n    echo \"proceed=false\" >> $GITHUB_OUTPUT\n    echo \"state=$DEPLOY_STATE\" >> $GITHUB_OUTPUT\n    echo \"url=${{ github.event.deployment_status.target_url }}\" >> $GITHUB_OUTPUT\n    echo \"commit=${{ github.event.deployment.sha }}\" >> $GITHUB_OUTPUT\n    echo \"reason=Deployment state is $DEPLOY_STATE\" >> $GITHUB_OUTPUT\n    echo \"neutralized=true\" >> $GITHUB_OUTPUT\n    exit 0\n  fi\n  \n  # For successful deployments, extract context\n  echo \"proceed=true\" >> $GITHUB_OUTPUT\n  echo \"state=success\" >> $GITHUB_OUTPUT\n  echo \"url=${{ github.event.deployment_status.target_url }}\" >> $GITHUB_OUTPUT\n  echo \"commit=${{ github.event.deployment.sha }}\" >> $GITHUB_OUTPUT\n  echo \"reason=Deployment successful\" >> $GITHUB_OUTPUT\n  echo \"environment=${{ github.event.deployment.environment }}\" >> $GITHUB_OUTPUT\n  exit 0\nfi\n\n# For push events, wait longer for deployment to materialize\nMAX_WAIT=4\nif [ \"${{ github.event_name }}\" = \"push\" ]; then\n  MAX_WAIT=6\n  echo \"üìå Push event: will wait up to ${MAX_WAIT} minutes for Vercel deployment\"\nfi\n\n# Run deployment context analysis for non-deployment_status events\nif ! pnpm tsx scripts/ci/lib/deploy-context.ts analyze --max-wait=$MAX_WAIT; then\n  EXIT_CODE=$?\n  if [ $EXIT_CODE -eq 78 ]; then\n    echo \"‚è∏Ô∏è Deployment not ready - will conclude neutrally\"\n    echo \"proceed=false\" >> $GITHUB_OUTPUT\n    echo \"neutralized=true\" >> $GITHUB_OUTPUT\n    exit 0\n  else\n    echo \"‚ùå Hard error analyzing deployment context\"\n    echo \"‚ö†Ô∏è Advisory mode: Deployment context analysis failed but neutralizing gate\"\n    echo \"proceed=false\" >> $GITHUB_OUTPUT\n    echo \"neutralized=true\" >> $GITHUB_OUTPUT\n    exit 0\n  fi\nfi\n\necho \"‚úÖ Deployment ready for gate validation\"\n",
              "env": {
                "VERCEL_TOKEN": "${{ secrets.VERCEL_TOKEN }}",
                "VERCEL_PROJECT_ID": "${{ secrets.VERCEL_PROJECT_ID }}",
                "VERCEL_TEAM_ID": "${{ secrets.VERCEL_TEAM_ID }}"
              }
            },
            {
              "name": "Neutralize on deployment failure",
              "if": "steps.ctx.outputs.proceed != 'true'",
              "run": "echo \"‚è∏Ô∏è Neutralizing deploy gate due to upstream deployment issue\"\necho \"State: ${{ steps.ctx.outputs.state }}\"\necho \"Reason: ${{ steps.ctx.outputs.reason }}\"\n\n# Write neutral summary\ncat >> $GITHUB_STEP_SUMMARY << 'EOF'\n## ‚è∏Ô∏è Deploy Gate Neutralized\n\nThe deployment gate has been neutralized because the upstream deployment is not ready for validation.\n\n| Field | Value |\n|-------|-------|\n| **Deployment State** | `${{ steps.ctx.outputs.state }}` |\n| **Reason** | ${{ steps.ctx.outputs.reason }} |\n| **URL Available** | ${{ steps.ctx.outputs.url && 'Yes' || 'No' }} |\n| **Commit** | `${{ steps.ctx.outputs.commit }}` |\n\n### üîÑ Next Steps\n\nThis is normal and expected for:\n- Failed deployments (fix deployment issues first)\n- Deployments still in progress (wait for completion)\n- Missing preview URLs (check Vercel configuration)\n\n**No action required** - the gate will automatically re-run when the deployment succeeds.\nEOF\n\nexit 0\n"
            }
          ]
        },
        "neutralize": {
          "runs-on": "ubuntu-latest",
          "name": "Neutralize Deployment Gate",
          "needs": [
            "context"
          ],
          "if": "needs.context.outputs.proceed != 'true'",
          "steps": [
            {
              "name": "Checkout for neutralize action",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Neutralize with summary",
              "uses": "./.github/actions/neutralize",
              "with": {
                "reason": "Upstream deployment not successful or preview URL unavailable",
                "deploy_state": "${{ needs.context.outputs.state }}",
                "deploy_reason": "${{ needs.context.outputs.reason }}",
                "deploy_url": "${{ needs.context.outputs.url }}"
              }
            }
          ]
        },
        "auth-token-validation": {
          "runs-on": "ubuntu-latest",
          "name": "Validate Runtime JWT Deploy Gate",
          "needs": "context",
          "if": "always() && \nneeds.context.result == 'success' &&\nneeds.context.outputs.proceed == 'true'\n",
          "steps": [
            {
              "name": "Checkout",
              "uses": "actions/checkout@v4"
            },
            {
              "uses": "pnpm/action-setup@v4"
            },
            {
              "name": "Setup Node.js with pnpm",
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "20",
                "cache": "pnpm"
              }
            },
            {
              "name": "Install dependencies",
              "run": "pnpm install --frozen-lockfile"
            },
            {
              "name": "Set deployment URL",
              "run": "echo \"üåê Using deployment URL from context analysis...\"\nDEPLOY_URL=\"${{ needs.context.outputs.url }}\"\nif [ -n \"$DEPLOY_URL\" ]; then\n  echo \"‚úÖ Using resolved deployment URL: $DEPLOY_URL\"\n  echo \"TARGET_URL=$DEPLOY_URL\" >> $GITHUB_ENV\nelse\n  echo \"üìå No deployment URL, falling back to production URL\"\n  echo \"TARGET_URL=${{ env.PROD_URL }}\" >> $GITHUB_ENV\nfi\n"
            },
            {
              "name": "Mint runtime JWT token",
              "run": "echo \"üîê Minting runtime JWT token for deploy gate...\"\n\n# Try runtime token first, fall back to legacy AUTH_TOKEN if minting fails\nif TOKEN=$(pnpm -s tsx scripts/ci/lib/jwt.ts mint --ttl 15m --sub ci-gate --aud admin --iss hotdog-diaries 2>/dev/null); then\n  echo \"‚úÖ Successfully minted runtime JWT token\"\n  echo \"TOKEN=$TOKEN\" >> $GITHUB_ENV\n  echo \"TOKEN_TYPE=runtime\" >> $GITHUB_ENV\nelif [ -n \"${{ secrets.AUTH_TOKEN }}\" ]; then\n  echo \"‚ö†Ô∏è Runtime token minting failed, falling back to legacy AUTH_TOKEN\"\n  echo \"TOKEN=${{ secrets.AUTH_TOKEN }}\" >> $GITHUB_ENV\n  echo \"TOKEN_TYPE=legacy\" >> $GITHUB_ENV\nelse\n  echo \"‚ùå No token available - neither runtime minting nor legacy AUTH_TOKEN works\"\n  echo \"‚ö†Ô∏è Advisory mode: Logging token issue but not failing deployment gate\"\n  echo \"TOKEN=advisory-mode-no-token\" >> $GITHUB_ENV\n  echo \"TOKEN_TYPE=advisory\" >> $GITHUB_ENV\nfi\n",
              "env": {
                "JWT_SECRET": "${{ secrets.JWT_SECRET }}"
              }
            },
            {
              "name": "Test runtime JWT validation",
              "run": "echo \"üîç Testing runtime JWT validation...\"\necho \"Token type: $TOKEN_TYPE\"\necho \"Target URL: $TARGET_URL\"\n\n# Test JWT token locally first (quick validation) - advisory mode\nif [ \"$TOKEN_TYPE\" = \"advisory\" ]; then\n  echo \"‚ö†Ô∏è Advisory mode: Skipping token verification (no token available)\"\nelif ! pnpm -s tsx scripts/ci/lib/jwt.ts verify --token \"$TOKEN\" 2>/dev/null; then\n  echo \"‚ùå Token failed local verification\"\n  echo \"‚ö†Ô∏è Advisory mode: Logging verification failure but not failing deployment gate\"\n  echo \"TOKEN_VERIFIED=false\" >> $GITHUB_ENV\nelse\n  echo \"‚úÖ Token passed local verification\"\n  echo \"TOKEN_VERIFIED=true\" >> $GITHUB_ENV\nfi\n\n# Check if this is a preview deployment and test basic connectivity\nif [[ \"$TARGET_URL\" == *\"vercel.app\"* ]] && [[ \"$TARGET_URL\" != *\"hotdog-diaries.vercel.app\"* ]]; then\n  echo \"‚ö†Ô∏è Preview deployment detected - checking basic connectivity\"\n  \n  # Try preview health endpoint first\n  RESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n    \"$TARGET_URL/api/health\" || echo \"HTTPSTATUS:000\")\n  \n  HTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\n  \n  # If preview fails with auth wall, fall back to production\n  if [ \"$HTTP_STATUS\" -eq 401 ] && echo \"$RESPONSE\" | grep -q \"Authentication Required\"; then\n    echo \"üîÑ Preview has Vercel auth protection - testing production instead\"\n    TARGET_URL=\"${{ env.PROD_URL }}\"\n    echo \"Fallback URL: $TARGET_URL\"\n    \n    RESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n      \"$TARGET_URL/api/health\")\n  fi\nelse\n  echo \"üåê Testing production or main deployment\"\n  RESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n    \"$TARGET_URL/api/health\")\nfi\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//g')\n\necho \"HTTP Status: $HTTP_STATUS\"\necho \"Response Body (first 200 chars): ${BODY:0:200}\"\n\nif [ \"$HTTP_STATUS\" -eq 200 ]; then\n  echo \"‚úÖ Basic health endpoint responding\"\n  \n  # Parse health response\n  if echo \"$BODY\" | jq -e '.success == true' > /dev/null; then\n    echo \"‚úÖ Health endpoint structure valid\"\n    \n    # Check for service info\n    SERVICE=$(echo \"$BODY\" | jq -r '.data.service // \"unknown\"')\n    ENV=$(echo \"$BODY\" | jq -r '.data.environment // \"unknown\"')\n    \n    echo \"üìä Deployment Details:\"\n    echo \"  - Service: $SERVICE\"\n    echo \"  - Environment: $ENV\"\n    echo \"  - Token Type: $TOKEN_TYPE\"\n    echo \"  - Target URL: $TARGET_URL\"\n    \n    # For runtime tokens, this validates both the minting process and basic connectivity\n    if [ \"$TOKEN_TYPE\" = \"runtime\" ]; then\n      echo \"‚úÖ Runtime JWT token and deployment connectivity validated\"\n    else\n      echo \"‚úÖ Legacy token and deployment connectivity validated\"\n    fi\n    \n  else\n    echo \"‚ùå Health endpoint returned unexpected structure\"\n    echo \"‚ö†Ô∏è Advisory mode: Logging structure issue but not failing deployment gate\"\n  fi\nelse\n  echo \"‚ùå Health endpoint failed with status $HTTP_STATUS\"\n  echo \"This indicates deployment or connectivity issues\"\n  echo \"‚ö†Ô∏è Advisory mode: Logging connectivity issue but not failing deployment gate\"\nfi\n"
            },
            {
              "name": "Test health probe with invalid token",
              "run": "echo \"üß™ Testing auth token health probe with invalid token...\"\n\n# Check if this is a preview deployment and handle Vercel auth protection\nif [[ \"$TARGET_URL\" == *\"vercel.app\"* ]] && [[ \"$TARGET_URL\" != *\"hotdog-diaries.vercel.app\"* ]]; then\n  echo \"‚ö†Ô∏è Preview deployment - testing for Vercel auth protection...\"\n  \n  # Try preview first\n  RESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n    -H \"Authorization: Bearer invalid-token-12345\" \\\n    \"$TARGET_URL/api/admin/health/auth-token\" || echo \"HTTPSTATUS:000\")\n  \n  HTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\n  \n  # If preview fails with auth wall, fall back to production\n  if [ \"$HTTP_STATUS\" -eq 401 ] && echo \"$RESPONSE\" | grep -q \"Authentication Required\"; then\n    echo \"üîÑ Preview has Vercel auth protection - testing production instead\"\n    TARGET_URL=\"${{ env.PROD_URL }}\"\n    echo \"Fallback URL: $TARGET_URL\"\n    \n    RESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n      -H \"Authorization: Bearer invalid-token-12345\" \\\n      \"$TARGET_URL/api/admin/health/auth-token\")\n  fi\nelse\n  echo \"üåê Testing production or main deployment\"\n  RESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n    -H \"Authorization: Bearer invalid-token-12345\" \\\n    \"$TARGET_URL/api/admin/health/auth-token\")\nfi\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//g')\n\necho \"HTTP Status: $HTTP_STATUS\"\necho \"Response Body (first 200 chars): ${BODY:0:200}\"\n\nif [ \"$HTTP_STATUS\" -eq 401 ]; then\n  echo \"‚úÖ Invalid token correctly rejected\"\n  \n  # Verify response contains mismatch code\n  CODE=$(echo \"$BODY\" | jq -r '.code // \"unknown\"')\n  if [ \"$CODE\" = \"AUTH_TOKEN_MISMATCH\" ]; then\n    echo \"‚úÖ Response contains correct error code: AUTH_TOKEN_MISMATCH\"\n  else\n    echo \"‚ö†Ô∏è Response contains unexpected error code: $CODE\"\n  fi\nelse\n  echo \"‚ùå Invalid token should have been rejected with 401\"\n  echo \"‚ö†Ô∏è Advisory mode: Logging auth validation issue but not failing deployment gate\"\nfi\n"
            },
            {
              "name": "Test health probe without token",
              "run": "echo \"üß™ Testing auth token health probe without token...\"\n\n# Check if this is a preview deployment and handle Vercel auth protection\nif [[ \"$TARGET_URL\" == *\"vercel.app\"* ]] && [[ \"$TARGET_URL\" != *\"hotdog-diaries.vercel.app\"* ]]; then\n  echo \"‚ö†Ô∏è Preview deployment - testing for Vercel auth protection...\"\n  \n  # Try preview first\n  RESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n    \"$TARGET_URL/api/admin/health/auth-token\" || echo \"HTTPSTATUS:000\")\n  \n  HTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\n  \n  # If preview fails with auth wall, fall back to production\n  if [ \"$HTTP_STATUS\" -eq 401 ] && echo \"$RESPONSE\" | grep -q \"Authentication Required\"; then\n    echo \"üîÑ Preview has Vercel auth protection - testing production instead\"\n    TARGET_URL=\"${{ env.PROD_URL }}\"\n    echo \"Fallback URL: $TARGET_URL\"\n    \n    RESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n      \"$TARGET_URL/api/admin/health/auth-token\")\n  fi\nelse\n  echo \"üåê Testing production or main deployment\"\n  RESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n    \"$TARGET_URL/api/admin/health/auth-token\")\nfi\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//g')\n\necho \"HTTP Status: $HTTP_STATUS\"\necho \"Response Body: $BODY\"\n\nif [ \"$HTTP_STATUS\" -eq 401 ]; then\n  echo \"‚úÖ Missing token correctly rejected\"\n  \n  # Verify response contains missing token code\n  CODE=$(echo \"$BODY\" | jq -r '.code // \"unknown\"')\n  if [ \"$CODE\" = \"AUTH_TOKEN_MISSING\" ]; then\n    echo \"‚úÖ Response contains correct error code: AUTH_TOKEN_MISSING\"\n  else\n    echo \"‚ö†Ô∏è Response contains unexpected error code: $CODE\"\n  fi\nelse\n  echo \"‚ùå Missing token should have been rejected with 401\"\n  echo \"‚ö†Ô∏è Advisory mode: Logging auth validation issue but not failing deployment gate\"\nfi\n"
            }
          ]
        },
        "deployment-gate-result": {
          "runs-on": "ubuntu-latest",
          "name": "Deployment Gate Result",
          "needs": [
            "context",
            "neutralize",
            "auth-token-validation"
          ],
          "if": "always()",
          "steps": [
            {
              "name": "Check gate results (Advisory Mode)",
              "run": "echo \"üö™ Deployment Gate Results (Advisory Mode)\"\necho \"===========================================\"\n\nCONTEXT_PROCEED=\"${{ needs.context.outputs.proceed }}\"\nCONTEXT_STATE=\"${{ needs.context.outputs.state }}\"\nAUTH_RESULT=\"${{ needs.auth-token-validation.result }}\"\n\necho \"Context Analysis: proceed=$CONTEXT_PROCEED, state=$CONTEXT_STATE\"\necho \"Auth Token Validation: $AUTH_RESULT\"\necho \"Note: Deep health checks moved to prod-watchdog.yml for scheduled monitoring\"\n\n# Always conclude neutrally/successfully in streamlined CI mode\n# Convert all results to advisory/informational status\n\nif [ \"$CONTEXT_PROCEED\" != \"true\" ]; then\n  echo \"\"\n  echo \"‚ÑπÔ∏è DEPLOYMENT GATE: ADVISORY STATUS\"\n  echo \"Deployment not ready for validation (state: $CONTEXT_STATE)\"\n  echo \"Reason: ${{ needs.context.outputs.reason }}\"\n  echo \"Status: Informational - this does not block deployment\"\n  exit 0\nfi\n\n# Report validation results as advisory information (streamlined)\nif [ \"$AUTH_RESULT\" = \"skipped\" ]; then\n  echo \"\"\n  echo \"‚ÑπÔ∏è DEPLOYMENT GATE: ADVISORY STATUS\" \n  echo \"Auth validation skipped due to upstream deployment context.\"\n  echo \"Status: Informational - this is expected for some deployment states\"\nelif [ \"$AUTH_RESULT\" = \"success\" ]; then\n  echo \"\"\n  echo \"‚úÖ DEPLOYMENT GATE: ADVISORY PASSED\"\n  echo \"Auth token validation successful.\"\n  echo \"Status: Informational - basic deployment validation looks good\"\n  echo \"Note: Deep health checks run separately in prod-watchdog.yml\"\nelse\n  echo \"\"\n  echo \"‚ö†Ô∏è DEPLOYMENT GATE: ADVISORY WARNINGS\"\n  echo \"Auth token validation had issues.\"\n  echo \"Auth result: $AUTH_RESULT\"\n  echo \"Status: Informational - consider investigating auth configuration\"\n  echo \"Note: This does not block deployment in streamlined CI mode\"\nfi\n\n# Always exit 0 for advisory mode - never fail the gate\necho \"\"\necho \"üîÑ Advisory mode: All deployment gate results are informational only\"\nexit 0\n"
            },
            {
              "name": "‚ÑπÔ∏è Advisory Success Summary",
              "if": "needs.context.outputs.proceed == 'true' && needs.auth-token-validation.result == 'success'",
              "run": "echo \"‚ÑπÔ∏è DEPLOYMENT GATE: ADVISORY SUCCESS\"\necho \"====================================\"\necho \"‚úÖ Auth Token Validation: Success\"\necho \"‚ÑπÔ∏è Deep Health Check: Moved to prod-watchdog.yml\"\necho \"‚ÑπÔ∏è Admin Endpoint Smoke Test: Moved to prod-watchdog.yml\"\necho \"\"\necho \"üìã Advisory Status: All validations passed\"\necho \"üåê Production URL: ${{ env.PROD_URL }}\"\necho \"üè• Health Probe: ${{ env.PROD_URL }}/api/admin/health/auth-token\"\necho \"\"\necho \"## ‚ÑπÔ∏è Deployment Gate Advisory Report\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Deployment**: \\`${{ github.sha }}\\`\" >> $GITHUB_STEP_SUMMARY\necho \"**Timestamp**: $(date -u -Iseconds)\" >> $GITHUB_STEP_SUMMARY\necho \"**Mode**: Advisory (Streamlined CI)\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"### ‚úÖ Validation Results (Advisory)\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"- ‚úÖ **Auth Token Validation**: Passed\" >> $GITHUB_STEP_SUMMARY\necho \"- ‚ÑπÔ∏è **Deep Health Check**: Moved to prod-watchdog.yml (scheduled)\" >> $GITHUB_STEP_SUMMARY\necho \"- ‚ÑπÔ∏è **Admin Endpoint Smoke Test**: Moved to prod-watchdog.yml (scheduled)\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"üìã **Status**: All deployment validations passed in advisory mode\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"> üîÑ **Note**: This gate runs in advisory mode and provides informational status only\" >> $GITHUB_STEP_SUMMARY\n"
            },
            {
              "name": "Log advisory warnings (No Issue Creation)",
              "if": "needs.context.outputs.proceed == 'true' && needs.auth-token-validation.result != 'success'",
              "run": "echo \"‚ö†Ô∏è ADVISORY MODE: Deployment gate warnings detected\"\necho \"=================================================\"\necho \"\"\necho \"**Deployment**: ${{ github.sha }}\"\necho \"**Timestamp**: $(date -u -Iseconds)\"\necho \"**Mode**: Advisory (Streamlined CI)\"\necho \"\"\necho \"### Validation Results\"\necho \"- Auth Token Validation: ${{ needs.auth-token-validation.result }}\"\necho \"- Deep Health Check: Moved to prod-watchdog.yml (scheduled monitoring)\"\necho \"\"\necho \"### Advisory Notice\"\necho \"In streamlined CI mode, deployment gate warnings are logged\"\necho \"but do not block deployment or create issues automatically.\"\necho \"\"\necho \"üìã Consider reviewing these warnings in workflow logs:\"\necho \"üîó ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"\necho \"\"\necho \"üè• Health endpoints for manual verification:\"\necho \"- Basic: ${{ env.PROD_URL }}/api/health\"\necho \"- Auth: ${{ env.PROD_URL }}/api/admin/health/auth-token\"\necho \"- Deep: ${{ env.PROD_URL }}/api/admin/health/deep\"\n"
            }
          ]
        }
      },
      "permissions": {
        "contents": "read",
        "actions": "read",
        "checks": "write"
      },
      "env": {
        "PROD_URL": "https://hotdog-diaries.vercel.app"
      },
      "secrets_refs": [
        "VERCEL_TOKEN",
        "VERCEL_PROJECT_ID",
        "VERCEL_TEAM_ID",
        "AUTH_TOKEN",
        "JWT_SECRET"
      ],
      "vars_refs": [],
      "composite_actions": [
        "./.github/actions/neutralize"
      ],
      "reusable_workflows": [],
      "job_count": 4
    },
    {
      "path": ".github/workflows/deployment-gate.yml",
      "filename": "deployment-gate.yml",
      "name": "üö™ Deployment Gate",
      "triggers": [
        {
          "event": "workflow_dispatch",
          "config": {
            "inputs": {
              "target_url": {
                "description": "Deployment base URL (e.g. https://hotdog-diaries.vercel.app)",
                "required": false,
                "type": "string"
              }
            }
          }
        },
        {
          "event": "deployment_status",
          "config": null
        },
        {
          "event": "workflow_call",
          "config": {
            "inputs": {
              "target_url": {
                "description": "Deployment base URL",
                "required": false,
                "type": "string"
              }
            },
            "secrets": {
              "AUTH_TOKEN": {
                "description": "Admin authentication token",
                "required": true
              }
            }
          }
        }
      ],
      "jobs": {
        "gate": {
          "name": "üîê Security & Health Gate",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 10,
          "if": "github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name == github.repository\n",
          "env": {
            "TARGET_URL": "${{ inputs.target_url || github.event.deployment_status.environment_url || vars.SITE_URL || secrets.SITE_URL }}",
            "AUTH_TOKEN": "${{ secrets.AUTH_TOKEN }}"
          },
          "steps": [
            {
              "name": "üß™ Preflight Validation",
              "shell": "bash",
              "run": "set -euo pipefail\necho \"üîç Validating deployment gate requirements...\"\necho \"Event: ${{ github.event_name }}\"\necho \"Environment URL: ${{ github.event.deployment_status.environment_url || 'none' }}\"\necho \"Target URL: ${TARGET_URL:-<empty>}\"\n\nmissing=0\nif [ -z \"${TARGET_URL:-}\" ]; then\n  echo \"::error title=Missing Target URL::No TARGET_URL available. Provide 'inputs.target_url' OR set repository variable 'SITE_URL' OR ensure 'deployment_status.environment_url' is present.\"\n  missing=1\nfi\n\nif [ -z \"${AUTH_TOKEN:-}\" ]; then\n  echo \"::error title=Missing Auth Token::AUTH_TOKEN secret not configured. Add repository secret AUTH_TOKEN with admin JWT token.\"\n  missing=1\nfi\n\nif [ \"$missing\" -ne 0 ]; then\n  echo \"‚ùå Preflight failed ‚Äî required context not available.\"\n  echo \"‚ö†Ô∏è Advisory mode: Missing context but continuing in advisory mode\"\n  echo \"TARGET_URL=${TARGET_URL:-https://hotdog-diaries.vercel.app}\" >> $GITHUB_ENV\nfi\n\necho \"‚úÖ Preflight validation passed\"\necho \"üéØ Target: ${TARGET_URL}\"\n"
            },
            {
              "name": "üîê Authentication Validation",
              "id": "auth",
              "shell": "bash",
              "run": "set -euo pipefail\necho \"üîê Validating admin authentication token...\"\n\nresponse=$(curl -fsS --retry 3 --retry-delay 2 --retry-connrefused -w \"HTTP_CODE:%{http_code}\" \\\n  -H \"x-admin-token: ${AUTH_TOKEN}\" \\\n  -H \"Authorization: Bearer ${AUTH_TOKEN}\" \\\n  \"${TARGET_URL}/api/admin/health/auth-token\" 2>/dev/null || echo \"HTTP_CODE:000\")\n\nhttp_code=$(echo \"$response\" | grep -o \"HTTP_CODE:[0-9]*\" | cut -d: -f2)\nbody=$(echo \"$response\" | sed 's/HTTP_CODE:[0-9]*$//')\n\nif [ \"$http_code\" != \"200\" ]; then\n  echo \"‚ùå Auth validation failed with HTTP $http_code\"\n  echo \"Response: $body\"\n  echo \"‚ö†Ô∏è Advisory mode: Auth validation failed but continuing in advisory mode\"\nfi\n\necho \"‚úÖ Authentication token valid\"\necho \"$body\" | jq . || echo \"$body\"\n"
            },
            {
              "name": "üíö Deep Health Check",
              "id": "health",
              "shell": "bash",
              "run": "set -euo pipefail\necho \"üíö Running comprehensive system health check...\"\n\nresponse=$(curl -fsS --retry 3 --retry-delay 2 --retry-connrefused -w \"HTTP_CODE:%{http_code}\" \\\n  -H \"x-admin-token: ${AUTH_TOKEN}\" \\\n  -H \"Authorization: Bearer ${AUTH_TOKEN}\" \\\n  \"${TARGET_URL}/api/admin/health/deep\" 2>/dev/null || echo \"HTTP_CODE:000\")\n\nhttp_code=$(echo \"$response\" | grep -o \"HTTP_CODE:[0-9]*\" | cut -d: -f2)\nbody=$(echo \"$response\" | sed 's/HTTP_CODE:[0-9]*$//')\n\nif [ \"$http_code\" != \"200\" ]; then\n  echo \"‚ùå Health check failed with HTTP $http_code\"\n  echo \"Response: $body\"\n  echo \"‚ö†Ô∏è Advisory mode: Health check failed but continuing in advisory mode\"\nfi\n\necho \"‚úÖ Deep health check passed\"\necho \"$body\" | jq '{\n  database: .database.status,\n  apis: (.apis | length),\n  scheduler: .scheduler.status,\n  overall: .status\n}' || echo \"$body\"\n"
            },
            {
              "name": "‚úÖ Gate Summary",
              "if": "always()",
              "run": "echo \"## üö™ Deployment Gate Results\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"| Check | Status |\" >> $GITHUB_STEP_SUMMARY\necho \"|-------|--------|\" >> $GITHUB_STEP_SUMMARY\necho \"| Auth Token | ${{ steps.auth.outcome == 'success' && '‚úÖ Valid' || '‚ùå Failed' }} |\" >> $GITHUB_STEP_SUMMARY\necho \"| Health Check | ${{ steps.health.outcome == 'success' && '‚úÖ Healthy' || '‚ùå Failed' }} |\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Target:** \\`${TARGET_URL}\\`\" >> $GITHUB_STEP_SUMMARY\n\necho \"üö™ Deployment Gate Results\"\necho \"==========================\"\necho \"Auth Token Validation: ${{ steps.auth.outcome }}\"\necho \"Health Check:          ${{ steps.health.outcome }}\"\n"
            },
            {
              "name": "‚ö†Ô∏è Advisory Mode Report",
              "if": "steps.auth.outcome != 'success' ||\nsteps.health.outcome != 'success'\n",
              "run": "echo \"‚ö†Ô∏è DEPLOYMENT GATE ADVISORY WARNINGS\"\necho \"Security or health validations had issues but running in advisory mode\"\necho \"Auth validation: ${{ steps.auth.outcome }}\"\necho \"Health check: ${{ steps.health.outcome }}\"\necho \"Status: Informational only - deployment not blocked\"\n"
            },
            {
              "name": "üéâ Gate Success",
              "run": "echo \"üéâ DEPLOYMENT GATE PASSED\"\necho \"All security and health validations successful\"\necho \"Deployment is cleared to proceed\"\n"
            }
          ]
        }
      },
      "permissions": {
        "contents": "read"
      },
      "secrets_refs": [
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/e2e.yml",
      "filename": "e2e.yml",
      "name": "ERROR",
      "triggers": [],
      "jobs": {},
      "secrets_refs": [],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 0,
      "error": "can not read an implicit mapping pair; a colon is missed (134:13)\n\n 131 |         browser: ${{ fromJSON(\n 132 |           needs.should-run.outputs.browser  ...\n 133 |           format('[\"{0}\"]', needs.should-ru ...\n 134 |         ) }}\n-------------------^\n 135 |     steps:\n 136 |       - name: Checkout code"
    },
    {
      "path": ".github/workflows/housekeeping.yml",
      "filename": "housekeeping.yml",
      "name": "Housekeeping",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 3 * * 1"
            },
            {
              "cron": "0 6 * * *"
            }
          ],
          "cron": [
            "0 3 * * 1",
            "0 6 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": {
            "inputs": {
              "tasks": {
                "description": "Tasks to run (comma-separated: cleanup, dead-links, licenses, audit, queue-monitor, secrets)",
                "type": "string",
                "default": "all"
              },
              "force-cleanup": {
                "description": "Force aggressive cleanup",
                "type": "boolean",
                "default": false
              }
            }
          }
        },
        {
          "event": "workflow_call",
          "config": {
            "inputs": {
              "tasks": {
                "description": "Tasks to run",
                "type": "string",
                "default": "all"
              },
              "force-cleanup": {
                "description": "Force aggressive cleanup",
                "type": "boolean",
                "default": false
              }
            }
          }
        }
      ],
      "jobs": {
        "determine-tasks": {
          "name": "Determine Tasks",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 2,
          "outputs": {
            "tasks": "${{ steps.strategy.outputs.tasks }}",
            "schedule-type": "${{ steps.strategy.outputs.schedule-type }}",
            "force-cleanup": "${{ steps.strategy.outputs.force-cleanup }}"
          },
          "steps": [
            {
              "name": "Determine housekeeping strategy",
              "id": "strategy",
              "run": "TASKS=\"${{ inputs.tasks || 'all' }}\"\nFORCE_CLEANUP=\"${{ inputs.force-cleanup || 'false' }}\"\nSCHEDULE_TYPE=\"manual\"\n\n# Determine tasks based on schedule\nif [[ \"${{ github.event_name }}\" == \"schedule\" ]]; then\n  SCHEDULE_TYPE=\"scheduled\"\n  HOUR=$(date +%H)\n  DAY=$(date +%u)  # 1=Monday, 7=Sunday\n  \n  if [[ $HOUR -eq 3 && $DAY -eq 1 ]]; then\n    # Weekly comprehensive housekeeping\n    TASKS=\"cleanup,dead-links,licenses,audit,queue-monitor,secrets\"\n    FORCE_CLEANUP=\"true\"\n    echo \"üè† Weekly comprehensive housekeeping\"\n  elif [[ $HOUR -eq 6 ]]; then\n    # Daily light housekeeping\n    TASKS=\"cleanup,queue-monitor\"\n    FORCE_CLEANUP=\"false\"\n    echo \"üßπ Daily light housekeeping\"\n  fi\nfi\n\nif [[ \"$TASKS\" == \"all\" ]]; then\n  TASKS=\"cleanup,dead-links,licenses,audit,queue-monitor,secrets\"\nfi\n\necho \"tasks=$TASKS\" >> $GITHUB_OUTPUT\necho \"schedule-type=$SCHEDULE_TYPE\" >> $GITHUB_OUTPUT\necho \"force-cleanup=$FORCE_CLEANUP\" >> $GITHUB_OUTPUT\necho \"Selected tasks: $TASKS (force: $FORCE_CLEANUP)\"\n"
            }
          ]
        },
        "cleanup-duplicates": {
          "name": "Cleanup Duplicates",
          "runs-on": "ubuntu-latest",
          "needs": "determine-tasks",
          "if": "contains(needs.determine-tasks.outputs.tasks, 'cleanup')",
          "timeout-minutes": 10,
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "20",
                "cache-key-suffix": "housekeeping-cleanup"
              }
            },
            {
              "name": "Setup Supabase environment",
              "uses": "./.github/actions/setup-supabase-rest",
              "with": {
                "supabase-url": "${{ secrets.SUPABASE_URL }}",
                "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY_V2 != '' && secrets.SUPABASE_SERVICE_ROLE_KEY_V2 || secrets.SUPABASE_SERVICE_ROLE_KEY }}",
                "database-url": "${{ secrets.DATABASE_URL }}"
              }
            },
            {
              "name": "Clean up duplicate content",
              "run": "echo \"üßπ Cleaning up duplicate content...\"\n\nAUTH_TOKEN=\"${{ secrets.AUTH_TOKEN }}\"\nFORCE_CLEANUP=\"${{ needs.determine-tasks.outputs.force-cleanup }}\"\n\n# Call the cleanup API endpoint\nRESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -X POST \\\n  -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\\\"force\\\": $FORCE_CLEANUP}\" \\\n  \"${{ secrets.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/admin/cleanup/duplicates\" \\\n  --max-time 300)\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//')\n\nif [ \"$HTTP_STATUS\" -eq 200 ]; then\n  echo \"‚úÖ Duplicate cleanup completed\"\n  echo \"$BODY\" > cleanup-duplicates-report.json\nelse\n  echo \"‚ö†Ô∏è Duplicate cleanup failed with status $HTTP_STATUS\"\n  echo \"Response: $BODY\"\nfi\n"
            },
            {
              "name": "Upload cleanup report",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "cleanup-duplicates-report",
                "path": "cleanup-duplicates-report.json",
                "retention-days": 7,
                "if-no-files-found": "ignore"
              }
            }
          ]
        },
        "dead-links-check": {
          "name": "Dead Links Check",
          "runs-on": "ubuntu-latest",
          "needs": "determine-tasks",
          "if": "contains(needs.determine-tasks.outputs.tasks, 'dead-links')",
          "timeout-minutes": 15,
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "20",
                "cache-key-suffix": "housekeeping-deadlinks"
              }
            },
            {
              "name": "Setup Supabase environment",
              "uses": "./.github/actions/setup-supabase-rest",
              "with": {
                "supabase-url": "${{ secrets.SUPABASE_URL }}",
                "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY_V2 != '' && secrets.SUPABASE_SERVICE_ROLE_KEY_V2 || secrets.SUPABASE_SERVICE_ROLE_KEY }}",
                "database-url": "${{ secrets.DATABASE_URL }}"
              }
            },
            {
              "name": "Check for dead links",
              "run": "echo \"üîó Checking for dead links in content...\"\n\n# Run dead link detection script\nnpm run check:dead-links || echo \"Dead link check completed with warnings\"\n\n# Count and report results\nif [ -f \"reports/dead-links.json\" ]; then\n  DEAD_COUNT=$(jq '.deadLinks | length' reports/dead-links.json 2>/dev/null || echo \"0\")\n  echo \"Found $DEAD_COUNT dead links\"\n  \n  if [ \"$DEAD_COUNT\" -gt 0 ]; then\n    echo \"‚ö†Ô∏è Dead links detected - content may need review\"\n  else\n    echo \"‚úÖ No dead links found\"\n  fi\nfi\n"
            },
            {
              "name": "Upload dead links report",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "dead-links-report",
                "path": "reports/dead-links.json\nreports/dead-links.md\n",
                "retention-days": 14,
                "if-no-files-found": "ignore"
              }
            }
          ]
        },
        "license-audit": {
          "name": "License Audit",
          "runs-on": "ubuntu-latest",
          "needs": "determine-tasks",
          "if": "contains(needs.determine-tasks.outputs.tasks, 'licenses')",
          "timeout-minutes": 8,
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "20",
                "cache-key-suffix": "housekeeping-licenses"
              }
            },
            {
              "name": "Audit licenses",
              "run": "echo \"üìú Auditing dependency licenses...\"\n\n# Generate license report\nnpm run licenses:check || echo \"License check completed\"\n\n# Check for problematic licenses\nif [ -f \"reports/licenses.json\" ]; then\n  PROBLEMATIC=$(jq '.problematic | length' reports/licenses.json 2>/dev/null || echo \"0\")\n  echo \"Found $PROBLEMATIC potentially problematic licenses\"\n  \n  if [ \"$PROBLEMATIC\" -gt 0 ]; then\n    echo \"‚ö†Ô∏è License issues detected - manual review required\"\n  else\n    echo \"‚úÖ All licenses appear compatible\"\n  fi\nfi\n"
            },
            {
              "name": "Upload license report",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "license-audit-report",
                "path": "reports/licenses.json\nreports/licenses.md\n",
                "retention-days": 30,
                "if-no-files-found": "ignore"
              }
            }
          ]
        },
        "security-audit": {
          "name": "Security Audit",
          "runs-on": "ubuntu-latest",
          "needs": "determine-tasks",
          "if": "contains(needs.determine-tasks.outputs.tasks, 'audit')",
          "timeout-minutes": 10,
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "20",
                "cache-key-suffix": "housekeeping-security"
              }
            },
            {
              "name": "Run security audit",
              "run": "echo \"üîí Running security audit...\"\n\n# Run npm audit\nnpm audit --audit-level=moderate > security-audit.txt 2>&1 || echo \"Audit completed with findings\"\n\n# Count vulnerabilities\nHIGH_VULNS=$(grep -c \"high\" security-audit.txt || echo \"0\")\nCRITICAL_VULNS=$(grep -c \"critical\" security-audit.txt || echo \"0\")\n\necho \"Found $CRITICAL_VULNS critical and $HIGH_VULNS high severity vulnerabilities\"\n\nif [ \"$CRITICAL_VULNS\" -gt 0 ]; then\n  echo \"‚ùå Critical vulnerabilities detected - immediate action required\"\n  exit 1\nelif [ \"$HIGH_VULNS\" -gt 0 ]; then\n  echo \"‚ö†Ô∏è High severity vulnerabilities detected - review recommended\"\nelse\n  echo \"‚úÖ No critical or high severity vulnerabilities\"\nfi\n"
            },
            {
              "name": "Upload security audit report",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "security-audit-report",
                "path": "security-audit.txt",
                "retention-days": 14
              }
            }
          ]
        },
        "queue-monitor": {
          "name": "Queue Monitor",
          "runs-on": "ubuntu-latest",
          "needs": "determine-tasks",
          "if": "contains(needs.determine-tasks.outputs.tasks, 'queue-monitor')",
          "timeout-minutes": 5,
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "20",
                "cache-key-suffix": "housekeeping-queue"
              }
            },
            {
              "name": "Setup Supabase environment",
              "uses": "./.github/actions/setup-supabase-rest",
              "with": {
                "supabase-url": "${{ secrets.SUPABASE_URL }}",
                "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY_V2 != '' && secrets.SUPABASE_SERVICE_ROLE_KEY_V2 || secrets.SUPABASE_SERVICE_ROLE_KEY }}",
                "database-url": "${{ secrets.DATABASE_URL }}"
              }
            },
            {
              "name": "Monitor content queue health",
              "run": "echo \"üìä Monitoring content queue health...\"\n\nAUTH_TOKEN=\"${{ secrets.AUTH_TOKEN }}\"\n\n# Get queue metrics\nRESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n  \"${{ secrets.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/admin/metrics\" \\\n  --max-time 30)\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//')\n\nif [ \"$HTTP_STATUS\" -eq 200 ]; then\n  echo \"‚úÖ Queue monitoring completed\"\n  echo \"$BODY\" > queue-health.json\n  \n  # Extract key metrics if possible\n  if command -v jq >/dev/null 2>&1; then\n    TOTAL_CONTENT=$(echo \"$BODY\" | jq '.totalContent // 0' 2>/dev/null || echo \"unknown\")\n    APPROVED_CONTENT=$(echo \"$BODY\" | jq '.approvedContent // 0' 2>/dev/null || echo \"unknown\")\n    echo \"üìà Queue stats: $TOTAL_CONTENT total, $APPROVED_CONTENT approved\"\n  fi\nelse\n  echo \"‚ö†Ô∏è Queue monitoring failed with status $HTTP_STATUS\"\n  echo \"Response: $BODY\"\nfi\n"
            },
            {
              "name": "Upload queue health report",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "queue-health-report",
                "path": "queue-health.json",
                "retention-days": 7,
                "if-no-files-found": "ignore"
              }
            }
          ]
        },
        "secrets-validation": {
          "name": "Secrets Validation",
          "runs-on": "ubuntu-latest",
          "needs": "determine-tasks",
          "if": "contains(needs.determine-tasks.outputs.tasks, 'secrets')",
          "timeout-minutes": 8,
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "20",
                "cache-key-suffix": "housekeeping-secrets"
              }
            },
            {
              "name": "Validate secrets configuration",
              "run": "echo \"üîê Validating secrets configuration...\"\n\n# Check essential secrets\nMISSING_SECRETS=\"\"\n\nif [[ -z \"${{ secrets.SUPABASE_URL }}\" ]]; then\n  MISSING_SECRETS=\"$MISSING_SECRETS SUPABASE_URL\"\nfi\n\nif [[ -z \"${{ secrets.SUPABASE_SERVICE_ROLE_KEY_V2 != '' && secrets.SUPABASE_SERVICE_ROLE_KEY_V2 || secrets.SUPABASE_SERVICE_ROLE_KEY }}\" ]]; then\n  MISSING_SECRETS=\"$MISSING_SECRETS SUPABASE_SERVICE_ROLE_KEY\"\nfi\n\nif [[ -z \"${{ secrets.AUTH_TOKEN }}\" ]]; then\n  MISSING_SECRETS=\"$MISSING_SECRETS AUTH_TOKEN\"\nfi\n\nif [[ -z \"${{ secrets.DATABASE_URL }}\" ]]; then\n  MISSING_SECRETS=\"$MISSING_SECRETS DATABASE_URL\"\nfi\n\n# Report results\nif [[ -n \"$MISSING_SECRETS\" ]]; then\n  echo \"‚ùå Missing essential secrets:$MISSING_SECRETS\"\n  echo \"missing-secrets=$MISSING_SECRETS\" >> $GITHUB_OUTPUT\n  exit 1\nelse\n  echo \"‚úÖ All essential secrets are configured\"\nfi\n\n# Check API keys\nAPI_KEYS=\"\"\nif [[ -n \"${{ secrets.REDDIT_CLIENT_ID }}\" ]]; then\n  API_KEYS=\"$API_KEYS reddit\"\nfi\nif [[ -n \"${{ secrets.YOUTUBE_API_KEY }}\" ]]; then\n  API_KEYS=\"$API_KEYS youtube\"\nfi\nif [[ -n \"${{ secrets.GIPHY_API_KEY }}\" ]]; then\n  API_KEYS=\"$API_KEYS giphy\"\nfi\n\necho \"üìä Configured API integrations:$API_KEYS\"\n"
            }
          ]
        },
        "summary": {
          "name": "Housekeeping Summary",
          "runs-on": "ubuntu-latest",
          "needs": [
            "determine-tasks",
            "cleanup-duplicates",
            "dead-links-check",
            "license-audit",
            "security-audit",
            "queue-monitor",
            "secrets-validation"
          ],
          "if": "always()",
          "steps": [
            {
              "name": "Generate housekeeping summary",
              "run": "echo \"## üè† Housekeeping Results\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Tasks:** ${{ needs.determine-tasks.outputs.tasks }}\" >> $GITHUB_STEP_SUMMARY\necho \"**Schedule Type:** ${{ needs.determine-tasks.outputs.schedule-type }}\" >> $GITHUB_STEP_SUMMARY\necho \"**Force Cleanup:** ${{ needs.determine-tasks.outputs.force-cleanup }}\" >> $GITHUB_STEP_SUMMARY\necho \"**Trigger:** ${{ github.event_name }}\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"| Task | Status | Notes |\" >> $GITHUB_STEP_SUMMARY\necho \"|------|--------|-------|\" >> $GITHUB_STEP_SUMMARY\n\n# Add task statuses\nif [[ \"${{ needs.cleanup-duplicates.result }}\" != \"\" ]]; then\n  echo \"| Cleanup Duplicates | ${{ needs.cleanup-duplicates.result }} | Content deduplication |\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [[ \"${{ needs.dead-links-check.result }}\" != \"\" ]]; then\n  echo \"| Dead Links Check | ${{ needs.dead-links-check.result }} | URL validation |\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [[ \"${{ needs.license-audit.result }}\" != \"\" ]]; then\n  echo \"| License Audit | ${{ needs.license-audit.result }} | Dependency licensing |\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [[ \"${{ needs.security-audit.result }}\" != \"\" ]]; then\n  echo \"| Security Audit | ${{ needs.security-audit.result }} | Vulnerability scanning |\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [[ \"${{ needs.queue-monitor.result }}\" != \"\" ]]; then\n  echo \"| Queue Monitor | ${{ needs.queue-monitor.result }} | Content queue health |\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [[ \"${{ needs.secrets-validation.result }}\" != \"\" ]]; then\n  echo \"| Secrets Validation | ${{ needs.secrets-validation.result }} | Configuration check |\" >> $GITHUB_STEP_SUMMARY\nfi\n\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Overall status\nFAILED_TASKS=0\n\nif [[ \"${{ needs.cleanup-duplicates.result }}\" == \"failure\" ]]; then\n  ((FAILED_TASKS++))\nfi\nif [[ \"${{ needs.dead-links-check.result }}\" == \"failure\" ]]; then\n  ((FAILED_TASKS++))\nfi\nif [[ \"${{ needs.license-audit.result }}\" == \"failure\" ]]; then\n  ((FAILED_TASKS++))\nfi\nif [[ \"${{ needs.security-audit.result }}\" == \"failure\" ]]; then\n  ((FAILED_TASKS++))\nfi\nif [[ \"${{ needs.queue-monitor.result }}\" == \"failure\" ]]; then\n  ((FAILED_TASKS++))\nfi\nif [[ \"${{ needs.secrets-validation.result }}\" == \"failure\" ]]; then\n  ((FAILED_TASKS++))\nfi\n\nif [[ $FAILED_TASKS -eq 0 ]]; then\n  echo \"## ‚úÖ All housekeeping tasks completed successfully\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"## ‚ö†Ô∏è $FAILED_TASKS housekeeping task(s) failed\" >> $GITHUB_STEP_SUMMARY\n  echo \"Check individual task logs for details.\" >> $GITHUB_STEP_SUMMARY\nfi\n\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"üìà **Next Steps:**\" >> $GITHUB_STEP_SUMMARY\necho \"- Review artifact reports for detailed findings\" >> $GITHUB_STEP_SUMMARY\necho \"- Address any security or license issues found\" >> $GITHUB_STEP_SUMMARY\necho \"- Monitor content queue health metrics\" >> $GITHUB_STEP_SUMMARY\n"
            }
          ]
        }
      },
      "env": {
        "NODE_ENV": "production",
        "CI": true
      },
      "concurrency": {
        "group": "housekeeping-${{ github.ref }}",
        "cancel-in-progress": true
      },
      "secrets_refs": [
        "SUPABASE_URL",
        "SUPABASE_SERVICE_ROLE_KEY_V2",
        "DATABASE_URL",
        "AUTH_TOKEN",
        "SITE_URL",
        "REDDIT_CLIENT_ID",
        "YOUTUBE_API_KEY",
        "GIPHY_API_KEY"
      ],
      "vars_refs": [],
      "composite_actions": [
        "./.github/actions/setup-node",
        "./.github/actions/setup-supabase-rest"
      ],
      "reusable_workflows": [],
      "job_count": 8
    },
    {
      "path": ".github/workflows/manual-operations.yml",
      "filename": "manual-operations.yml",
      "name": "Manual Operations",
      "triggers": [
        {
          "event": "workflow_dispatch",
          "config": {
            "inputs": {
              "operation": {
                "description": "Operation to perform",
                "required": true,
                "type": "choice",
                "options": [
                  "post-now",
                  "scan-all-emergency",
                  "scan-all-normal",
                  "catch-up-missed-posts",
                  "approve-pending",
                  "system-health-check",
                  "clear-queue-test",
                  "fix-duplicates"
                ]
              },
              "count": {
                "description": "Number of times (for catch-up posts)",
                "required": false,
                "default": "1",
                "type": "string"
              },
              "platform": {
                "description": "Specific platform (for single platform scans)",
                "required": false,
                "type": "choice",
                "options": [
                  "all",
                  "reddit",
                  "youtube",
                  "giphy",
                  "pixabay",
                  "bluesky",
                  "imgur",
                  "lemmy",
                  "tumblr"
                ]
              }
            }
          }
        }
      ],
      "jobs": {
        "execute-operation": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Execute Operation",
              "run": "case \"${{ github.event.inputs.operation }}\" in\n  \"post-now\")\n    echo \"üöÄ Posting content immediately...\"\n    curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/posting/post-now\" \\\n      -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\"immediate\": true}' \\\n      --fail --show-error\n    ;;\n    \n  \"catch-up-missed-posts\")\n    echo \"üìÖ Catching up ${{ github.event.inputs.count }} missed posts...\"\n    for i in $(seq 1 ${{ github.event.inputs.count }}); do\n      echo \"  Posting #$i...\"\n      curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/posting/post-now\" \\\n        -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n        -H \"Content-Type: application/json\" \\\n        -d \"{\\\"catchUp\\\": true, \\\"sequence\\\": $i}\" \\\n        --fail --show-error\n      \n      if [ $i -lt ${{ github.event.inputs.count }} ]; then\n        echo \"  Waiting 10 seconds before next post...\"\n        sleep 10\n      fi\n    done\n    ;;\n    \n  \"scan-all-emergency\")\n    echo \"üö® EMERGENCY: Scanning all platforms with auto-approval...\"\n    curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/content/emergency-scan\" \\\n      -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\"autoApprove\": true, \"maxItems\": 100}' \\\n      --fail --show-error\n    ;;\n    \n  \"scan-all-normal\")\n    echo \"üì° Scanning all platforms (normal mode)...\"\n    curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/content/scan-all\" \\\n      -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\"maxItems\": 50}' \\\n      --fail --show-error\n    ;;\n    \n  \"approve-pending\")\n    echo \"‚úÖ Auto-approving pending content...\"\n    curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/content/auto-approve\" \\\n      -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\"maxItems\": 30, \"onlyHighConfidence\": true}' \\\n      --fail --show-error\n    ;;\n    \n  \"system-health-check\")\n    echo \"üè• Running system health check...\"\n    curl -L -X GET \"${{ secrets.SITE_URL }}/api/admin/health\" \\\n      -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n      --fail --show-error\n    ;;\n    \n  \"clear-queue-test\")\n    echo \"üßπ Clearing test/duplicate content...\"\n    curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/content/cleanup\" \\\n      -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\"removeTestContent\": true, \"removeDuplicates\": true}' \\\n      --fail --show-error\n    ;;\n    \n  \"fix-duplicates\")\n    echo \"üîß Fixing duplicate content to prevent reposting...\"\n    curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/fix-duplicate-content\" \\\n      -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n      -H \"Content-Type: application/json\" \\\n      --fail --show-error\n    ;;\n    \n  *)\n    echo \"‚ùå Unknown operation: ${{ github.event.inputs.operation }}\"\n    exit 1\n    ;;\nesac\n"
            },
            {
              "name": "Report Operation Results",
              "if": "always()",
              "run": "echo \"üìä Operation completed: ${{ github.event.inputs.operation }}\"\nif [ \"${{ github.event.inputs.count }}\" != \"1\" ]; then\n  echo \"Count: ${{ github.event.inputs.count }}\"\nfi\nif [ \"${{ github.event.inputs.platform }}\" != \"all\" ]; then\n  echo \"Platform: ${{ github.event.inputs.platform }}\"\nfi\n"
            }
          ]
        }
      },
      "secrets_refs": [
        "SITE_URL",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/meta-ci-audit.yml",
      "filename": "meta-ci-audit.yml",
      "name": "Meta CI Audit",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 8 * * 1"
            }
          ],
          "cron": [
            "0 8 * * 1"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": null
        }
      ],
      "jobs": {
        "audit-ci-health": {
          "name": "Audit CI Health",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 15,
          "steps": [
            {
              "name": "Checkout repository",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js",
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "20",
                "cache": "npm"
              }
            },
            {
              "name": "Install dependencies",
              "run": "npm ci"
            },
            {
              "name": "üß† Install tsx (temporary CI dependency)",
              "run": "npm install --no-save tsx"
            },
            {
              "name": "Run CI health audit",
              "run": "npx tsx scripts/validateRefactorPlan.ts",
              "id": "audit"
            },
            {
              "name": "Generate health report",
              "run": "npx tsx scripts/auditWorkflows.ts",
              "env": {
                "GH_TOKEN": "${{ secrets.GITHUB_TOKEN }}"
              }
            },
            {
              "name": "Notify on failure",
              "if": "failure()",
              "uses": "slackapi/slack-github-action@v1.27.0",
              "with": {
                "payload": "{\n  \"text\": \"üö® Weekly CI health audit failed\",\n  \"blocks\": [\n    {\n      \"type\": \"section\",\n      \"text\": {\n        \"type\": \"mrkdwn\",\n        \"text\": \"*CI Health Alert* üè•\\n\\nWeekly audit detected issues in workflow configuration.\\n\\n*Repository:* ${{ github.repository }}\\n*Run:* <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Details>\"\n      }\n    }\n  ]\n}\n"
              },
              "env": {
                "SLACK_WEBHOOK_URL": "${{ secrets.SLACK_WEBHOOK_URL }}"
              }
            },
            {
              "name": "Post success summary",
              "if": "success()",
              "uses": "slackapi/slack-github-action@v1.27.0",
              "with": {
                "payload": "{\n  \"text\": \"‚úÖ Weekly CI health audit passed - all workflows healthy!\"\n}\n"
              },
              "env": {
                "SLACK_WEBHOOK_URL": "${{ secrets.SLACK_WEBHOOK_URL }}"
              }
            }
          ]
        }
      },
      "secrets_refs": [
        "GITHUB_TOKEN",
        "SLACK_WEBHOOK_URL"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/phase3-auto-healing.yml",
      "filename": "phase3-auto-healing.yml",
      "name": "Phase 3 CI Auto-Healing: Security & Build Diagnostics",
      "triggers": [
        {
          "event": "workflow_call",
          "config": {
            "inputs": {
              "trigger_deep_remediation": {
                "description": "Force deep remediation even if not triggered by failures",
                "required": false,
                "type": "boolean",
                "default": false
              }
            },
            "outputs": {
              "remediation_applied": {
                "description": "Whether Phase 3 auto-healing was applied",
                "value": "${{ jobs.deep-remediation.outputs.remediation_applied }}"
              },
              "security_score_improvement": {
                "description": "Security score improvement from remediation",
                "value": "${{ jobs.deep-remediation.outputs.security_improvement }}"
              },
              "build_diagnostic_available": {
                "description": "Whether build diagnostics were generated",
                "value": "${{ jobs.deep-remediation.outputs.build_diagnostics }}"
              }
            }
          }
        },
        {
          "event": "workflow_dispatch",
          "config": {
            "inputs": {
              "force_aggressive_mode": {
                "description": "Use aggressive security remediation",
                "required": false,
                "type": "boolean",
                "default": false
              }
            }
          }
        }
      ],
      "jobs": {
        "deep-remediation": {
          "name": "Phase 3: Deep Security & Build Remediation",
          "runs-on": "ubuntu-latest",
          "if": "failure() || inputs.trigger_deep_remediation",
          "outputs": {
            "remediation_applied": "${{ steps.remediation-summary.outputs.applied }}",
            "security_improvement": "${{ steps.remediation-summary.outputs.security_score }}",
            "build_diagnostics": "${{ steps.remediation-summary.outputs.build_report }}"
          },
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4",
              "with": {
                "token": "${{ secrets.GITHUB_TOKEN }}",
                "fetch-depth": 1
              }
            },
            {
              "name": "Setup Node.js with caching",
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "18",
                "cache": "npm"
              }
            },
            {
              "name": "Install dependencies",
              "run": "npm ci --prefer-offline --no-audit --no-fund\nnpm install --no-save tsx\n"
            },
            {
              "name": "Phase 3.1 - Security Deep Remediation",
              "id": "security-remediation",
              "continue-on-error": true,
              "run": "echo \"üõ°Ô∏è Running Phase 3 Security Deep Remediation...\"\n\n# Set aggressive mode based on input or critical vulnerability count\nAGGRESSIVE_FLAG=\"\"\nif [[ \"${{ inputs.force_aggressive_mode }}\" == \"true\" ]]; then\n  AGGRESSIVE_FLAG=\"--aggressive\"\nfi\n\n# Run security deep fix\nnpx tsx scripts/securityDeepFix.ts $AGGRESSIVE_FLAG || true\n\n# Check if remediation was effective\nif [[ -f \"reports/security-deep-fix.md\" ]]; then\n  echo \"security_report_generated=true\" >> $GITHUB_OUTPUT\n  \n  # Extract effectiveness score if available\n  if grep -q \"Effectiveness Score:\" reports/security-deep-fix.md; then\n    EFFECTIVENESS=$(grep \"Effectiveness Score:\" reports/security-deep-fix.md | head -1 | grep -oE '[0-9]+' || echo \"0\")\n    echo \"effectiveness_score=$EFFECTIVENESS\" >> $GITHUB_OUTPUT\n  fi\nfi\n"
            },
            {
              "name": "Phase 3.2 - Build Failure Diagnostics",
              "id": "build-diagnostics",
              "continue-on-error": true,
              "run": "echo \"üèóÔ∏è Running Phase 3 Build Failure Diagnostics...\"\n\n# Run build diagnostics with comprehensive logging\nnpx tsx scripts/analyzeBuildFailure.ts --verbose --save-logs || true\n\n# Check if diagnostics were generated\nif [[ -f \"reports/build-diagnostics.md\" ]]; then\n  echo \"build_report_generated=true\" >> $GITHUB_OUTPUT\n  \n  # Check if quick fixes were identified\n  if grep -q \"Quick Fixes\" reports/build-diagnostics.md; then\n    QUICK_FIXES=$(grep -c \"Quick Fixes\" reports/build-diagnostics.md || echo \"0\")\n    echo \"quick_fixes_available=$QUICK_FIXES\" >> $GITHUB_OUTPUT\n  fi\nfi\n"
            },
            {
              "name": "Phase 3.3 - Apply Quick Fixes",
              "id": "apply-quick-fixes",
              "if": "steps.build-diagnostics.outputs.quick_fixes_available > 0",
              "continue-on-error": true,
              "run": "echo \"‚ö° Applying automated quick fixes...\"\n\n# Clear Next.js cache if build issues detected\nif [[ -d \".next\" ]]; then\n  echo \"üßπ Clearing Next.js cache...\"\n  rm -rf .next\n  echo \"cache_cleared=true\" >> $GITHUB_OUTPUT\nfi\n\n# Reinstall dependencies if dependency issues detected\nif grep -q \"dependency\" reports/build-diagnostics.md; then\n  echo \"üì¶ Reinstalling dependencies...\"\n  rm -f package-lock.json\n  npm install\n  echo \"dependencies_reinstalled=true\" >> $GITHUB_OUTPUT\nfi\n"
            },
            {
              "name": "Phase 3.4 - Re-run Critical Checks",
              "id": "post-remediation-check",
              "continue-on-error": true,
              "run": "echo \"üîÑ Re-running critical checks after remediation...\"\n\n# Re-run the main gatekeeper to measure improvement\nnpx tsx scripts/checkCriticalFailures.ts --report-only || true\n\n# Extract final health score\nif [[ -f \"reports/ci-health-gate.md\" ]]; then\n  if grep -q \"Confidence Score:\" reports/ci-health-gate.md; then\n    FINAL_SCORE=$(grep \"Confidence Score:\" reports/ci-health-gate.md | head -1 | grep -oE '[0-9]+' || echo \"0\")\n    echo \"final_health_score=$FINAL_SCORE\" >> $GITHUB_OUTPUT\n  fi\nfi\n"
            },
            {
              "name": "Summarize Remediation Results",
              "id": "remediation-summary",
              "run": "echo \"üìä Summarizing Phase 3 Auto-Healing Results...\"\n\nAPPLIED=\"false\"\nSECURITY_SCORE=\"0\"\nBUILD_REPORT=\"false\"\n\n# Check if security remediation was applied\nif [[ \"${{ steps.security-remediation.outputs.security_report_generated }}\" == \"true\" ]]; then\n  APPLIED=\"true\"\n  SECURITY_SCORE=\"${{ steps.security-remediation.outputs.effectiveness_score }}\"\nfi\n\n# Check if build diagnostics were generated\nif [[ \"${{ steps.build-diagnostics.outputs.build_report_generated }}\" == \"true\" ]]; then\n  BUILD_REPORT=\"true\"\nfi\n\necho \"applied=$APPLIED\" >> $GITHUB_OUTPUT\necho \"security_score=$SECURITY_SCORE\" >> $GITHUB_OUTPUT\necho \"build_report=$BUILD_REPORT\" >> $GITHUB_OUTPUT\n\n# Create summary for GitHub\ncat >> $GITHUB_STEP_SUMMARY << 'EOF'\n## üõ°Ô∏è Phase 3 Auto-Healing Summary\n\n### Security Deep Remediation\n- **Applied:** ${{ steps.security-remediation.outputs.security_report_generated == 'true' && '‚úÖ Yes' || '‚ùå No' }}\n- **Effectiveness Score:** ${{ steps.security-remediation.outputs.effectiveness_score || 'N/A' }}/100\n\n### Build Diagnostics\n- **Generated:** ${{ steps.build-diagnostics.outputs.build_report_generated == 'true' && '‚úÖ Yes' || '‚ùå No' }}\n- **Quick Fixes Available:** ${{ steps.build-diagnostics.outputs.quick_fixes_available || '0' }}\n\n### Quick Fixes Applied\n- **Cache Cleared:** ${{ steps.apply-quick-fixes.outputs.cache_cleared == 'true' && '‚úÖ Yes' || '‚ùå No' }}\n- **Dependencies Reinstalled:** ${{ steps.apply-quick-fixes.outputs.dependencies_reinstalled == 'true' && '‚úÖ Yes' || '‚ùå No' }}\n\n### Final Health Score\n- **Post-Remediation Score:** ${{ steps.post-remediation-check.outputs.final_health_score || 'N/A' }}/100\n\n### üìã Generated Reports\n- Security Deep Fix: `reports/security-deep-fix.md`\n- Build Diagnostics: `reports/build-diagnostics.md`\n- CI Health Gate: `reports/ci-health-gate.md`\nEOF\n"
            },
            {
              "name": "Upload Remediation Reports",
              "uses": "actions/upload-artifact@v4",
              "if": "always()",
              "with": {
                "name": "phase3-auto-healing-reports",
                "path": "reports/security-deep-fix.md\nreports/build-diagnostics.md\nreports/security-manual-review.md\nreports/build-log.txt\n",
                "retention-days": 30
              }
            },
            {
              "name": "Comment on PR with Remediation Results",
              "if": "github.event_name == 'pull_request' && always()",
              "uses": "actions/github-script@v7",
              "with": {
                "script": "const fs = require('fs');\nconst securityReportExists = fs.existsSync('reports/security-deep-fix.md');\nconst buildReportExists = fs.existsSync('reports/build-diagnostics.md');\n\nlet comment = '## üõ°Ô∏è Phase 3 Auto-Healing Results\\n\\n';\n\nif (securityReportExists || buildReportExists) {\n  comment += '### üìä Remediation Summary\\n';\n  comment += `- **Security Remediation:** ${securityReportExists ? '‚úÖ Applied' : '‚ùå Not needed'}\\n`;\n  comment += `- **Build Diagnostics:** ${buildReportExists ? '‚úÖ Generated' : '‚ùå Not needed'}\\n`;\n  comment += `- **Effectiveness Score:** ${{ steps.security-remediation.outputs.effectiveness_score || 'N/A' }}/100\\n\\n`;\n  \n  comment += '### üìã Available Reports\\n';\n  if (securityReportExists) comment += '- üõ°Ô∏è Security Deep Fix Report\\n';\n  if (buildReportExists) comment += '- üèóÔ∏è Build Diagnostics Report\\n';\n  comment += '\\n> Reports are available in the workflow artifacts.\\n';\n} else {\n  comment += '‚úÖ **No deep remediation required** - Initial CI checks were sufficient.\\n';\n}\n\ngithub.rest.issues.createComment({\n  issue_number: context.issue.number,\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  body: comment\n});\n"
              }
            }
          ]
        },
        "trigger-recheck": {
          "name": "Secure Re-Dispatch & Post-Remediation Check",
          "runs-on": "ubuntu-latest",
          "needs": "deep-remediation",
          "if": "needs.deep-remediation.outputs.remediation_applied == 'true'",
          "steps": [
            {
              "name": "Checkout code for re-dispatch",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "üîÑ Trigger Post-Remediation CI Run",
              "env": {
                "GH_TOKEN": "${{ secrets.CI_REDISPATCH_TOKEN }}"
              },
              "run": "echo \"üöÄ Triggering repository_dispatch for post-remediation CI...\"\n\n# Verify token is available\nif [ -z \"$GH_TOKEN\" ]; then\n  echo \"‚ùå CI_REDISPATCH_TOKEN secret not found\"\n  echo \"üìã Please follow docs/PHASE4_SECURE_TOKEN_SETUP.md to configure the token\"\n  exit 1\nfi\n\n# Prepare client payload\nSECURITY_IMPROVEMENT=\"${{ needs.deep-remediation.outputs.security_improvement }}\"\nBUILD_DIAGNOSTICS=\"${{ needs.deep-remediation.outputs.build_diagnostic_available }}\"\n\n# Create dispatch event with secure token\nHTTP_STATUS=$(curl -s -o /dev/null -w \"%{http_code}\" -X POST \\\n  -H \"Accept: application/vnd.github.v3+json\" \\\n  -H \"Authorization: Bearer $GH_TOKEN\" \\\n  https://api.github.com/repos/${{ github.repository }}/dispatches \\\n  -d \"{\n    \\\"event_type\\\": \\\"post-remediation-check\\\",\n    \\\"client_payload\\\": {\n      \\\"security_improvement\\\": \\\"$SECURITY_IMPROVEMENT\\\",\n      \\\"build_diagnostics\\\": \\\"$BUILD_DIAGNOSTICS\\\",\n      \\\"triggered_by\\\": \\\"phase3-auto-healing\\\",\n      \\\"commit_sha\\\": \\\"${{ github.sha }}\\\",\n      \\\"remediation_timestamp\\\": \\\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\\\"\n    }\n  }\")\n\n# Check response status\ncase $HTTP_STATUS in\n  200|201|204)\n    echo \"‚úÖ Repository dispatch successful (HTTP $HTTP_STATUS)\"\n    echo \"üîÑ Post-remediation validation workflow should start shortly\"\n    ;;\n  401)\n    echo \"‚ùå Authentication failed (HTTP 401)\"\n    echo \"üîß Check CI_REDISPATCH_TOKEN secret configuration\"\n    exit 1\n    ;;\n  403)\n    echo \"‚ùå Forbidden (HTTP 403)\"\n    echo \"üîß Token may lack required permissions (repo, workflow scopes needed)\"\n    exit 1\n    ;;\n  404)\n    echo \"‚ùå Repository not found (HTTP 404)\"\n    echo \"üîß Check repository path or token repository access\"\n    exit 1\n    ;;\n  *)\n    echo \"‚ùå Unexpected response (HTTP $HTTP_STATUS)\"\n    echo \"üîß Check GitHub API status and token configuration\"\n    exit 1\n    ;;\nesac\n"
            },
            {
              "name": "üìä Record Re-Dispatch Metrics",
              "run": "echo \"üìà Phase 4 Re-Dispatch Metrics:\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"- **Trigger Time:** $(date -u +%Y-%m-%dT%H:%M:%SZ)\" >> $GITHUB_STEP_SUMMARY\necho \"- **Security Improvement:** ${{ needs.deep-remediation.outputs.security_improvement }}/100\" >> $GITHUB_STEP_SUMMARY\necho \"- **Build Diagnostics:** ${{ needs.deep-remediation.outputs.build_diagnostic_available == 'true' && 'Available' || 'Not Generated' }}\" >> $GITHUB_STEP_SUMMARY\necho \"- **Remediation Applied:** ‚úÖ Yes\" >> $GITHUB_STEP_SUMMARY\necho \"- **Post-Validation:** üîÑ Triggered\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"> Post-remediation validation will run independently to verify fixes.\" >> $GITHUB_STEP_SUMMARY\n"
            },
            {
              "name": "üè∑Ô∏è Add Remediation Success Labels",
              "if": "needs.deep-remediation.outputs.security_improvement >= 70 && github.event_name == 'pull_request'",
              "env": {
                "GH_TOKEN": "${{ secrets.CI_REDISPATCH_TOKEN }}"
              },
              "run": "echo \"üè∑Ô∏è Adding success labels to PR...\"\n\n# Add labels indicating successful auto-healing\ncurl -X POST \\\n  -H \"Accept: application/vnd.github.v3+json\" \\\n  -H \"Authorization: Bearer $GH_TOKEN\" \\\n  https://api.github.com/repos/${{ github.repository }}/issues/${{ github.event.pull_request.number }}/labels \\\n  -d '{\"labels\": [\"auto-healing-applied\", \"security-improved\", \"phase3-remediation\"]}'\n\necho \"‚úÖ Labels added successfully\"\n"
            },
            {
              "name": "üìù Update PR with Remediation Summary",
              "if": "github.event_name == 'pull_request'",
              "env": {
                "GH_TOKEN": "${{ secrets.CI_REDISPATCH_TOKEN }}"
              },
              "run": "echo \"üìù Posting remediation summary to PR...\"\n\n# Create detailed PR comment\ncat > pr_comment.md << 'EOF'\n## üõ°Ô∏è Phase 3 Auto-Healing Complete\n\n### üìä Remediation Results\n- **Security Score Improvement:** ${{ needs.deep-remediation.outputs.security_improvement }}/100\n- **Build Diagnostics:** ${{ needs.deep-remediation.outputs.build_diagnostic_available == 'true' && '‚úÖ Generated' || '‚ùå Not needed' }}\n- **Automated Fixes Applied:** ‚úÖ Yes\n\n### üîÑ Next Steps\n1. **Post-Remediation Validation:** Automatically triggered\n2. **Review Reports:** Check workflow artifacts for detailed analysis\n3. **Manual Review:** Address any remaining issues in security reports\n\n### üìã Available Reports\n- üõ°Ô∏è Security Deep Fix Report\n- üèóÔ∏è Build Diagnostics Report\n- üìä Updated CI Health Gate Report\n\n> **Note:** This comment was generated by Phase 3 Auto-Healing system. \n> Post-remediation validation is running independently.\nEOF\n\n# Post comment to PR\ncurl -X POST \\\n  -H \"Accept: application/vnd.github.v3+json\" \\\n  -H \"Authorization: Bearer $GH_TOKEN\" \\\n  https://api.github.com/repos/${{ github.repository }}/issues/${{ github.event.pull_request.number }}/comments \\\n  -d \"{\\\"body\\\": $(cat pr_comment.md | jq -Rs .)}\"\n\necho \"‚úÖ PR comment posted successfully\"\n"
            },
            {
              "name": "üîç Verify Re-Dispatch Success",
              "run": "echo \"üîç Verifying Phase 4 re-dispatch completed successfully...\"\necho \"\"\necho \"‚úÖ Repository dispatch event sent\"\necho \"‚úÖ Remediation metrics recorded\"\necho \"‚úÖ PR labels and comments updated (if applicable)\"\necho \"\"\necho \"üéØ Phase 4 Secure Re-Dispatch System: OPERATIONAL\"\necho \"\"\necho \"üìã Post-remediation validation should begin within 1-2 minutes\"\necho \"üìä Check the 'Actions' tab for 'Post-Remediation Check' workflow\"\n"
            }
          ]
        }
      },
      "env": {
        "NODE_OPTIONS": "--max-old-space-size=4096"
      },
      "secrets_refs": [
        "GITHUB_TOKEN",
        "CI_REDISPATCH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 2
    },
    {
      "path": ".github/workflows/planner-contract.yml",
      "filename": "planner-contract.yml",
      "name": "üìã Planner Contract",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "30 3 * * *"
            }
          ],
          "cron": [
            "30 3 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": {
            "inputs": {
              "test_scenarios": {
                "description": "Test scenarios to run (all,basic,edge,stress)",
                "type": "choice",
                "default": "all",
                "options": [
                  "all",
                  "basic",
                  "edge",
                  "stress"
                ]
              },
              "fail_fast": {
                "description": "Stop on first test failure",
                "type": "boolean",
                "default": false
              }
            }
          }
        }
      ],
      "jobs": {
        "planner-contract": {
          "name": "üìã Validate Planner Behavior",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 10,
          "permissions": {
            "contents": "read",
            "issues": "write"
          },
          "steps": [
            {
              "name": "üì• Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "üîß Setup Node.js",
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": 20,
                "cache": "npm"
              }
            },
            {
              "name": "üì¶ Install dependencies",
              "run": "echo \"üîß Installing test dependencies...\"\nnpm ci --prefer-offline --no-audit\n"
            },
            {
              "name": "üß™ Run planner unit tests",
              "id": "unit-tests",
              "run": "echo \"üß™ Running comprehensive planner unit tests...\"\n\n# Set test configuration\nexport SCAN_MIN_PER_PLATFORM=40\nexport SCAN_MAX_PER_PLATFORM=120\nexport SCAN_GLOBAL_MAX=800\nexport SCAN_COOLDOWN_MIN=180\nexport MIN_CONF=0.70\nexport MIN_CANDIDATES=20\nexport PLATFORM_ALLOW=\"reddit,youtube,giphy,imgur,bluesky,tumblr,lemmy,pixabay\"\n\n# Run Node.js tests with timeout and detailed output\ntimeout 300s node --test --test-reporter=spec scripts/scan-plan.test.mjs > test_output.txt 2>&1\nTEST_EXIT_CODE=$?\n\necho \"TEST_EXIT_CODE=$TEST_EXIT_CODE\" >> $GITHUB_ENV\n\n# Display results\ncat test_output.txt\n\nif [ $TEST_EXIT_CODE -eq 0 ]; then\n  echo \"‚úÖ All planner unit tests passed\"\n  echo \"UNIT_TESTS_STATUS=passed\" >> $GITHUB_ENV\nelse\n  echo \"‚ùå Planner unit tests failed with exit code: $TEST_EXIT_CODE\"\n  echo \"UNIT_TESTS_STATUS=failed\" >> $GITHUB_ENV\nfi\n"
            },
            {
              "name": "üîç Contract validation scenarios",
              "id": "contract-validation",
              "if": "inputs.test_scenarios == 'all' || inputs.test_scenarios == 'edge' || github.event_name == 'schedule'",
              "run": "echo \"üîç Running contract validation scenarios...\"\n\n# Test 1: Global cap enforcement\necho \"üìä Testing global cap enforcement...\"\nnode -e \"\nimport { spawn } from 'node:child_process';\nimport { writeFile } from 'node:fs/promises';\n\nconst wrapper = \\`\n  globalThis.fetch = async (url) => {\n    if (url.includes('/api/system/metrics')) {\n      return new Response(JSON.stringify({\n        queue_depth_by_platform: { reddit: 500, youtube: 400 },\n        last_scan_times: {}\n      }), { status: 200, headers: {'content-type':'application/json'}});\n    }\n    return new Response('[]', { status: 200, headers: {'content-type':'application/json'}});\n  };\n  import('./scripts/scan-plan.mjs').catch(e => { console.error(e); process.exit(1); });\n\\`;\n\nawait writeFile('__tmp_global_cap_test.mjs', wrapper);\n\nconst proc = spawn('node', ['__tmp_global_cap_test.mjs'], {\n  env: {\n    ...process.env,\n    TARGET_URL: 'https://example.com',\n    SUPABASE_URL: 'https://supabase.local',\n    SUPABASE_SERVICE_ROLE_KEY: 'test',\n    SCAN_GLOBAL_MAX: '800'\n  }\n});\n\nlet stdout = '';\nproc.stdout.on('data', d => stdout += d.toString());\n\nconst code = await new Promise(res => proc.on('close', res));\n\nif (stdout.includes('global_cap_reached')) {\n  console.log('‚úÖ Global cap enforcement: PASSED');\n} else {\n  console.log('‚ùå Global cap enforcement: FAILED');\n  console.log('Output:', stdout);\n  process.exit(1);\n}\n\" || exit 1\n\necho \"CONTRACT_VALIDATION_STATUS=passed\" >> $GITHUB_ENV\n"
            },
            {
              "name": "üöÄ Stress test scenarios",
              "id": "stress-tests",
              "if": "inputs.test_scenarios == 'all' || inputs.test_scenarios == 'stress' || github.event_name == 'schedule'",
              "run": "echo \"üöÄ Running stress test scenarios...\"\n\n# Test with extreme platform counts\nnode -e \"\nimport { spawn } from 'node:child_process';\nimport { writeFile } from 'node:fs/promises';\n\n// Generate 50 platforms with various queue depths\nconst platforms = {};\nfor (let i = 0; i < 50; i++) {\n  platforms[\\`platform\\${i}\\`] = Math.floor(Math.random() * 200);\n}\n\nconst wrapper = \\`\n  globalThis.fetch = async (url) => {\n    if (url.includes('/api/system/metrics')) {\n      return new Response(JSON.stringify({\n        queue_depth_by_platform: ${JSON.stringify(platforms)},\n        last_scan_times: {}\n      }), { status: 200, headers: {'content-type':'application/json'}});\n    }\n    // Generate large supabase response\n    const rows = Array.from({length: 1000}, (_, i) => ({\n      source_platform: \\`platform\\${i % 50}\\`,\n      confidence_score: Math.random(),\n      ingest_priority: 0,\n      is_posted: false,\n      is_approved: true\n    }));\n    return new Response(JSON.stringify(rows), { status: 200, headers: {'content-type':'application/json'}});\n  };\n  import('./scripts/scan-plan.mjs').catch(e => { console.error(e); process.exit(1); });\n\\`;\n\nawait writeFile('__tmp_stress_test.mjs', wrapper);\n\nconst start = Date.now();\nconst proc = spawn('node', ['__tmp_stress_test.mjs'], {\n  env: {\n    ...process.env,\n    TARGET_URL: 'https://example.com',\n    SUPABASE_URL: 'https://supabase.local',  \n    SUPABASE_SERVICE_ROLE_KEY: 'test',\n    PLATFORM_ALLOW: Object.keys(platforms).join(',')\n  }\n});\n\nconst code = await new Promise(res => proc.on('close', res));\nconst duration = Date.now() - start;\n\nif (code === 0 && duration < 5000) {\n  console.log(\\`‚úÖ Stress test: PASSED (completed in \\${duration}ms)\\`);\n} else {\n  console.log(\\`‚ùå Stress test: FAILED (exit code: \\${code}, duration: \\${duration}ms)\\`);\n  process.exit(1);\n}\n\" || exit 1\n\necho \"STRESS_TESTS_STATUS=passed\" >> $GITHUB_ENV\n"
            },
            {
              "name": "üìä Generate contract summary",
              "if": "always()",
              "run": "echo \"## üìã Planner Contract Validation Results\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Date:** $(date -u -Iseconds)\" >> $GITHUB_STEP_SUMMARY\necho \"**Test Scenarios:** ${{ inputs.test_scenarios || 'all' }}\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Results table\necho \"| Test Category | Status | Notes |\" >> $GITHUB_STEP_SUMMARY\necho \"|---------------|--------|-------|\" >> $GITHUB_STEP_SUMMARY\n\nif [ \"${{ env.UNIT_TESTS_STATUS }}\" = \"passed\" ]; then\n  echo \"| Unit Tests | ‚úÖ Passed | All core logic validated |\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"| Unit Tests | ‚ùå Failed | Critical planner logic broken |\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [ \"${{ env.CONTRACT_VALIDATION_STATUS }}\" = \"passed\" ]; then\n  echo \"| Contract Validation | ‚úÖ Passed | Policy enforcement working |\" >> $GITHUB_STEP_SUMMARY\nelif [ \"${{ env.CONTRACT_VALIDATION_STATUS }}\" = \"\" ]; then\n  echo \"| Contract Validation | ‚è≠Ô∏è Skipped | Not run in this scenario |\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"| Contract Validation | ‚ùå Failed | Policy violations detected |\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [ \"${{ env.STRESS_TESTS_STATUS }}\" = \"passed\" ]; then\n  echo \"| Stress Tests | ‚úÖ Passed | Performance within limits |\" >> $GITHUB_STEP_SUMMARY\nelif [ \"${{ env.STRESS_TESTS_STATUS }}\" = \"\" ]; then\n  echo \"| Stress Tests | ‚è≠Ô∏è Skipped | Not run in this scenario |\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"| Stress Tests | ‚ùå Failed | Performance degradation |\" >> $GITHUB_STEP_SUMMARY\nfi\n\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Overall status\nif [ \"${{ env.UNIT_TESTS_STATUS }}\" = \"passed\" ] && \\\n   ( [ \"${{ env.CONTRACT_VALIDATION_STATUS }}\" = \"passed\" ] || [ \"${{ env.CONTRACT_VALIDATION_STATUS }}\" = \"\" ] ) && \\\n   ( [ \"${{ env.STRESS_TESTS_STATUS }}\" = \"passed\" ] || [ \"${{ env.STRESS_TESTS_STATUS }}\" = \"\" ] ); then\n  echo \"### üéâ Overall Status: **HEALTHY**\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"The demand-driven scanner planner is operating within contract specifications.\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"### üö® Overall Status: **CONTRACT VIOLATION**\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"Critical planner behavior violations detected. Immediate attention required.\" >> $GITHUB_STEP_SUMMARY\nfi\n"
            },
            {
              "name": "üßπ Cleanup test artifacts",
              "if": "always()",
              "run": "rm -f __tmp_*.mjs test_output.txt scan_plan.json scan_matrix.json\n"
            },
            {
              "name": "üì§ Upload test artifacts",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "planner-contract-test-results-${{ github.run_number }}",
                "path": "test_output.txt\nscan_plan.json\nscan_matrix.json\n",
                "retention-days": 7,
                "if-no-files-found": "ignore"
              }
            },
            {
              "name": "üö® Create issue on contract violation",
              "if": "failure() && github.event_name == 'schedule'",
              "uses": "actions/github-script@v7",
              "with": {
                "script": "const issueTitle = `üö® Planner Contract Violation - ${new Date().toISOString().split('T')[0]}`;\nconst issueBody = `# Planner Contract Violation Report\n\nThe daily planner contract validation detected critical violations on ${new Date().toISOString()}.\n\n**Workflow Run:** [#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})\n**Test Scenarios:** ${{ inputs.test_scenarios || 'all' }}\n\n## Failure Details\n\n- **Unit Tests:** ${{ env.UNIT_TESTS_STATUS || 'unknown' }}\n- **Contract Validation:** ${{ env.CONTRACT_VALIDATION_STATUS || 'unknown' }}\n- **Stress Tests:** ${{ env.STRESS_TESTS_STATUS || 'unknown' }}\n\n## Impact Assessment\n\nContract violations in the demand-driven scanner planner can lead to:\n- Inefficient resource allocation\n- Platform diversity degradation  \n- API quota exhaustion\n- Poor content quality selection\n- Scanner infinite loops or crashes\n\n## Investigation Steps\n\n1. üîç Download test artifacts: \\`planner-contract-test-results-${{ github.run_number }}\\`\n2. üìä Review test output for specific failures\n3. üß™ Run tests locally: \\`node --test scripts/scan-plan.test.mjs\\`\n4. üîß Check recent changes to \\`scripts/scan-plan.mjs\\`\n5. üìà Verify environment variables and thresholds\n\n## Common Contract Violations\n\n- **Global Cap Bypass:** Planner allows scanning when total queue > SCAN_GLOBAL_MAX\n- **Cooldown Ignored:** Recent scans not respected for cooldown period\n- **Quality Gates Broken:** Low-confidence content allowed when high-quality alternatives exist\n- **Performance Regression:** Planning takes >5 seconds or excessive memory\n- **Logic Errors:** Incorrect deficit calculations or matrix generation\n\n## Immediate Actions\n\n- [ ] Stop all scheduled scanning workflows\n- [ ] Review and fix planner logic\n- [ ] Run manual validation: \\`npm run test:planner\\`\n- [ ] Deploy fix and verify contract compliance\n- [ ] Re-enable scheduled scans\n- [ ] Close this issue when resolved\n\n---\n\nThis issue was automatically created by the Planner Contract workflow.\n`;\n\nawait github.rest.issues.create({\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  title: issueTitle,\n  body: issueBody,\n  labels: ['critical', 'scanner', 'contract-violation', 'auto-created']\n});\n"
              }
            }
          ]
        }
      },
      "env": {
        "NODE_ENV": "test",
        "CI": true
      },
      "concurrency": {
        "group": "planner-contract-${{ github.ref }}",
        "cancel-in-progress": true
      },
      "secrets_refs": [],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/post-breakfast.yml",
      "filename": "post-breakfast.yml",
      "name": "Post Breakfast Content",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 7 * * *"
            }
          ],
          "cron": [
            "0 7 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": null
        }
      ],
      "jobs": {
        "refresh-token": {
          "uses": "./.github/workflows/token-refresh.yml",
          "secrets": {
            "SITE_URL": "${{ secrets.SITE_URL }}",
            "SERVICE_ACCOUNT_SECRET": "${{ secrets.SERVICE_ACCOUNT_SECRET }}",
            "REFRESH_TOKEN": "${{ secrets.REFRESH_TOKEN }}",
            "AUTH_TOKEN": "${{ secrets.AUTH_TOKEN }}"
          },
          "needs": "guard",
          "timeout-minutes": 10,
          "concurrency": {
            "group": "refresh-token-${{ github.ref }}",
            "cancel-in-progress": true
          }
        },
        "post-breakfast": {
          "needs": "refresh-token",
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Post Breakfast Content",
              "env": {
                "AUTH_TOKEN": "${{ needs.refresh-token.outputs.auth_token }}"
              },
              "run": "echo \"üåÖ Posting breakfast content...\"\n\n# Store response in variable for error analysis\nRESPONSE=$(curl -s -w \"HTTP_CODE:%{http_code}\" -L -X POST \"${{ secrets.SITE_URL }}/api/admin/posting/post-now\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"immediate\": true}' \\\n  --retry 2 --retry-delay 10 2>&1 || true)\n\n# Extract HTTP code and response body\nHTTP_CODE=$(echo \"$RESPONSE\" | grep -o 'HTTP_CODE:[0-9]*' | cut -d':' -f2)\nRESPONSE_BODY=$(echo \"$RESPONSE\" | sed 's/HTTP_CODE:[0-9]*$//')\n\necho \"HTTP Status: $HTTP_CODE\"\necho \"Response: $RESPONSE_BODY\"\n\n# Check for specific error conditions\nif [[ \"$HTTP_CODE\" == \"401\" ]]; then\n  echo \"‚ùå CRITICAL: Authentication failed - AUTH_TOKEN is invalid or expired\"\n  echo \"This indicates a security breach recovery issue - AUTH_TOKEN needs to be updated in GitHub secrets\"\n  exit 1\nelif [[ \"$HTTP_CODE\" != \"200\" ]]; then\n  echo \"‚ùå FAILED: API returned HTTP $HTTP_CODE\"\n  echo \"Response body: $RESPONSE_BODY\"\n  exit 1\nelse\n  echo \"‚úÖ SUCCESS: Breakfast content posted (HTTP $HTTP_CODE)\"\nfi\n"
            },
            {
              "name": "Verify Post Success",
              "if": "success()",
              "run": "echo \"‚úÖ Breakfast content posted successfully\"\n# Optional: Send success notification\n"
            },
            {
              "name": "Handle Failure",
              "if": "failure()",
              "run": "echo \"‚ùå CRITICAL: Breakfast posting failed - this may indicate:\"\necho \"  1. AUTH_TOKEN is invalid/expired (most common after security breach)\"\necho \"  2. API endpoint is down\"\necho \"  3. No approved content available\"\necho \"  4. Database connectivity issues\"\necho \"\"\necho \"üîß TROUBLESHOOTING STEPS:\"\necho \"  1. Check AUTH_TOKEN in GitHub secrets matches current production token\"\necho \"  2. Verify Vercel deployment is live\"\necho \"  3. Check content queue has approved posts available\"\necho \"  4. Review API logs in Vercel dashboard\"\necho \"\"\necho \"üìû IMMEDIATE ACTIONS REQUIRED:\"\necho \"  - Update AUTH_TOKEN in GitHub secrets if authentication failed\"\necho \"  - Run content approval if queue is empty\"\necho \"  - Check Vercel deployment status\"\n"
            }
          ]
        },
        "guard": {
          "uses": "./.github/workflows/_posting-guard.yml"
        }
      },
      "permissions": {
        "contents": "read"
      },
      "secrets_refs": [
        "SITE_URL",
        "SERVICE_ACCOUNT_SECRET",
        "REFRESH_TOKEN",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [
        "./.github/workflows/token-refresh.yml",
        "./.github/workflows/_posting-guard.yml"
      ],
      "job_count": 3
    },
    {
      "path": ".github/workflows/post-deploy-check.yml",
      "filename": "post-deploy-check.yml",
      "name": "ERROR",
      "triggers": [],
      "jobs": {},
      "secrets_refs": [],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 0,
      "error": "duplicated mapping key (330:5)\n\n 327 |             echo '{\"error\": \"metric ...\n 328 |             echo \"snapshot=failed\"  ...\n 329 |           fi\n 330 |     needs: guard\n-----------^\n 331 |     concurrency:\n 332 |       group: health-check-${{ githu ..."
    },
    {
      "path": ".github/workflows/post-dinner.yml",
      "filename": "post-dinner.yml",
      "name": "Post Dinner Content",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 18 * * *"
            }
          ],
          "cron": [
            "0 18 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": null
        }
      ],
      "jobs": {
        "post-dinner": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Post Dinner Content",
              "run": "echo \"üçΩÔ∏è Posting dinner content...\"\n\n# Store response in variable for error analysis\nRESPONSE=$(curl -s -w \"HTTP_CODE:%{http_code}\" -L -X POST \"${{ secrets.SITE_URL }}/api/admin/posting/post-now\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"immediate\": true}' \\\n  --retry 2 --retry-delay 10 2>&1 || true)\n\n# Extract HTTP code and response body\nHTTP_CODE=$(echo \"$RESPONSE\" | grep -o 'HTTP_CODE:[0-9]*' | cut -d':' -f2)\nRESPONSE_BODY=$(echo \"$RESPONSE\" | sed 's/HTTP_CODE:[0-9]*$//')\n\necho \"HTTP Status: $HTTP_CODE\"\necho \"Response: $RESPONSE_BODY\"\n\n# Check for specific error conditions\nif [[ \"$HTTP_CODE\" == \"401\" ]]; then\n  echo \"‚ùå CRITICAL: Authentication failed - AUTH_TOKEN is invalid or expired\"\n  echo \"This indicates a security breach recovery issue - AUTH_TOKEN needs to be updated in GitHub secrets\"\n  exit 1\nelif [[ \"$HTTP_CODE\" != \"200\" ]]; then\n  echo \"‚ùå FAILED: API returned HTTP $HTTP_CODE\"\n  echo \"Response body: $RESPONSE_BODY\"\n  exit 1\nelse\n  echo \"‚úÖ SUCCESS: Dinner content posted (HTTP $HTTP_CODE)\"\nfi\n"
            },
            {
              "name": "Verify Post Success",
              "if": "success()",
              "run": "echo \"‚úÖ Dinner content posted successfully\"\n"
            },
            {
              "name": "Handle Failure",
              "if": "failure()",
              "run": "echo \"‚ùå CRITICAL: Dinner posting failed - this may indicate:\"\necho \"  1. AUTH_TOKEN is invalid/expired (most common after security breach)\"\necho \"  2. API endpoint is down\"\necho \"  3. No approved content available\"\necho \"  4. Database connectivity issues\"\necho \"\"\necho \"üîß TROUBLESHOOTING STEPS:\"\necho \"  1. Check AUTH_TOKEN in GitHub secrets matches current production token\"\necho \"  2. Verify Vercel deployment is live\"\necho \"  3. Check content queue has approved posts available\"\necho \"  4. Review API logs in Vercel dashboard\"\necho \"\"\necho \"üìû IMMEDIATE ACTIONS REQUIRED:\"\necho \"  - Update AUTH_TOKEN in GitHub secrets if authentication failed\"\necho \"  - Run content approval if queue is empty\"\necho \"  - Check Vercel deployment status\"\n"
            }
          ],
          "needs": "guard",
          "timeout-minutes": 10,
          "concurrency": {
            "group": "post-dinner-${{ github.ref }}",
            "cancel-in-progress": true
          }
        },
        "guard": {
          "uses": "./.github/workflows/_posting-guard.yml"
        }
      },
      "permissions": {
        "contents": "read"
      },
      "secrets_refs": [
        "SITE_URL",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [
        "./.github/workflows/_posting-guard.yml"
      ],
      "job_count": 2
    },
    {
      "path": ".github/workflows/post-evening.yml",
      "filename": "post-evening.yml",
      "name": "Post Evening Content",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 22 * * *"
            }
          ],
          "cron": [
            "0 22 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": null
        }
      ],
      "jobs": {
        "post-evening": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Post Evening Content",
              "run": "echo \"üåô Posting evening content...\"\n\n# Store response in variable for error analysis\nRESPONSE=$(curl -s -w \"HTTP_CODE:%{http_code}\" -L -X POST \"${{ secrets.SITE_URL }}/api/admin/posting/post-now\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"immediate\": true}' \\\n  --retry 2 --retry-delay 10 2>&1 || true)\n\n# Extract HTTP code and response body\nHTTP_CODE=$(echo \"$RESPONSE\" | grep -o 'HTTP_CODE:[0-9]*' | cut -d':' -f2)\nRESPONSE_BODY=$(echo \"$RESPONSE\" | sed 's/HTTP_CODE:[0-9]*$//')\n\necho \"HTTP Status: $HTTP_CODE\"\necho \"Response: $RESPONSE_BODY\"\n\n# Check for specific error conditions\nif [[ \"$HTTP_CODE\" == \"401\" ]]; then\n  echo \"‚ùå CRITICAL: Authentication failed - AUTH_TOKEN is invalid or expired\"\n  echo \"This indicates a security breach recovery issue - AUTH_TOKEN needs to be updated in GitHub secrets\"\n  exit 1\nelif [[ \"$HTTP_CODE\" != \"200\" ]]; then\n  echo \"‚ùå FAILED: API returned HTTP $HTTP_CODE\"\n  echo \"Response body: $RESPONSE_BODY\"\n  exit 1\nelse\n  echo \"‚úÖ SUCCESS: Evening content posted (HTTP $HTTP_CODE)\"\nfi\n"
            },
            {
              "name": "Verify Post Success",
              "if": "success()",
              "run": "echo \"‚úÖ Evening content posted successfully\"\n"
            },
            {
              "name": "Handle Failure",
              "if": "failure()",
              "run": "echo \"‚ùå CRITICAL: Evening posting failed - this may indicate:\"\necho \"  1. AUTH_TOKEN is invalid/expired (most common after security breach)\"\necho \"  2. API endpoint is down\"\necho \"  3. No approved content available\"\necho \"  4. Database connectivity issues\"\necho \"\"\necho \"üîß TROUBLESHOOTING STEPS:\"\necho \"  1. Check AUTH_TOKEN in GitHub secrets matches current production token\"\necho \"  2. Verify Vercel deployment is live\"\necho \"  3. Check content queue has approved posts available\"\necho \"  4. Review API logs in Vercel dashboard\"\necho \"\"\necho \"üìû IMMEDIATE ACTIONS REQUIRED:\"\necho \"  - Update AUTH_TOKEN in GitHub secrets if authentication failed\"\necho \"  - Run content approval if queue is empty\"\necho \"  - Check Vercel deployment status\"\n"
            }
          ],
          "needs": "guard",
          "timeout-minutes": 10,
          "concurrency": {
            "group": "post-evening-${{ github.ref }}",
            "cancel-in-progress": true
          }
        },
        "guard": {
          "uses": "./.github/workflows/_posting-guard.yml"
        }
      },
      "permissions": {
        "contents": "read"
      },
      "secrets_refs": [
        "SITE_URL",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [
        "./.github/workflows/_posting-guard.yml"
      ],
      "job_count": 2
    },
    {
      "path": ".github/workflows/post-late-night.yml",
      "filename": "post-late-night.yml",
      "name": "Post Late Night Content",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "30 22 * * *"
            }
          ],
          "cron": [
            "30 22 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": null
        }
      ],
      "jobs": {
        "post-late-night": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Post Late Night Content",
              "run": "echo \"üåÉ Posting late night content...\"\n\n# Store response in variable for error analysis\nRESPONSE=$(curl -s -w \"HTTP_CODE:%{http_code}\" -L -X POST \"${{ secrets.SITE_URL }}/api/admin/posting/post-now\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"immediate\": true}' \\\n  --retry 2 --retry-delay 10 2>&1 || true)\n\n# Extract HTTP code and response body\nHTTP_CODE=$(echo \"$RESPONSE\" | grep -o 'HTTP_CODE:[0-9]*' | cut -d':' -f2)\nRESPONSE_BODY=$(echo \"$RESPONSE\" | sed 's/HTTP_CODE:[0-9]*$//')\n\necho \"HTTP Status: $HTTP_CODE\"\necho \"Response: $RESPONSE_BODY\"\n\n# Check for specific error conditions\nif [[ \"$HTTP_CODE\" == \"401\" ]]; then\n  echo \"‚ùå CRITICAL: Authentication failed - AUTH_TOKEN is invalid or expired\"\n  echo \"This indicates a security breach recovery issue - AUTH_TOKEN needs to be updated in GitHub secrets\"\n  exit 1\nelif [[ \"$HTTP_CODE\" != \"200\" ]]; then\n  echo \"‚ùå FAILED: API returned HTTP $HTTP_CODE\"\n  echo \"Response body: $RESPONSE_BODY\"\n  exit 1\nelse\n  echo \"‚úÖ SUCCESS: Late night content posted (HTTP $HTTP_CODE)\"\nfi\n"
            },
            {
              "name": "Verify Post Success",
              "if": "success()",
              "run": "echo \"‚úÖ Late night content posted successfully\"\n"
            },
            {
              "name": "Handle Failure",
              "if": "failure()",
              "run": "echo \"‚ùå CRITICAL: Late night posting failed - this may indicate:\"\necho \"  1. AUTH_TOKEN is invalid/expired (most common after security breach)\"\necho \"  2. API endpoint is down\"\necho \"  3. No approved content available\"\necho \"  4. Database connectivity issues\"\necho \"\"\necho \"üîß TROUBLESHOOTING STEPS:\"\necho \"  1. Check AUTH_TOKEN in GitHub secrets matches current production token\"\necho \"  2. Verify Vercel deployment is live\"\necho \"  3. Check content queue has approved posts available\"\necho \"  4. Review API logs in Vercel dashboard\"\necho \"\"\necho \"üìû IMMEDIATE ACTIONS REQUIRED:\"\necho \"  - Update AUTH_TOKEN in GitHub secrets if authentication failed\"\necho \"  - Run content approval if queue is empty\"\necho \"  - Check Vercel deployment status\"\n"
            }
          ],
          "needs": "guard",
          "timeout-minutes": 10,
          "concurrency": {
            "group": "post-late-night-${{ github.ref }}",
            "cancel-in-progress": true
          }
        },
        "guard": {
          "uses": "./.github/workflows/_posting-guard.yml"
        }
      },
      "permissions": {
        "contents": "read"
      },
      "secrets_refs": [
        "SITE_URL",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [
        "./.github/workflows/_posting-guard.yml"
      ],
      "job_count": 2
    },
    {
      "path": ".github/workflows/post-lunch.yml",
      "filename": "post-lunch.yml",
      "name": "Post Lunch Content",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 12 * * *"
            }
          ],
          "cron": [
            "0 12 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": null
        }
      ],
      "jobs": {
        "post-lunch": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Post Lunch Content",
              "run": "echo \"ü•™ Posting lunch content...\"\n\n# Store response in variable for error analysis\nRESPONSE=$(curl -s -w \"HTTP_CODE:%{http_code}\" -L -X POST \"${{ secrets.SITE_URL }}/api/admin/posting/post-now\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"immediate\": true}' \\\n  --retry 2 --retry-delay 10 2>&1 || true)\n\n# Extract HTTP code and response body\nHTTP_CODE=$(echo \"$RESPONSE\" | grep -o 'HTTP_CODE:[0-9]*' | cut -d':' -f2)\nRESPONSE_BODY=$(echo \"$RESPONSE\" | sed 's/HTTP_CODE:[0-9]*$//')\n\necho \"HTTP Status: $HTTP_CODE\"\necho \"Response: $RESPONSE_BODY\"\n\n# Check for specific error conditions\nif [[ \"$HTTP_CODE\" == \"401\" ]]; then\n  echo \"‚ùå CRITICAL: Authentication failed - AUTH_TOKEN is invalid or expired\"\n  echo \"This indicates a security breach recovery issue - AUTH_TOKEN needs to be updated in GitHub secrets\"\n  exit 1\nelif [[ \"$HTTP_CODE\" != \"200\" ]]; then\n  echo \"‚ùå FAILED: API returned HTTP $HTTP_CODE\"\n  echo \"Response body: $RESPONSE_BODY\"\n  exit 1\nelse\n  echo \"‚úÖ SUCCESS: Lunch content posted (HTTP $HTTP_CODE)\"\nfi\n"
            },
            {
              "name": "Verify Post Success",
              "if": "success()",
              "run": "echo \"‚úÖ Lunch content posted successfully\"\n"
            },
            {
              "name": "Handle Failure",
              "if": "failure()",
              "run": "echo \"‚ùå CRITICAL: Lunch posting failed - this may indicate:\"\necho \"  1. AUTH_TOKEN is invalid/expired (most common after security breach)\"\necho \"  2. API endpoint is down\"\necho \"  3. No approved content available\"\necho \"  4. Database connectivity issues\"\necho \"\"\necho \"üîß TROUBLESHOOTING STEPS:\"\necho \"  1. Check AUTH_TOKEN in GitHub secrets matches current production token\"\necho \"  2. Verify Vercel deployment is live\"\necho \"  3. Check content queue has approved posts available\"\necho \"  4. Review API logs in Vercel dashboard\"\necho \"\"\necho \"üìû IMMEDIATE ACTIONS REQUIRED:\"\necho \"  - Update AUTH_TOKEN in GitHub secrets if authentication failed\"\necho \"  - Run content approval if queue is empty\"\necho \"  - Check Vercel deployment status\"\n"
            }
          ],
          "needs": "guard",
          "timeout-minutes": 10,
          "concurrency": {
            "group": "post-lunch-${{ github.ref }}",
            "cancel-in-progress": true
          }
        },
        "guard": {
          "uses": "./.github/workflows/_posting-guard.yml"
        }
      },
      "permissions": {
        "contents": "read"
      },
      "secrets_refs": [
        "SITE_URL",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [
        "./.github/workflows/_posting-guard.yml"
      ],
      "job_count": 2
    },
    {
      "path": ".github/workflows/post-remediation-check.yml",
      "filename": "post-remediation-check.yml",
      "name": "Post-Remediation Validation",
      "triggers": [
        {
          "event": "repository_dispatch",
          "config": {
            "types": [
              "post-remediation-check"
            ]
          }
        },
        {
          "event": "workflow_dispatch",
          "config": {
            "inputs": {
              "manual_trigger": {
                "description": "Manually trigger post-remediation validation",
                "required": false,
                "type": "boolean",
                "default": false
              }
            }
          }
        }
      ],
      "jobs": {
        "validate-remediation": {
          "name": "Validate Phase 3 Remediation Results",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 10,
          "steps": [
            {
              "name": "Checkout repository",
              "uses": "actions/checkout@v4",
              "with": {
                "token": "${{ secrets.GITHUB_TOKEN }}",
                "fetch-depth": 1
              }
            },
            {
              "name": "üìä Extract Remediation Context",
              "id": "context",
              "run": "echo \"üîç Extracting remediation context from dispatch payload...\"\n\n# Extract data from repository_dispatch payload\nSECURITY_IMPROVEMENT=\"${{ github.event.client_payload.security_improvement }}\"\nBUILD_DIAGNOSTICS=\"${{ github.event.client_payload.build_diagnostics }}\"\nTRIGGERED_BY=\"${{ github.event.client_payload.triggered_by }}\"\nCOMMIT_SHA=\"${{ github.event.client_payload.commit_sha }}\"\nREMEDIATION_TIME=\"${{ github.event.client_payload.remediation_timestamp }}\"\n\n# Set defaults for manual triggers\nif [ \"${{ github.event_name }}\" = \"workflow_dispatch\" ]; then\n  SECURITY_IMPROVEMENT=\"unknown\"\n  BUILD_DIAGNOSTICS=\"unknown\"\n  TRIGGERED_BY=\"manual\"\n  COMMIT_SHA=\"${{ github.sha }}\"\n  REMEDIATION_TIME=\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"\nfi\n\necho \"security_improvement=$SECURITY_IMPROVEMENT\" >> $GITHUB_OUTPUT\necho \"build_diagnostics=$BUILD_DIAGNOSTICS\" >> $GITHUB_OUTPUT\necho \"triggered_by=$TRIGGERED_BY\" >> $GITHUB_OUTPUT\necho \"commit_sha=$COMMIT_SHA\" >> $GITHUB_OUTPUT\necho \"remediation_time=$REMEDIATION_TIME\" >> $GITHUB_OUTPUT\n\necho \"üìã Remediation Context:\"\necho \"- Security Improvement: $SECURITY_IMPROVEMENT\"\necho \"- Build Diagnostics: $BUILD_DIAGNOSTICS\"\necho \"- Triggered By: $TRIGGERED_BY\"\necho \"- Commit SHA: $COMMIT_SHA\"\necho \"- Remediation Time: $REMEDIATION_TIME\"\n"
            },
            {
              "name": "Setup Node.js with caching",
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "18",
                "cache": "npm"
              }
            },
            {
              "name": "Install dependencies",
              "run": "npm ci --prefer-offline --no-audit --no-fund\nnpm install --no-save tsx\n"
            },
            {
              "name": "üß™ Re-run CI Health Checks",
              "id": "health-recheck",
              "run": "echo \"üß™ Re-running CI health checks to validate remediation...\"\n\n# Run the critical failure gatekeeper in report-only mode\nnpx tsx scripts/checkCriticalFailures.ts --report-only || true\n\n# Extract health metrics from the report\nif [[ -f \"reports/ci-health-gate.md\" ]]; then\n  echo \"health_report_generated=true\" >> $GITHUB_OUTPUT\n  \n  # Extract confidence score\n  if grep -q \"Confidence Score:\" reports/ci-health-gate.md; then\n    CONFIDENCE=$(grep \"Confidence Score:\" reports/ci-health-gate.md | head -1 | grep -oE '[0-9]+' || echo \"0\")\n    echo \"confidence_score=$CONFIDENCE\" >> $GITHUB_OUTPUT\n    echo \"üìä Current confidence score: $CONFIDENCE/100\"\n  fi\n  \n  # Check CI readiness\n  if grep -q \"CI Readiness: ‚úÖ Ready\" reports/ci-health-gate.md; then\n    echo \"ci_ready=true\" >> $GITHUB_OUTPUT\n    echo \"‚úÖ CI is ready to proceed\"\n  else\n    echo \"ci_ready=false\" >> $GITHUB_OUTPUT\n    echo \"‚ùå CI is still blocked\"\n  fi\n  \n  # Check for remaining blockers\n  BLOCKERS=$(grep -c \"Blocking Issues:\" reports/ci-health-gate.md || echo \"0\")\n  echo \"blocking_issues=$BLOCKERS\" >> $GITHUB_OUTPUT\nelse\n  echo \"health_report_generated=false\" >> $GITHUB_OUTPUT\n  echo \"ci_ready=false\" >> $GITHUB_OUTPUT\n  echo \"confidence_score=0\" >> $GITHUB_OUTPUT\nfi\n"
            },
            {
              "name": "üîç Validate Specific Components",
              "id": "component-validation",
              "run": "echo \"üîç Validating specific components after remediation...\"\n\n# Test lint status\necho \"üìù Testing lint status...\"\nif npm run lint:check 2>/dev/null || true; then\n  echo \"lint_status=pass\" >> $GITHUB_OUTPUT\n  echo \"‚úÖ Lint checks passing\"\nelse\n  echo \"lint_status=fail\" >> $GITHUB_OUTPUT\n  echo \"‚ùå Lint checks still failing\"\nfi\n\n# Test security status (basic check)\necho \"üîí Testing security status...\"\nif npm audit --audit-level=critical 2>/dev/null; then\n  echo \"security_status=pass\" >> $GITHUB_OUTPUT\n  echo \"‚úÖ No critical security vulnerabilities\"\nelse\n  echo \"security_status=fail\" >> $GITHUB_OUTPUT\n  echo \"‚ö†Ô∏è Critical security vulnerabilities remain\"\nfi\n\n# Test build status\necho \"üèóÔ∏è Testing build status...\"\nif timeout 120 npm run build 2>/dev/null; then\n  echo \"build_status=pass\" >> $GITHUB_OUTPUT\n  echo \"‚úÖ Build completed successfully\"\nelse\n  echo \"build_status=fail\" >> $GITHUB_OUTPUT\n  echo \"‚ùå Build still failing\"\nfi\n"
            },
            {
              "name": "üìà Calculate Remediation Effectiveness",
              "id": "effectiveness",
              "run": "echo \"üìà Calculating remediation effectiveness...\"\n\n# Get current health metrics\nCONFIDENCE=\"${{ steps.health-recheck.outputs.confidence_score }}\"\nCI_READY=\"${{ steps.health-recheck.outputs.ci_ready }}\"\nLINT_STATUS=\"${{ steps.component-validation.outputs.lint_status }}\"\nSECURITY_STATUS=\"${{ steps.component-validation.outputs.security_status }}\"\nBUILD_STATUS=\"${{ steps.component-validation.outputs.build_status }}\"\n\n# Calculate effectiveness score\nEFFECTIVENESS=0\n\n# Base score from confidence\nif [[ \"$CONFIDENCE\" =~ ^[0-9]+$ ]]; then\n  EFFECTIVENESS=$CONFIDENCE\nfi\n\n# Bonus points for component health\n[[ \"$LINT_STATUS\" == \"pass\" ]] && EFFECTIVENESS=$((EFFECTIVENESS + 5))\n[[ \"$SECURITY_STATUS\" == \"pass\" ]] && EFFECTIVENESS=$((EFFECTIVENESS + 10))\n[[ \"$BUILD_STATUS\" == \"pass\" ]] && EFFECTIVENESS=$((EFFECTIVENESS + 15))\n[[ \"$CI_READY\" == \"true\" ]] && EFFECTIVENESS=$((EFFECTIVENESS + 20))\n\n# Cap at 100\n[[ $EFFECTIVENESS -gt 100 ]] && EFFECTIVENESS=100\n\necho \"effectiveness_score=$EFFECTIVENESS\" >> $GITHUB_OUTPUT\necho \"üìä Remediation effectiveness: $EFFECTIVENESS/100\"\n\n# Determine overall status\nif [[ $EFFECTIVENESS -ge 80 && \"$CI_READY\" == \"true\" ]]; then\n  echo \"overall_status=success\" >> $GITHUB_OUTPUT\n  echo \"‚úÖ Remediation was highly effective\"\nelif [[ $EFFECTIVENESS -ge 60 ]]; then\n  echo \"overall_status=partial\" >> $GITHUB_OUTPUT\n  echo \"‚ö†Ô∏è Remediation was partially effective\"\nelse\n  echo \"overall_status=failed\" >> $GITHUB_OUTPUT\n  echo \"‚ùå Remediation was not effective\"\nfi\n"
            },
            {
              "name": "üö® Check Rollback Requirements",
              "id": "rollback-check",
              "if": "steps.effectiveness.outputs.effectiveness_score < 30",
              "run": "echo \"üö® Checking if rollback is required...\"\n\nEFFECTIVENESS=\"${{ steps.effectiveness.outputs.effectiveness_score }}\"\nCI_READY=\"${{ steps.health-recheck.outputs.ci_ready }}\"\n\necho \"Current effectiveness: $EFFECTIVENESS/100\"\necho \"CI Ready: $CI_READY\"\n\n# Determine if rollback is needed\nif [[ $EFFECTIVENESS -lt 30 && \"$CI_READY\" == \"false\" ]]; then\n  echo \"rollback_required=true\" >> $GITHUB_OUTPUT\n  echo \"üö® ROLLBACK REQUIRED: Health critically low after remediation\"\n  echo \"rollback_reason=Health score $EFFECTIVENESS < 30 and CI still blocked\" >> $GITHUB_OUTPUT\nelse\n  echo \"rollback_required=false\" >> $GITHUB_OUTPUT\n  echo \"‚úÖ No rollback required\"\nfi\n"
            },
            {
              "name": "üìä Generate Validation Summary",
              "run": "echo \"üìä Post-Remediation Validation Summary\" >> $GITHUB_STEP_SUMMARY\necho \"=======================================\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\necho \"### üéØ Remediation Context\" >> $GITHUB_STEP_SUMMARY\necho \"- **Triggered By:** ${{ steps.context.outputs.triggered_by }}\" >> $GITHUB_STEP_SUMMARY\necho \"- **Commit SHA:** ${{ steps.context.outputs.commit_sha }}\" >> $GITHUB_STEP_SUMMARY\necho \"- **Remediation Time:** ${{ steps.context.outputs.remediation_time }}\" >> $GITHUB_STEP_SUMMARY\necho \"- **Security Improvement:** ${{ steps.context.outputs.security_improvement }}\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\necho \"### üìà Validation Results\" >> $GITHUB_STEP_SUMMARY\necho \"- **Overall Status:** ${{ steps.effectiveness.outputs.overall_status }}\" >> $GITHUB_STEP_SUMMARY\necho \"- **Effectiveness Score:** ${{ steps.effectiveness.outputs.effectiveness_score }}/100\" >> $GITHUB_STEP_SUMMARY\necho \"- **Confidence Score:** ${{ steps.health-recheck.outputs.confidence_score }}/100\" >> $GITHUB_STEP_SUMMARY\necho \"- **CI Ready:** ${{ steps.health-recheck.outputs.ci_ready == 'true' && '‚úÖ Yes' || '‚ùå No' }}\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\necho \"### üîç Component Status\" >> $GITHUB_STEP_SUMMARY\necho \"- **Lint:** ${{ steps.component-validation.outputs.lint_status == 'pass' && '‚úÖ Pass' || '‚ùå Fail' }}\" >> $GITHUB_STEP_SUMMARY\necho \"- **Security:** ${{ steps.component-validation.outputs.security_status == 'pass' && '‚úÖ Pass' || '‚ùå Fail' }}\" >> $GITHUB_STEP_SUMMARY\necho \"- **Build:** ${{ steps.component-validation.outputs.build_status == 'pass' && '‚úÖ Pass' || '‚ùå Fail' }}\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\nif [[ \"${{ steps.rollback-check.outputs.rollback_required }}\" == \"true\" ]]; then\n  echo \"### üö® Rollback Required\" >> $GITHUB_STEP_SUMMARY\n  echo \"**Reason:** ${{ steps.rollback-check.outputs.rollback_reason }}\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\nfi\n\necho \"### üìã Next Actions\" >> $GITHUB_STEP_SUMMARY\nif [[ \"${{ steps.effectiveness.outputs.overall_status }}\" == \"success\" ]]; then\n  echo \"‚úÖ **Remediation successful** - CI can proceed normally\" >> $GITHUB_STEP_SUMMARY\nelif [[ \"${{ steps.effectiveness.outputs.overall_status }}\" == \"partial\" ]]; then\n  echo \"‚ö†Ô∏è **Partial success** - Review remaining issues and consider manual fixes\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"‚ùå **Remediation failed** - Manual intervention required\" >> $GITHUB_STEP_SUMMARY\n  if [[ \"${{ steps.rollback-check.outputs.rollback_required }}\" == \"true\" ]]; then\n    echo \"üö® **Consider rollback** - Health critically low\" >> $GITHUB_STEP_SUMMARY\n  fi\nfi\n"
            },
            {
              "name": "Upload Post-Remediation Reports",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "post-remediation-validation-reports",
                "path": "reports/ci-health-gate.md\nreports/build-diagnostics.md\nreports/security-deep-fix.md\n",
                "retention-days": 30
              }
            },
            {
              "name": "üìù Update Original PR with Validation Results",
              "if": "github.event_name == 'repository_dispatch' && github.event.client_payload.commit_sha",
              "env": {
                "GH_TOKEN": "${{ secrets.GITHUB_TOKEN }}"
              },
              "run": "echo \"üìù Updating original PR with validation results...\"\n\n# Find PR associated with the commit SHA\nCOMMIT_SHA=\"${{ steps.context.outputs.commit_sha }}\"\n\n# Create validation results comment\ncat > validation_comment.md << EOF\n## üß™ Post-Remediation Validation Results\n\n**Validation Time:** $(date -u +%Y-%m-%dT%H:%M:%SZ)  \n**Triggered By:** ${{ steps.context.outputs.triggered_by }}\n\n### üìä Results Summary\n- **Overall Status:** ${{ steps.effectiveness.outputs.overall_status == 'success' && '‚úÖ Success' || steps.effectiveness.outputs.overall_status == 'partial' && '‚ö†Ô∏è Partial' || '‚ùå Failed' }}\n- **Effectiveness Score:** ${{ steps.effectiveness.outputs.effectiveness_score }}/100\n- **CI Ready:** ${{ steps.health-recheck.outputs.ci_ready == 'true' && '‚úÖ Yes' || '‚ùå No' }}\n\n### üîç Component Validation\n- **Lint:** ${{ steps.component-validation.outputs.lint_status == 'pass' && '‚úÖ' || '‚ùå' }}\n- **Security:** ${{ steps.component-validation.outputs.security_status == 'pass' && '‚úÖ' || '‚ùå' }}\n- **Build:** ${{ steps.component-validation.outputs.build_status == 'pass' && '‚úÖ' || '‚ùå' }}\n\n${{ steps.rollback-check.outputs.rollback_required == 'true' && '### üö® Rollback Required\\n**Reason:** Health critically low after remediation\\n' || '' }}\n\n> **Note:** This validation ran independently after Phase 3 auto-healing completed.\nEOF\n\necho \"‚úÖ Validation results prepared for PR update\"\n"
            },
            {
              "name": "üéØ Final Validation Status",
              "run": "echo \"üéØ Post-Remediation Validation Complete\"\necho \"=======================================\"\necho \"\"\necho \"üìä Overall Status: ${{ steps.effectiveness.outputs.overall_status }}\"\necho \"üìà Effectiveness: ${{ steps.effectiveness.outputs.effectiveness_score }}/100\"\necho \"üöÄ CI Ready: ${{ steps.health-recheck.outputs.ci_ready }}\"\necho \"\"\n\nif [[ \"${{ steps.effectiveness.outputs.overall_status }}\" == \"success\" ]]; then\n  echo \"‚úÖ Phase 3 auto-healing was successful!\"\n  echo \"üéâ CI pipeline can proceed normally\"\n  exit 0\nelif [[ \"${{ steps.effectiveness.outputs.overall_status }}\" == \"partial\" ]]; then\n  echo \"‚ö†Ô∏è Phase 3 auto-healing was partially successful\"\n  echo \"üîß Some manual intervention may be needed\"\n  exit 0\nelse\n  echo \"‚ùå Phase 3 auto-healing was not effective\"\n  echo \"üö® Manual intervention required\"\n  if [[ \"${{ steps.rollback-check.outputs.rollback_required }}\" == \"true\" ]]; then\n    echo \"üîÑ Consider rollback due to critically low health\"\n  fi\n  exit 1\nfi\n"
            }
          ],
          "needs": "guard",
          "concurrency": {
            "group": "validate-remediation-${{ github.ref }}",
            "cancel-in-progress": true
          }
        },
        "guard": {
          "uses": "./.github/workflows/_posting-guard.yml"
        }
      },
      "permissions": {
        "contents": "read"
      },
      "env": {
        "NODE_OPTIONS": "--max-old-space-size=4096"
      },
      "secrets_refs": [
        "GITHUB_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [
        "./.github/workflows/_posting-guard.yml"
      ],
      "job_count": 2
    },
    {
      "path": ".github/workflows/post-snack.yml",
      "filename": "post-snack.yml",
      "name": "Post Afternoon Snack",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 15 * * *"
            }
          ],
          "cron": [
            "0 15 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": null
        }
      ],
      "jobs": {
        "post-snack": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Post Snack Content",
              "run": "echo \"üçø Posting afternoon snack content...\"\n\n# Store response in variable for error analysis\nRESPONSE=$(curl -s -w \"HTTP_CODE:%{http_code}\" -L -X POST \"${{ secrets.SITE_URL }}/api/admin/posting/post-now\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"immediate\": true}' \\\n  --retry 2 --retry-delay 10 2>&1 || true)\n\n# Extract HTTP code and response body\nHTTP_CODE=$(echo \"$RESPONSE\" | grep -o 'HTTP_CODE:[0-9]*' | cut -d':' -f2)\nRESPONSE_BODY=$(echo \"$RESPONSE\" | sed 's/HTTP_CODE:[0-9]*$//')\n\necho \"HTTP Status: $HTTP_CODE\"\necho \"Response: $RESPONSE_BODY\"\n\n# Check for specific error conditions\nif [[ \"$HTTP_CODE\" == \"401\" ]]; then\n  echo \"‚ùå CRITICAL: Authentication failed - AUTH_TOKEN is invalid or expired\"\n  echo \"This indicates a security breach recovery issue - AUTH_TOKEN needs to be updated in GitHub secrets\"\n  exit 1\nelif [[ \"$HTTP_CODE\" != \"200\" ]]; then\n  echo \"‚ùå FAILED: API returned HTTP $HTTP_CODE\"\n  echo \"Response body: $RESPONSE_BODY\"\n  exit 1\nelse\n  echo \"‚úÖ SUCCESS: Snack content posted (HTTP $HTTP_CODE)\"\nfi\n"
            },
            {
              "name": "Verify Post Success",
              "if": "success()",
              "run": "echo \"‚úÖ Snack content posted successfully\"\n"
            },
            {
              "name": "Handle Failure",
              "if": "failure()",
              "run": "echo \"‚ùå CRITICAL: Snack posting failed - this may indicate:\"\necho \"  1. AUTH_TOKEN is invalid/expired (most common after security breach)\"\necho \"  2. API endpoint is down\"\necho \"  3. No approved content available\"\necho \"  4. Database connectivity issues\"\necho \"\"\necho \"üîß TROUBLESHOOTING STEPS:\"\necho \"  1. Check AUTH_TOKEN in GitHub secrets matches current production token\"\necho \"  2. Verify Vercel deployment is live\"\necho \"  3. Check content queue has approved posts available\"\necho \"  4. Review API logs in Vercel dashboard\"\necho \"\"\necho \"üìû IMMEDIATE ACTIONS REQUIRED:\"\necho \"  - Update AUTH_TOKEN in GitHub secrets if authentication failed\"\necho \"  - Run content approval if queue is empty\"\necho \"  - Check Vercel deployment status\"\n"
            }
          ],
          "needs": "guard",
          "timeout-minutes": 10,
          "concurrency": {
            "group": "post-snack-${{ github.ref }}",
            "cancel-in-progress": true
          }
        },
        "guard": {
          "uses": "./.github/workflows/_posting-guard.yml"
        }
      },
      "permissions": {
        "contents": "read"
      },
      "secrets_refs": [
        "SITE_URL",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [
        "./.github/workflows/_posting-guard.yml"
      ],
      "job_count": 2
    },
    {
      "path": ".github/workflows/post.yml",
      "filename": "post.yml",
      "name": "Content Posting",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 13 * * *"
            },
            {
              "cron": "0 17 * * *"
            },
            {
              "cron": "0 20 * * *"
            },
            {
              "cron": "0 23 * * *"
            },
            {
              "cron": "0 2 * * *"
            },
            {
              "cron": "30 4 * * *"
            }
          ],
          "cron": [
            "0 13 * * *",
            "0 17 * * *",
            "0 20 * * *",
            "0 23 * * *",
            "0 2 * * *",
            "30 4 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": {
            "inputs": {
              "slot": {
                "description": "Time slot to post (breakfast, lunch, snack, dinner, evening, late-night, or manual)",
                "type": "choice",
                "options": [
                  "manual",
                  "breakfast",
                  "lunch",
                  "snack",
                  "dinner",
                  "evening",
                  "late-night"
                ],
                "default": "manual"
              },
              "dry-run": {
                "description": "Dry run mode (preview only)",
                "type": "boolean",
                "default": false
              }
            }
          }
        },
        {
          "event": "workflow_call",
          "config": {
            "inputs": {
              "slot": {
                "description": "Time slot to post",
                "type": "string",
                "default": "manual"
              },
              "dry-run": {
                "description": "Dry run mode",
                "type": "boolean",
                "default": false
              }
            }
          }
        }
      ],
      "jobs": {
        "determine-slot": {
          "name": "Determine Posting Slot",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 2,
          "outputs": {
            "slot": "${{ steps.strategy.outputs.slot }}",
            "slot-index": "${{ steps.strategy.outputs.slot-index }}",
            "dry-run": "${{ steps.strategy.outputs.dry-run }}",
            "auth-token": "${{ steps.strategy.outputs.auth-token }}"
          },
          "steps": [
            {
              "name": "Determine posting strategy",
              "id": "strategy",
              "run": "SLOT=\"${{ inputs.slot || 'manual' }}\"\nDRY_RUN=\"${{ inputs.dry-run || 'false' }}\"\nSLOT_INDEX=\"0\"\n\n# Determine slot based on schedule\nif [[ \"${{ github.event_name }}\" == \"schedule\" ]]; then\n  HOUR=$(date +%H)\n  MINUTE=$(date +%M)\n  \n  if [[ $HOUR -eq 13 && $MINUTE -eq 0 ]]; then\n    SLOT=\"breakfast\"\n    SLOT_INDEX=\"0\"\n    echo \"üåÖ Breakfast posting slot (8:00 AM ET)\"\n  elif [[ $HOUR -eq 17 && $MINUTE -eq 0 ]]; then\n    SLOT=\"lunch\"\n    SLOT_INDEX=\"1\"\n    echo \"üçΩÔ∏è Lunch posting slot (12:00 PM ET)\"\n  elif [[ $HOUR -eq 20 && $MINUTE -eq 0 ]]; then\n    SLOT=\"snack\"\n    SLOT_INDEX=\"2\"\n    echo \"üçø Snack posting slot (3:00 PM ET)\"\n  elif [[ $HOUR -eq 23 && $MINUTE -eq 0 ]]; then\n    SLOT=\"dinner\"\n    SLOT_INDEX=\"3\"\n    echo \"üçΩÔ∏è Dinner posting slot (6:00 PM ET)\"\n  elif [[ $HOUR -eq 2 && $MINUTE -eq 0 ]]; then\n    SLOT=\"evening\"\n    SLOT_INDEX=\"4\"\n    echo \"üåÜ Evening posting slot (9:00 PM ET)\"\n  elif [[ $HOUR -eq 4 && $MINUTE -eq 30 ]]; then\n    SLOT=\"late-night\"\n    SLOT_INDEX=\"5\"\n    echo \"üåô Late night posting slot (11:30 PM ET)\"\n  fi\nelse\n  # Map manual slots to indices\n  case \"$SLOT\" in\n    breakfast) SLOT_INDEX=\"0\" ;;\n    lunch) SLOT_INDEX=\"1\" ;;\n    snack) SLOT_INDEX=\"2\" ;;\n    dinner) SLOT_INDEX=\"3\" ;;\n    evening) SLOT_INDEX=\"4\" ;;\n    late-night) SLOT_INDEX=\"5\" ;;\n    manual) SLOT_INDEX=\"6\" ;;\n  esac\nfi\n\necho \"slot=$SLOT\" >> $GITHUB_OUTPUT\necho \"slot-index=$SLOT_INDEX\" >> $GITHUB_OUTPUT\necho \"dry-run=$DRY_RUN\" >> $GITHUB_OUTPUT\necho \"auth-token=${{ secrets.AUTH_TOKEN }}\" >> $GITHUB_OUTPUT\necho \"Selected slot: $SLOT (index: $SLOT_INDEX), dry-run: $DRY_RUN\"\n"
            }
          ]
        },
        "pre-post-check": {
          "name": "Pre-Post Validation",
          "runs-on": "ubuntu-latest",
          "needs": "determine-slot",
          "timeout-minutes": 5,
          "outputs": {
            "can-post": "${{ steps.validation.outputs.can-post }}",
            "content-available": "${{ steps.validation.outputs.content-available }}"
          },
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "20",
                "cache-key-suffix": "post-validation"
              }
            },
            {
              "name": "Setup Supabase environment",
              "uses": "./.github/actions/setup-supabase-rest",
              "with": {
                "supabase-url": "${{ secrets.SUPABASE_URL }}",
                "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY_V2 != '' && secrets.SUPABASE_SERVICE_ROLE_KEY_V2 || secrets.SUPABASE_SERVICE_ROLE_KEY }}",
                "database-url": "${{ secrets.DATABASE_URL }}"
              }
            },
            {
              "name": "Validate posting conditions",
              "id": "validation",
              "run": "echo \"üîç Validating posting conditions for slot: ${{ needs.determine-slot.outputs.slot }}...\"\n\nAUTH_TOKEN=\"${{ needs.determine-slot.outputs.auth-token }}\"\nSLOT_INDEX=\"${{ needs.determine-slot.outputs.slot-index }}\"\nTODAY=$(date +%Y-%m-%d)\n\n# Check if content is available for this slot\nRESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n  \"${{ secrets.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/admin/schedule/forecast?date=$TODAY\" \\\n  --max-time 30)\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//')\n\nCAN_POST=\"false\"\nCONTENT_AVAILABLE=\"false\"\n\nif [ \"$HTTP_STATUS\" -eq 200 ]; then\n  echo \"‚úÖ Schedule API accessible\"\n  CONTENT_AVAILABLE=\"true\"\n  \n  # Check if this slot hasn't been posted yet today\n  # This is a simplified check - you'd parse the JSON response\n  if [[ \"$BODY\" == *\"upcoming\"* ]]; then\n    CAN_POST=\"true\"\n    echo \"‚úÖ Content ready for posting\"\n  else\n    echo \"‚ö†Ô∏è Content may already be posted for this slot\"\n  fi\nelse\n  echo \"‚ùå Schedule API not accessible (status: $HTTP_STATUS)\"\nfi\n\n# Dry run mode always allows posting for testing\nif [[ \"${{ needs.determine-slot.outputs.dry-run }}\" == \"true\" ]]; then\n  CAN_POST=\"true\"\n  echo \"üß™ Dry run mode - posting validation bypassed\"\nfi\n\necho \"can-post=$CAN_POST\" >> $GITHUB_OUTPUT\necho \"content-available=$CONTENT_AVAILABLE\" >> $GITHUB_OUTPUT\necho \"Validation result: can-post=$CAN_POST, content-available=$CONTENT_AVAILABLE\"\n"
            }
          ]
        },
        "post-content": {
          "name": "Post Content",
          "runs-on": "ubuntu-latest",
          "needs": [
            "determine-slot",
            "pre-post-check"
          ],
          "if": "needs.pre-post-check.outputs.can-post == 'true'",
          "timeout-minutes": 10,
          "outputs": {
            "post-status": "${{ steps.post.outputs.post-status }}",
            "post-id": "${{ steps.post.outputs.post-id }}"
          },
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "20",
                "cache-key-suffix": "post-content"
              }
            },
            {
              "name": "Setup Supabase environment",
              "uses": "./.github/actions/setup-supabase-rest",
              "with": {
                "supabase-url": "${{ secrets.SUPABASE_URL }}",
                "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY_V2 != '' && secrets.SUPABASE_SERVICE_ROLE_KEY_V2 || secrets.SUPABASE_SERVICE_ROLE_KEY }}",
                "database-url": "${{ secrets.DATABASE_URL }}"
              }
            },
            {
              "name": "Execute content posting",
              "id": "post",
              "run": "echo \"üì§ Posting content for slot: ${{ needs.determine-slot.outputs.slot }}...\"\n\nAUTH_TOKEN=\"${{ needs.determine-slot.outputs.auth-token }}\"\nSLOT=\"${{ needs.determine-slot.outputs.slot }}\"\nSLOT_INDEX=\"${{ needs.determine-slot.outputs.slot-index }}\"\nDRY_RUN=\"${{ needs.determine-slot.outputs.dry-run }}\"\n\n# Prepare the posting request\nJSON_PAYLOAD=$(cat <<EOF\n{\n  \"slot\": \"$SLOT\",\n  \"slotIndex\": $SLOT_INDEX,\n  \"dryRun\": $DRY_RUN\n}\nEOF\n)\n\n# Execute the post\nRESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -X POST \\\n  -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"$JSON_PAYLOAD\" \\\n  \"${{ secrets.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/admin/post\" \\\n  --max-time 300)\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//')\n\necho \"post-status=$HTTP_STATUS\" >> $GITHUB_OUTPUT\n\nif [ \"$HTTP_STATUS\" -eq 200 ]; then\n  if [[ \"$DRY_RUN\" == \"true\" ]]; then\n    echo \"‚úÖ Dry run completed successfully\"\n    echo \"post-id=dry-run-$(date +%s)\" >> $GITHUB_OUTPUT\n  else\n    echo \"‚úÖ Content posted successfully\"\n    # Extract post ID from response if available\n    POST_ID=$(echo \"$BODY\" | grep -o '\"id\":\"[^\"]*\"' | cut -d'\"' -f4 || echo \"posted-$(date +%s)\")\n    echo \"post-id=$POST_ID\" >> $GITHUB_OUTPUT\n  fi\n  \n  echo \"Response: $BODY\"\nelse\n  echo \"‚ùå Content posting failed with status $HTTP_STATUS\"\n  echo \"Response: $BODY\"\n  exit 1\nfi\n"
            },
            {
              "name": "Upload posting logs",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "posting-logs-${{ needs.determine-slot.outputs.slot }}",
                "path": "logs/post-*.log\nreports/post-*.json\n",
                "retention-days": 7,
                "if-no-files-found": "ignore"
              }
            }
          ]
        },
        "post-validation": {
          "name": "Post-Posting Validation",
          "runs-on": "ubuntu-latest",
          "needs": [
            "determine-slot",
            "post-content"
          ],
          "if": "needs.post-content.outputs.post-status == '200'",
          "timeout-minutes": 5,
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "20",
                "cache-key-suffix": "post-validation"
              }
            },
            {
              "name": "Validate post success",
              "run": "echo \"‚úÖ Validating successful post: ${{ needs.post-content.outputs.post-id }}...\"\n\nAUTH_TOKEN=\"${{ needs.determine-slot.outputs.auth-token }}\"\nSLOT=\"${{ needs.determine-slot.outputs.slot }}\"\nDRY_RUN=\"${{ needs.determine-slot.outputs.dry-run }}\"\n\nif [[ \"$DRY_RUN\" == \"true\" ]]; then\n  echo \"üß™ Dry run mode - post validation skipped\"\nelse\n  # Verify the content was actually posted\n  sleep 10  # Give the system time to update\n  \n  # Check the schedule to see if the content is marked as posted\n  TODAY=$(date +%Y-%m-%d)\n  RESPONSE=$(curl -s \\\n    -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n    \"${{ secrets.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/admin/schedule/forecast?date=$TODAY\" \\\n    --max-time 30)\n  \n  if [[ \"$RESPONSE\" == *\"posted\"* ]]; then\n    echo \"‚úÖ Post validation confirmed - content appears in schedule as posted\"\n  else\n    echo \"‚ö†Ô∏è Post validation warning - content may not be marked as posted yet\"\n  fi\nfi\n\necho \"üéâ Posting workflow completed for slot: $SLOT\"\n"
            }
          ]
        },
        "summary": {
          "name": "Posting Summary",
          "runs-on": "ubuntu-latest",
          "needs": [
            "determine-slot",
            "pre-post-check",
            "post-content",
            "post-validation"
          ],
          "if": "always()",
          "steps": [
            {
              "name": "Generate posting summary",
              "run": "echo \"## üì§ Content Posting Results\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Slot:** ${{ needs.determine-slot.outputs.slot }}\" >> $GITHUB_STEP_SUMMARY\necho \"**Slot Index:** ${{ needs.determine-slot.outputs.slot-index }}\" >> $GITHUB_STEP_SUMMARY\necho \"**Dry Run:** ${{ needs.determine-slot.outputs.dry-run }}\" >> $GITHUB_STEP_SUMMARY\necho \"**Trigger:** ${{ github.event_name }}\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"| Step | Status | Details |\" >> $GITHUB_STEP_SUMMARY\necho \"|------|--------|---------|\" >> $GITHUB_STEP_SUMMARY\necho \"| Pre-Check | ${{ needs.pre-post-check.result }} | Can post: ${{ needs.pre-post-check.outputs.can-post }} |\" >> $GITHUB_STEP_SUMMARY\n\nif [[ \"${{ needs.post-content.result }}\" != \"\" ]]; then\n  echo \"| Posting | ${{ needs.post-content.result }} | Status: ${{ needs.post-content.outputs.post-status }} |\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"| Posting | skipped | Pre-checks failed |\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [[ \"${{ needs.post-validation.result }}\" != \"\" ]]; then\n  echo \"| Validation | ${{ needs.post-validation.result }} | Post verification |\" >> $GITHUB_STEP_SUMMARY\nfi\n\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Overall status\nif [[ \"${{ needs.post-content.result }}\" == \"success\" ]]; then\n  if [[ \"${{ needs.determine-slot.outputs.dry-run }}\" == \"true\" ]]; then\n    echo \"## üß™ Dry run completed successfully\" >> $GITHUB_STEP_SUMMARY\n    echo \"Content would have been posted for **${{ needs.determine-slot.outputs.slot }}** slot.\" >> $GITHUB_STEP_SUMMARY\n  else\n    echo \"## ‚úÖ Content posted successfully\" >> $GITHUB_STEP_SUMMARY\n    echo \"**Post ID:** ${{ needs.post-content.outputs.post-id }}\" >> $GITHUB_STEP_SUMMARY\n    echo \"Posted for **${{ needs.determine-slot.outputs.slot }}** slot.\" >> $GITHUB_STEP_SUMMARY\n  fi\nelif [[ \"${{ needs.pre-post-check.outputs.can-post }}\" == \"false\" ]]; then\n  echo \"## ‚ÑπÔ∏è Posting skipped\" >> $GITHUB_STEP_SUMMARY\n  echo \"Content was not available or already posted for this slot.\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"## ‚ùå Posting failed\" >> $GITHUB_STEP_SUMMARY\n  echo \"Check posting logs for details.\" >> $GITHUB_STEP_SUMMARY\nfi\n"
            }
          ]
        }
      },
      "env": {
        "NODE_ENV": "production",
        "CI": true
      },
      "concurrency": {
        "group": "posting-${{ github.ref }}-${{ inputs.slot || 'scheduled' }}",
        "cancel-in-progress": false
      },
      "secrets_refs": [
        "AUTH_TOKEN",
        "SUPABASE_URL",
        "SUPABASE_SERVICE_ROLE_KEY_V2",
        "DATABASE_URL",
        "SITE_URL"
      ],
      "vars_refs": [],
      "composite_actions": [
        "./.github/actions/setup-node",
        "./.github/actions/setup-supabase-rest"
      ],
      "reusable_workflows": [],
      "job_count": 5
    },
    {
      "path": ".github/workflows/prod-watchdog.yml",
      "filename": "prod-watchdog.yml",
      "name": "Production Autonomy Watchdog",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "7 * * * *"
            },
            {
              "cron": "7 10 * * *"
            },
            {
              "cron": "37 10 * * *"
            },
            {
              "cron": "7 11 * * *"
            }
          ],
          "cron": [
            "7 * * * *",
            "7 10 * * *",
            "37 10 * * *",
            "7 11 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": {
            "inputs": {
              "run_canary": {
                "description": "Run DRY_RUN posting canary",
                "required": false,
                "type": "boolean",
                "default": false
              },
              "date": {
                "description": "ISO date (ET) to analyze; default today",
                "required": false,
                "type": "string"
              }
            }
          }
        }
      ],
      "jobs": {
        "watchdog": {
          "permissions": {
            "contents": "read",
            "issues": "write",
            "checks": "write"
          },
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 10,
          "concurrency": {
            "group": "prod-watchdog-${{ github.ref }}",
            "cancel-in-progress": true
          },
          "env": {
            "SUPABASE_URL": "${{ secrets.SUPABASE_URL }}",
            "SUPABASE_SERVICE_ROLE_KEY": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY_V2 != '' && secrets.SUPABASE_SERVICE_ROLE_KEY_V2 || secrets.SUPABASE_SERVICE_ROLE_KEY }}",
            "PROD_BASE_URL": "${{ vars.PROD_BASE_URL || 'https://hotdog-diaries.vercel.app' }}",
            "RUN_CANARY": "${{ inputs.run_canary || 'false' }}",
            "GH_TOKEN": "${{ secrets.GITHUB_TOKEN }}"
          },
          "steps": [
            {
              "uses": "actions/checkout@v4"
            },
            {
              "uses": "pnpm/action-setup@v4"
            },
            {
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "20",
                "cache": "pnpm"
              }
            },
            {
              "run": "pnpm install --frozen-lockfile || pnpm install"
            },
            {
              "name": "Mint runtime JWT token for watchdog checks",
              "run": "echo \"üîê Minting runtime JWT token for production watchdog...\"\n\n# Try runtime token first, fall back to legacy AUTH_TOKEN if minting fails\nif TOKEN=$(pnpm -s tsx scripts/ci/lib/jwt.ts mint --ttl 30m --sub prod-watchdog --aud watchdog --iss hotdog-diaries 2>/dev/null); then\n  echo \"‚úÖ Successfully minted runtime JWT token\"\n  echo \"TOKEN=$TOKEN\" >> $GITHUB_ENV\n  echo \"TOKEN_TYPE=runtime\" >> $GITHUB_ENV\nelif [ -n \"${{ secrets.AUTH_TOKEN }}\" ]; then\n  echo \"‚ö†Ô∏è Runtime token minting failed, falling back to legacy AUTH_TOKEN\"\n  echo \"TOKEN=${{ secrets.AUTH_TOKEN }}\" >> $GITHUB_ENV\n  echo \"TOKEN_TYPE=legacy\" >> $GITHUB_ENV\nelse\n  echo \"‚ùå No token available for watchdog checks\"\n  exit 1\nfi\n",
              "env": {
                "JWT_SECRET": "${{ secrets.JWT_SECRET }}"
              }
            },
            {
              "name": "Test auth self-test endpoint before watchdog checks",
              "run": "echo \"üîç Testing auth self-test endpoint before running watchdog...\"\necho \"Token type: $TOKEN_TYPE\"\n\nSELFTEST_RESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  --max-time 10 \\\n  \"$PROD_BASE_URL/api/health/auth-selftest\")\n\nSELFTEST_STATUS=$(echo $SELFTEST_RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nSELFTEST_BODY=$(echo $SELFTEST_RESPONSE | sed -e 's/HTTPSTATUS:.*//g')\n\necho \"Auth Self-Test Status: $SELFTEST_STATUS\"\n\nif [ \"$SELFTEST_STATUS\" -eq 200 ]; then\n  echo \"‚úÖ Auth self-test passed - token valid\"\n  \n  # Parse auth details for logging\n  if echo \"$SELFTEST_BODY\" | jq -e '.ok == true' > /dev/null; then\n    ISS=$(echo \"$SELFTEST_BODY\" | jq -r '.iss // \"unknown\"')\n    AUD=$(echo \"$SELFTEST_BODY\" | jq -r '.aud // \"unknown\"')\n    SUB=$(echo \"$SELFTEST_BODY\" | jq -r '.sub // \"unknown\"')\n    KEY_VERSION=$(echo \"$SELFTEST_BODY\" | jq -r '.keyVersion // \"null\"')\n    \n    echo \"üìä Token Details:\"\n    echo \"  - Type: $TOKEN_TYPE\"\n    echo \"  - Issuer: $ISS\"\n    echo \"  - Audience: $AUD\"\n    echo \"  - Subject: $SUB\"\n    echo \"  - Key Version: $KEY_VERSION\"\n  fi\nelse\n  echo \"‚ùå Auth self-test failed with status $SELFTEST_STATUS\"\n  \n  if [ \"$SELFTEST_STATUS\" -eq 401 ]; then\n    CODE=$(echo \"$SELFTEST_BODY\" | jq -r '.code // \"unknown\"')\n    echo \"üö® Auth error: $CODE\"\n    if [ \"$CODE\" = \"INVALID_SIGNATURE\" ]; then\n      echo \"JWT_SECRET mismatch - production auth config issue\"\n    fi\n  fi\n  \n  echo \"‚ö†Ô∏è Watchdog will continue with degraded auth status\"\nfi\n"
            },
            {
              "name": "Actions check",
              "run": "pnpm tsx scripts/prod/check-actions-today.ts --date \"${{ inputs.date }}\"",
              "env": {
                "AUTH_TOKEN": "${{ env.TOKEN }}"
              }
            },
            {
              "name": "DB check",
              "run": "pnpm tsx scripts/prod/check-db-posting.ts --date \"${{ inputs.date }}\"",
              "env": {
                "AUTH_TOKEN": "${{ env.TOKEN }}"
              }
            },
            {
              "name": "UI probe",
              "run": "pnpm tsx scripts/prod/probe-ui.ts --date \"${{ inputs.date }}\"",
              "env": {
                "AUTH_TOKEN": "${{ env.TOKEN }}"
              }
            },
            {
              "name": "Deep deployment health validation",
              "run": "echo \"üè• Running comprehensive deployment health validation...\"\n\n# Deep health check (moved from deploy-gate.yml)\nRESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -H \"Authorization: Bearer $TOKEN\" \\\n  \"$PROD_BASE_URL/api/admin/health/deep\")\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//g')\n\necho \"Deep Health Status: $HTTP_STATUS\"\n\nif [ \"$HTTP_STATUS\" -eq 200 ]; then\n  echo \"‚úÖ Deep health check passed\"\n  \n  # Verify response structure\n  OK_STATUS=$(echo \"$BODY\" | jq -r '.ok // false')\n  if [ \"$OK_STATUS\" = \"true\" ]; then\n    echo \"‚úÖ Health status is OK\"\n    echo \"üìä Component Health:\"\n    echo \"$BODY\" | jq '.components // {}' || echo \"No component details available\"\n  else\n    echo \"‚ö†Ô∏è Health status indicates issues\"\n    echo \"$BODY\" | jq '.components // {}' || echo \"No component details available\"\n  fi\nelse\n  echo \"‚ùå Deep health check failed with status $HTTP_STATUS\"\n  echo \"Response: $BODY\"\nfi\n"
            },
            {
              "name": "Admin endpoint smoke test suite",
              "run": "echo \"üí® Running comprehensive admin endpoint smoke test...\"\n\n# Test critical admin endpoints (moved from deploy-gate.yml)\nENDPOINTS=(\n  \"/api/admin/dashboard/stats\"\n  \"/api/admin/queue/health\"\n  \"/api/admin/platforms/status\"\n  \"/api/admin/metrics\"\n  \"/api/admin/content/scheduled\"\n  \"/api/admin/content/recent\"\n)\n\nFAILED_ENDPOINTS=()\n\nfor ENDPOINT in \"${ENDPOINTS[@]}\"; do\n  echo \"Testing $ENDPOINT...\"\n  \n  RESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n    -H \"Authorization: Bearer $TOKEN\" \\\n    \"$PROD_BASE_URL$ENDPOINT\")\n  \n  HTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\n  \n  if [ \"$HTTP_STATUS\" -eq 200 ]; then\n    echo \"‚úÖ $ENDPOINT responding\"\n  else\n    echo \"‚ùå $ENDPOINT failed with status $HTTP_STATUS\"\n    FAILED_ENDPOINTS+=(\"$ENDPOINT\")\n  fi\ndone\n\nif [ ${#FAILED_ENDPOINTS[@]} -eq 0 ]; then\n  echo \"‚úÖ All admin endpoints passed smoke test\"\nelse\n  echo \"‚ö†Ô∏è ${#FAILED_ENDPOINTS[@]} admin endpoints failed:\"\n  printf '  - %s\\n' \"${FAILED_ENDPOINTS[@]}\"\nfi\n"
            },
            {
              "name": "Canary (optional)",
              "if": "${{ inputs.run_canary == true }}",
              "run": "pnpm tsx scripts/prod/synthetic-post-canary.ts",
              "env": {
                "AUTH_TOKEN": "${{ env.TOKEN }}"
              }
            },
            {
              "name": "Emit report",
              "id": "report",
              "run": "pnpm tsx scripts/prod/emit-watchdog-report.ts --date \"${{ inputs.date }}\"",
              "env": {
                "AUTH_TOKEN": "${{ env.TOKEN }}"
              }
            },
            {
              "name": "Upload artifact",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "PROD_WATCHDOG_REPORT",
                "path": "ci_audit/watchdog/",
                "retention-days": 7
              }
            },
            {
              "name": "Create/Update issue on failure",
              "if": "failure()",
              "run": "TITLE=\"‚ùå Production Watchdog Failure $(date -u +%F)\"\nBODY=\"$(cat ci_audit/watchdog/PROD_WATCHDOG_REPORT.md)\"\n# Try to find existing open issue\nNUM=$(gh issue list --search \"Production Watchdog Failure\" --state open --limit 1 --json number -q '.[0].number' || true)\nif [ -n \"$NUM\" ]; then\n  gh issue comment \"$NUM\" --body \"$BODY\"\nelse\n  gh issue create --title \"$TITLE\" --body \"$BODY\" --label \"production-watchdog\"\nfi\n"
            },
            {
              "name": "Alert webhook (optional)",
              "if": "failure()",
              "env": {
                "ALERT_WEBHOOK_URL": "${{ secrets.ALERT_WEBHOOK_URL }}"
              },
              "run": "if [ -n \"$ALERT_WEBHOOK_URL\" ]; then\n  printf '%s' \"{\\\"text\\\":\\\"‚ùå Hotdog Diaries: Production watchdog detected failures. See artifact: PROD_WATCHDOG_REPORT.\\\"}\" \\\n    | curl -s -X POST -H 'content-type: application/json' -d @- \"$ALERT_WEBHOOK_URL\" || true\nfi\n"
            }
          ]
        }
      },
      "secrets_refs": [
        "SUPABASE_URL",
        "SUPABASE_SERVICE_ROLE_KEY_V2",
        "GITHUB_TOKEN",
        "AUTH_TOKEN",
        "JWT_SECRET",
        "ALERT_WEBHOOK_URL"
      ],
      "vars_refs": [
        "PROD_BASE_URL"
      ],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/production-audit.yml",
      "filename": "production-audit.yml",
      "name": "üîç Production Audit",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 9 * * 2"
            }
          ],
          "cron": [
            "0 9 * * 2"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": {
            "inputs": {
              "force_run": {
                "description": "Force run audit even if recent run exists",
                "required": false,
                "type": "boolean",
                "default": false
              },
              "audit_window_days": {
                "description": "Number of days to audit (default: 7)",
                "required": false,
                "type": "number",
                "default": 7
              }
            }
          }
        }
      ],
      "jobs": {
        "production-audit": {
          "name": "üîç 7-Day Production Health Audit",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 20,
          "permissions": {
            "contents": "read",
            "actions": "read",
            "issues": "write"
          },
          "steps": [
            {
              "name": "üì• Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "üîß Install dependencies",
              "run": "# Install required tools for audit script\nsudo apt-get update\nsudo apt-get install -y bc jq curl\n"
            },
            {
              "name": "üîê Configure environment",
              "run": "echo \"APP_ORIGIN=${{ env.PRODUCTION_URL }}\" >> $GITHUB_ENV\necho \"GITHUB_REPO=${{ env.GITHUB_REPO }}\" >> $GITHUB_ENV\n\n# Validate required secrets are available\nif [[ -z \"${{ secrets.AUTH_TOKEN }}\" ]]; then\n  echo \"‚ùå AUTH_TOKEN secret not configured\"\n  exit 1\nfi\n\nif [[ -z \"${{ secrets.SUPABASE_URL }}\" ]]; then\n  echo \"‚ùå SUPABASE_URL secret not configured\"  \n  exit 1\nfi\n\nif [[ -z \"${{ secrets.SUPABASE_SERVICE_ROLE_KEY_V2 != '' && secrets.SUPABASE_SERVICE_ROLE_KEY_V2 || secrets.SUPABASE_SERVICE_ROLE_KEY }}\" ]]; then\n  echo \"‚ùå SUPABASE_SERVICE_ROLE_KEY secret not configured\"\n  exit 1\nfi\n\necho \"‚úÖ All required secrets available\"\n"
            },
            {
              "name": "üîç Run production audit",
              "id": "audit",
              "env": {
                "AUTH_TOKEN": "${{ secrets.AUTH_TOKEN }}",
                "SUPABASE_URL": "${{ secrets.SUPABASE_URL }}",
                "SUPABASE_SERVICE_KEY": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY_V2 != '' && secrets.SUPABASE_SERVICE_ROLE_KEY_V2 || secrets.SUPABASE_SERVICE_ROLE_KEY }}",
                "GITHUB_TOKEN": "${{ secrets.GITHUB_TOKEN }}"
              },
              "run": "echo \"üîç Starting 7-day production audit...\"\necho \"Target: $APP_ORIGIN\"\necho \"Repository: $GITHUB_REPO\"\necho \"\"\n\n# Make audit script executable\nchmod +x scripts/production-audit.sh\n\n# Run audit and capture exit code\nset +e  # Don't exit on failure, we want to capture results\n\nif scripts/production-audit.sh; then\n  AUDIT_EXIT_CODE=0\n  echo \"‚úÖ Production audit completed successfully\"\nelse\n  AUDIT_EXIT_CODE=$?\n  echo \"‚ùå Production audit detected issues (exit code: $AUDIT_EXIT_CODE)\"\nfi\n\necho \"AUDIT_EXIT_CODE=$AUDIT_EXIT_CODE\" >> $GITHUB_ENV\n\n# Check if artifacts were generated\nif [[ -d \"prod_audit_artifacts\" ]]; then\n  echo \"üìä Audit artifacts generated:\"\n  ls -la prod_audit_artifacts/\n  \n  # Extract key metrics for summary\n  if [[ -f \"prod_audit_artifacts/production_audit_$(date +%F).md\" ]]; then\n    echo \"üìã Audit report available\"\n    \n    # Extract summary information if available\n    if grep -q \"Total posts (7d):\" prod_audit_artifacts/production_audit_*.md; then\n      TOTAL_POSTS=$(grep \"Total posts (7d):\" prod_audit_artifacts/production_audit_*.md | grep -o '[0-9]\\+' | head -1)\n      echo \"TOTAL_POSTS=${TOTAL_POSTS:-0}\" >> $GITHUB_ENV\n    fi\n    \n    if grep -q \"Platform diversity:\" prod_audit_artifacts/production_audit_*.md; then\n      PLATFORM_COUNT=$(grep \"Platform diversity:\" prod_audit_artifacts/production_audit_*.md | grep -o '[0-9]\\+' | head -1)\n      echo \"PLATFORM_COUNT=${PLATFORM_COUNT:-0}\" >> $GITHUB_ENV\n    fi\n  fi\nelse\n  echo \"‚ö†Ô∏è No audit artifacts generated\"\nfi\n"
            },
            {
              "name": "üì§ Upload audit artifacts",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "production-audit-${{ github.run_number }}",
                "path": "prod_audit_artifacts/",
                "retention-days": 90,
                "compression-level": 6
              }
            },
            {
              "name": "üì§ Upload latest audit (overwrites)",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "production-audit-latest",
                "path": "prod_audit_artifacts/",
                "retention-days": 365,
                "compression-level": 6
              }
            },
            {
              "name": "üìä Generate audit summary",
              "if": "always()",
              "run": "echo \"## üîç Production Audit Summary\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Date:** $(date -u -Iseconds)\" >> $GITHUB_STEP_SUMMARY\necho \"**Target:** $APP_ORIGIN\" >> $GITHUB_STEP_SUMMARY\necho \"**Audit Window:** 7 days\" >> $GITHUB_STEP_SUMMARY\necho \"**Exit Code:** ${AUDIT_EXIT_CODE:-unknown}\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Status badge\nif [[ \"${AUDIT_EXIT_CODE:-1}\" -eq 0 ]]; then\n  echo \"**Status:** üü¢ **HEALTHY** - No issues detected\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"**Status:** üî¥ **ISSUES DETECTED** - Requires attention\" >> $GITHUB_STEP_SUMMARY\nfi\n\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"### üìä Key Metrics\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"| Metric | Value |\" >> $GITHUB_STEP_SUMMARY\necho \"|--------|-------|\" >> $GITHUB_STEP_SUMMARY\necho \"| Posts (7d) | ${TOTAL_POSTS:-N/A} |\" >> $GITHUB_STEP_SUMMARY\necho \"| Unique Platforms | ${PLATFORM_COUNT:-N/A} |\" >> $GITHUB_STEP_SUMMARY\necho \"| Audit Status | $([ \"${AUDIT_EXIT_CODE:-1}\" -eq 0 ] && echo \"‚úÖ Passed\" || echo \"‚ùå Failed\") |\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Add artifacts info\necho \"### üìÅ Generated Artifacts\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"- **Latest:** \\`production-audit-latest\\`\" >> $GITHUB_STEP_SUMMARY\necho \"- **Versioned:** \\`production-audit-${{ github.run_number }}\\`\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Include audit report summary if available\nif [[ -f prod_audit_artifacts/production_audit_*.md ]]; then\n  echo \"### üìã Audit Report\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"\\`\\`\\`\" >> $GITHUB_STEP_SUMMARY\n  head -20 prod_audit_artifacts/production_audit_*.md | grep -E \"^(#|##|\\*|-|[0-9])\" >> $GITHUB_STEP_SUMMARY || echo \"Report preview not available\" >> $GITHUB_STEP_SUMMARY\n  echo \"\\`\\`\\`\" >> $GITHUB_STEP_SUMMARY\nfi\n"
            },
            {
              "name": "üö® Create issue on critical failure",
              "if": "failure() && env.AUDIT_EXIT_CODE == '1' && github.event_name == 'schedule'",
              "uses": "actions/github-script@v7",
              "with": {
                "script": "const issueTitle = `üö® Production Audit Failed - ${new Date().toISOString().split('T')[0]}`;\nconst issueBody = `# Production Audit Failure Report\n\nThe weekly production audit detected critical issues on ${new Date().toISOString()}.\n\n**Workflow Run:** [#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})\n**Target Environment:** ${{ env.APP_ORIGIN }}\n**Audit Window:** 7 days\n\n## Failure Details\n\n- **Audit Exit Code:** ${{ env.AUDIT_EXIT_CODE }}\n- **Total Posts (7d):** ${{ env.TOTAL_POSTS || 'N/A' }}\n- **Platform Diversity:** ${{ env.PLATFORM_COUNT || 'N/A' }} unique platforms\n\n## Investigation Steps\n\n1. üîç Download audit artifacts: \\`production-audit-${{ github.run_number }}\\`\n2. üìä Review the generated markdown report for detailed findings\n3. üè• Check the [admin dashboard](${{ env.APP_ORIGIN }}/admin)\n4. üìà Review [system metrics](${{ env.APP_ORIGIN }}/api/system/metrics)\n5. üìã Consult the [SRE runbook](./docs/runbook.md) for incident response\n\n## Common Issues to Check\n\n- **Platform Diversity:** Is one platform dominating content (>60% share)?\n- **Content Queue:** Is the queue running low on approved content?\n- **Forecast Integrity:** Are there mismatches between API and database?\n- **CI Health:** Are GitHub Actions workflows failing repeatedly?\n- **Database Issues:** Are there connection or query problems?\n\n## Immediate Actions\n\n- [ ] Download and review audit artifacts\n- [ ] Check admin dashboard for obvious issues\n- [ ] Review recent deployment logs\n- [ ] Verify all API keys and secrets are valid\n- [ ] Run manual smoke tests if needed\n- [ ] Apply fixes and re-run audit to verify resolution\n- [ ] Close this issue when resolved\n\n---\n\nThis issue was automatically created by the Production Audit workflow.\n`;\n\nawait github.rest.issues.create({\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  title: issueTitle,\n  body: issueBody,\n  labels: ['critical', 'production', 'audit-failure', 'auto-created']\n});\n"
              }
            },
            {
              "name": "‚úÖ Success notification",
              "if": "success()",
              "run": "echo \"üéâ Production audit completed successfully!\"\necho \"\"\necho \"üìä Audit summary:\"\necho \"  - Target: $APP_ORIGIN\"\necho \"  - Window: 7 days\"\necho \"  - Posts: ${TOTAL_POSTS:-N/A}\"\necho \"  - Platforms: ${PLATFORM_COUNT:-N/A}\"\necho \"  - Status: ‚úÖ Healthy\"\necho \"\"\necho \"üîç Production system is operating within normal parameters.\"\n"
            }
          ]
        }
      },
      "env": {
        "PRODUCTION_URL": "https://hotdog-diaries.vercel.app",
        "GITHUB_REPO": "${{ github.repository }}"
      },
      "secrets_refs": [
        "AUTH_TOKEN",
        "SUPABASE_URL",
        "SUPABASE_SERVICE_ROLE_KEY_V2",
        "GITHUB_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/queue-monitor-hook.yml",
      "filename": "queue-monitor-hook.yml",
      "name": "Queue Monitor Hook",
      "triggers": [
        {
          "event": "workflow_call",
          "config": {
            "inputs": {
              "trigger_source": {
                "description": "Source that triggered this hook",
                "required": true,
                "type": "string"
              },
              "post_count": {
                "description": "Number of posts just made",
                "required": false,
                "default": "1",
                "type": "string"
              }
            },
            "secrets": {
              "SITE_URL": {
                "required": true
              },
              "AUTH_TOKEN": {
                "required": true
              }
            }
          }
        }
      ],
      "jobs": {
        "queue-monitor": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Check Queue Health After Posting",
              "id": "health-check",
              "run": "echo \"ü©∫ Checking queue health after posting from ${{ inputs.trigger_source }}...\"\n\n# Get current queue health\nHEALTH_RESPONSE=$(curl -s -w \"HTTP_CODE:%{http_code}\" -L \\\n  \"${{ secrets.SITE_URL }}/api/admin/queue-health\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  --retry 2 --retry-delay 5 2>&1 || true)\n\n# Extract HTTP code and response body\nHTTP_CODE=$(echo \"$HEALTH_RESPONSE\" | grep -o 'HTTP_CODE:[0-9]*' | cut -d':' -f2)\nRESPONSE_BODY=$(echo \"$HEALTH_RESPONSE\" | sed 's/HTTP_CODE:[0-9]*$//')\n\nif [[ \"$HTTP_CODE\" == \"200\" ]]; then\n  echo \"‚úÖ Queue health check successful\"\n  \n  # Extract key metrics\n  HEALTH_STATUS=$(echo \"$RESPONSE_BODY\" | jq -r '.health.status // \"unknown\"')\n  DAYS_CONTENT=$(echo \"$RESPONSE_BODY\" | jq -r '.queue.daysOfContent // 0')\n  APPROVED_CONTENT=$(echo \"$RESPONSE_BODY\" | jq -r '.queue.totalApproved // 0')\n  NEEDS_SCANNING=$(echo \"$RESPONSE_BODY\" | jq -r '.queue.needsScanning // false')\n  \n  echo \"üìä Queue Status: $HEALTH_STATUS\"\n  echo \"üìÖ Days of Content: $DAYS_CONTENT\"\n  echo \"‚úÖ Approved Content: $APPROVED_CONTENT\"\n  echo \"üîç Needs Scanning: $NEEDS_SCANNING\"\n  \n  # Set outputs for auto-scan decision\n  echo \"health_status=$HEALTH_STATUS\" >> $GITHUB_OUTPUT\n  echo \"days_content=$DAYS_CONTENT\" >> $GITHUB_OUTPUT\n  echo \"approved_content=$APPROVED_CONTENT\" >> $GITHUB_OUTPUT\n  echo \"needs_scanning=$NEEDS_SCANNING\" >> $GITHUB_OUTPUT\n  echo \"should_auto_scan=false\" >> $GITHUB_OUTPUT\n  \n  # Determine if we should trigger auto-scan\n  if [[ \"$HEALTH_STATUS\" == \"critical\" ]] || [[ \"$HEALTH_STATUS\" == \"emergency\" ]]; then\n    echo \"üö® CRITICAL/EMERGENCY: Auto-scan will be triggered\"\n    echo \"should_auto_scan=true\" >> $GITHUB_OUTPUT\n  elif [[ \"$NEEDS_SCANNING\" == \"true\" ]] && [[ \"${DAYS_CONTENT%.*}\" -lt 3 ]]; then\n    echo \"‚ö†Ô∏è WARNING: Low content - auto-scan will be triggered\"\n    echo \"should_auto_scan=true\" >> $GITHUB_OUTPUT\n  else\n    echo \"‚úÖ Queue is healthy - no auto-scan needed\"\n  fi\n  \nelse\n  echo \"‚ùå Queue health check failed with HTTP $HTTP_CODE\"\n  echo \"Response: $RESPONSE_BODY\"\n  # Don't fail the hook, just log the issue\n  echo \"should_auto_scan=false\" >> $GITHUB_OUTPUT\nfi\n"
            },
            {
              "name": "Trigger Auto-Scan if Needed",
              "if": "steps.health-check.outputs.should_auto_scan == 'true'",
              "run": "HEALTH_STATUS=\"${{ steps.health-check.outputs.health_status }}\"\nDAYS_CONTENT=\"${{ steps.health-check.outputs.days_content }}\"\n\necho \"üöÄ Triggering auto-scan due to low queue...\"\necho \"Health Status: $HEALTH_STATUS\"\necho \"Days of Content: $DAYS_CONTENT\"\n\n# Determine scan mode based on urgency\nif [[ \"$HEALTH_STATUS\" == \"emergency\" ]]; then\n  SCAN_MODE=\"emergency\"\n  echo \"üÜò Using EMERGENCY mode\"\nelif [[ \"$HEALTH_STATUS\" == \"critical\" ]]; then\n  SCAN_MODE=\"emergency\"\n  echo \"üö® Using EMERGENCY mode for critical status\"\nelse\n  SCAN_MODE=\"auto\"\n  echo \"ü§ñ Using AUTO mode\"\nfi\n\n# Trigger auto-scan\nAUTO_SCAN_RESPONSE=$(curl -s -w \"HTTP_CODE:%{http_code}\" -L -X POST \\\n  \"${{ secrets.SITE_URL }}/api/admin/auto-scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\\\"mode\\\": \\\"$SCAN_MODE\\\", \\\"triggeredBy\\\": \\\"queue-monitor-hook-${{ inputs.trigger_source }}\\\"}\" \\\n  --retry 2 --retry-delay 10 2>&1 || true)\n\n# Extract results\nAUTO_HTTP_CODE=$(echo \"$AUTO_SCAN_RESPONSE\" | grep -o 'HTTP_CODE:[0-9]*' | cut -d':' -f2)\nAUTO_RESPONSE_BODY=$(echo \"$AUTO_SCAN_RESPONSE\" | sed 's/HTTP_CODE:[0-9]*$//')\n\nif [[ \"$AUTO_HTTP_CODE\" == \"200\" ]]; then\n  echo \"‚úÖ Auto-scan triggered successfully\"\n  \n  # Extract scan results\n  TRIGGERED=$(echo \"$AUTO_RESPONSE_BODY\" | jq -r '.summary.totalTriggered // 0')\n  ERRORS=$(echo \"$AUTO_RESPONSE_BODY\" | jq -r '.summary.totalErrors // 0')\n  \n  echo \"üìä Auto-scan Results:\"\n  echo \"  - Triggered: $TRIGGERED scans\"\n  echo \"  - Errors: $ERRORS\"\n  \n  if [[ \"$TRIGGERED\" -gt 0 ]]; then\n    echo \"üéØ Platforms scanned:\"\n    echo \"$AUTO_RESPONSE_BODY\" | jq -r '.triggeredScans[]? // empty' | sed 's/^/  - /'\n  fi\n  \n  if [[ \"$ERRORS\" -gt 0 ]]; then\n    echo \"‚ö†Ô∏è Scan errors occurred:\"\n    echo \"$AUTO_RESPONSE_BODY\" | jq -r '.errors[]? // empty' | sed 's/^/  - /'\n  fi\n  \nelse\n  echo \"‚ùå Auto-scan failed with HTTP $AUTO_HTTP_CODE\"\n  echo \"Response: $AUTO_RESPONSE_BODY\"\n  # Don't fail the hook - the original posting was successful\nfi\n"
            },
            {
              "name": "Log Hook Completion",
              "if": "always()",
              "run": "HEALTH_STATUS=\"${{ steps.health-check.outputs.health_status || 'unknown' }}\"\nSHOULD_SCAN=\"${{ steps.health-check.outputs.should_auto_scan || 'false' }}\"\n\necho \"üìã Queue Monitor Hook Summary:\"\necho \"  - Triggered by: ${{ inputs.trigger_source }}\"\necho \"  - Posts made: ${{ inputs.post_count }}\"\necho \"  - Queue health: $HEALTH_STATUS\"\necho \"  - Auto-scan triggered: $SHOULD_SCAN\"\necho \"  - Hook completed: $(date -u)\"\n\nif [[ \"$SHOULD_SCAN\" == \"true\" ]]; then\n  echo \"‚úÖ Queue monitoring and auto-replenishment completed\"\nelse\n  echo \"‚úÖ Queue monitoring completed - no action needed\"\nfi\n"
            }
          ]
        }
      },
      "secrets_refs": [
        "SITE_URL",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/queue-monitor.yml",
      "filename": "queue-monitor.yml",
      "name": "Monitor Queue Health & Scan if Needed",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 */3 * * *"
            }
          ],
          "cron": [
            "0 */3 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": null
        }
      ],
      "jobs": {
        "check-queue-health": {
          "runs-on": "ubuntu-latest",
          "outputs": {
            "needs_scan": "${{ steps.check.outputs.needs_scan }}",
            "urgency": "${{ steps.check.outputs.urgency }}",
            "days_left": "${{ steps.check.outputs.days_left }}",
            "approved_count": "${{ steps.check.outputs.approved_count }}"
          },
          "steps": [
            {
              "name": "Check Queue Status",
              "id": "check",
              "run": "echo \"üîç Checking queue health...\"\n\nSTATUS=$(curl -L -s \"${{ secrets.SITE_URL }}/api/admin/schedule\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Accept: application/json\")\n\nif [ $? -ne 0 ]; then\n  echo \"‚ùå Failed to fetch queue status\"\n  echo \"needs_scan=true\" >> $GITHUB_OUTPUT\n  echo \"urgency=critical\" >> $GITHUB_OUTPUT\n  echo \"days_left=0\" >> $GITHUB_OUTPUT\n  echo \"approved_count=0\" >> $GITHUB_OUTPUT\n  exit 1\nfi\n\nAPPROVED_COUNT=$(echo \"$STATUS\" | jq -r '.queueStatus.totalApproved // 0')\nDAYS_OF_CONTENT=$(echo \"scale=1; $APPROVED_COUNT / 6\" | bc -l)\n\necho \"üìä Queue Status: $APPROVED_COUNT approved items = $DAYS_OF_CONTENT days\"\necho \"approved_count=$APPROVED_COUNT\" >> $GITHUB_OUTPUT\necho \"days_left=$DAYS_OF_CONTENT\" >> $GITHUB_OUTPUT\n\n# Determine if we need to scan\nif (( $(echo \"$DAYS_OF_CONTENT < 1\" | bc -l) )); then\n  echo \"üö® CRITICAL: Less than 1 day of content!\"\n  echo \"needs_scan=true\" >> $GITHUB_OUTPUT\n  echo \"urgency=critical\" >> $GITHUB_OUTPUT\nelif (( $(echo \"$DAYS_OF_CONTENT < 3\" | bc -l) )); then\n  echo \"‚ö†Ô∏è WARNING: Less than 3 days of content\"\n  echo \"needs_scan=true\" >> $GITHUB_OUTPUT\n  echo \"urgency=high\" >> $GITHUB_OUTPUT\nelif (( $(echo \"$DAYS_OF_CONTENT < 7\" | bc -l) )); then\n  echo \"üì° Normal scan needed (less than 7 days)\"\n  echo \"needs_scan=true\" >> $GITHUB_OUTPUT\n  echo \"urgency=normal\" >> $GITHUB_OUTPUT\nelse\n  echo \"‚úÖ Queue is healthy ($DAYS_OF_CONTENT days)\"\n  echo \"needs_scan=false\" >> $GITHUB_OUTPUT\n  echo \"urgency=none\" >> $GITHUB_OUTPUT\nfi\n"
            }
          ]
        },
        "scan-platforms": {
          "needs": "check-queue-health",
          "if": "needs.check-queue-health.outputs.needs_scan == 'true'",
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Emergency Scan All Platforms",
              "if": "needs.check-queue-health.outputs.urgency == 'critical'",
              "run": "echo \"üö® EMERGENCY: Scanning all platforms with auto-approval\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/content/emergency-scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"autoApprove\": true, \"maxItems\": 100, \"platforms\": [\"reddit\", \"youtube\", \"giphy\", \"pixabay\", \"bluesky\", \"imgur\", \"lemmy\", \"tumblr\"]}' \\\n  --fail --show-error --retry 2\n"
            },
            {
              "name": "High Priority Scan",
              "if": "needs.check-queue-health.outputs.urgency == 'high'",
              "run": "echo \"‚ö†Ô∏è HIGH PRIORITY: Scanning top platforms\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/content/priority-scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"platforms\": [\"reddit\", \"youtube\", \"giphy\", \"pixabay\"], \"maxItems\": 50}' \\\n  --fail --show-error --retry 2\n"
            },
            {
              "name": "Normal Scan",
              "if": "needs.check-queue-health.outputs.urgency == 'normal'",
              "run": "echo \"üì° Normal scan of select platforms\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/content/normal-scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"platforms\": [\"reddit\", \"giphy\", \"bluesky\"], \"maxItems\": 25}' \\\n  --fail --show-error --retry 2\n"
            },
            {
              "name": "Report Scan Results",
              "if": "always()",
              "run": "echo \"üìä Scan completed for urgency level: ${{ needs.check-queue-health.outputs.urgency }}\"\necho \"Queue had ${{ needs.check-queue-health.outputs.approved_count }} approved items\"\necho \"Estimated ${{ needs.check-queue-health.outputs.days_left }} days of content\"\n"
            }
          ]
        }
      },
      "secrets_refs": [
        "SITE_URL",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 2
    },
    {
      "path": ".github/workflows/queue-readiness.yml",
      "filename": "queue-readiness.yml",
      "name": "Queue Readiness",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 10 * * *"
            }
          ],
          "cron": [
            "0 10 * * *"
          ]
        }
      ],
      "jobs": {
        "check": {
          "permissions": {
            "contents": "read",
            "issues": "write"
          },
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 10,
          "concurrency": {
            "group": "queue-readiness-${{ github.ref }}",
            "cancel-in-progress": true
          },
          "steps": [
            {
              "uses": "actions/checkout@v4"
            },
            {
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "20",
                "cache": "pnpm"
              }
            },
            {
              "run": "pnpm install --frozen-lockfile || pnpm install"
            },
            {
              "id": "check",
              "run": "pnpm tsx scripts/ops/check-queue-readiness.ts --tz America/New_York --min 12"
            },
            {
              "name": "Open shortage issue (if needed)",
              "if": "failure()",
              "env": {
                "GH_TOKEN": "${{ secrets.GITHUB_TOKEN }}"
              },
              "run": "gh issue create --title \"Queue shortage detected (auto)\" --body \"$(cat ci_audit/queue-readiness-latest.md)\" --label \"queue-shortage\" || true\n"
            }
          ]
        }
      },
      "secrets_refs": [
        "GITHUB_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/runbook-artifact.yml",
      "filename": "runbook-artifact.yml",
      "name": "üìã Generate Runbook Artifacts",
      "triggers": [
        {
          "event": "push",
          "config": {
            "branches": [
              "main"
            ],
            "paths": [
              "docs/runbook.md",
              "scripts/md-to-pdf.ts",
              ".github/workflows/runbook-artifact.yml"
            ]
          }
        },
        {
          "event": "workflow_dispatch",
          "config": {
            "inputs": {
              "force_regenerate": {
                "description": "Force regenerate all artifacts",
                "required": false,
                "type": "boolean",
                "default": false
              }
            }
          }
        }
      ],
      "jobs": {
        "generate-runbook-artifacts": {
          "name": "üîÑ Generate PDF and Upload Artifacts",
          "runs-on": "ubuntu-latest",
          "permissions": {
            "contents": "read",
            "actions": "write"
          },
          "steps": [
            {
              "name": "üì• Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "üìã Setup Node.js",
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "18",
                "cache": "npm"
              }
            },
            {
              "name": "üì¶ Install dependencies",
              "run": "npm ci\n# Install additional system dependencies for PDF generation\nsudo apt-get update\nsudo apt-get install -y fonts-liberation fonts-dejavu-core\n"
            },
            {
              "name": "üìÑ Generate PDF from Markdown",
              "run": "echo \"üîÑ Generating runbook PDF...\"\nnpm run runbook:pdf\n\n# Verify PDF was generated\nif [ ! -f \"docs/runbook.pdf\" ]; then\n  echo \"‚ùå PDF generation failed\"\n  exit 1\nfi\n\n# Get file size for logging\nPDF_SIZE=$(du -h docs/runbook.pdf | cut -f1)\necho \"‚úÖ PDF generated successfully ($PDF_SIZE)\"\n"
            },
            {
              "name": "üìä Generate metadata",
              "run": "echo \"üìä Generating artifact metadata...\"\n\ncat > docs/runbook-metadata.json << EOF\n{\n  \"generated_at\": \"$(date -u -Iseconds)\",\n  \"git_commit\": \"${{ github.sha }}\",\n  \"git_ref\": \"${{ github.ref }}\",\n  \"workflow_run_id\": \"${{ github.run_id }}\",\n  \"workflow_run_number\": \"${{ github.run_number }}\",\n  \"triggered_by\": \"${{ github.event_name }}\",\n  \"actor\": \"${{ github.actor }}\",\n  \"repository\": \"${{ github.repository }}\",\n  \"pdf_size_bytes\": $(stat -c%s docs/runbook.pdf),\n  \"pdf_checksum_sha256\": \"$(sha256sum docs/runbook.pdf | cut -d' ' -f1)\",\n  \"source_files\": [\n    \"docs/runbook.md\",\n    \"scripts/md-to-pdf.ts\"\n  ],\n  \"format_version\": \"1.0.0\"\n}\nEOF\n\necho \"‚úÖ Metadata generated\"\ncat docs/runbook-metadata.json\n"
            },
            {
              "name": "üîç Validate artifacts",
              "run": "echo \"üîç Validating generated artifacts...\"\n\n# Check PDF is valid (basic validation)\nif file docs/runbook.pdf | grep -q \"PDF\"; then\n  echo \"‚úÖ PDF format validated\"\nelse\n  echo \"‚ùå Invalid PDF format\"\n  exit 1\nfi\n\n# Check metadata is valid JSON\nif jq . docs/runbook-metadata.json > /dev/null; then\n  echo \"‚úÖ Metadata JSON validated\"\nelse\n  echo \"‚ùå Invalid metadata JSON\"\n  exit 1\nfi\n\n# Check file sizes are reasonable\nPDF_SIZE_BYTES=$(stat -c%s docs/runbook.pdf)\nif [ $PDF_SIZE_BYTES -lt 10000 ]; then\n  echo \"‚ùå PDF suspiciously small ($PDF_SIZE_BYTES bytes)\"\n  exit 1\nelif [ $PDF_SIZE_BYTES -gt 50000000 ]; then\n  echo \"‚ùå PDF suspiciously large ($PDF_SIZE_BYTES bytes)\"\n  exit 1\nelse\n  echo \"‚úÖ PDF size reasonable ($PDF_SIZE_BYTES bytes)\"\nfi\n"
            },
            {
              "name": "üì§ Upload runbook artifacts",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "sre-runbook-${{ github.run_number }}",
                "path": "docs/runbook.pdf\ndocs/runbook.md\ndocs/runbook-metadata.json\n",
                "retention-days": 90,
                "compression-level": 6
              }
            },
            {
              "name": "üì§ Upload latest runbook (overwrites)",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "sre-runbook-latest",
                "path": "docs/runbook.pdf\ndocs/runbook.md\ndocs/runbook-metadata.json\n",
                "retention-days": 365,
                "compression-level": 6
              }
            },
            {
              "name": "üìã Generate summary",
              "run": "echo \"## üìã Runbook Artifacts Generated\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Generated at:** $(date -u -Iseconds)\" >> $GITHUB_STEP_SUMMARY\necho \"**Commit:** \\`${{ github.sha }}\\`\" >> $GITHUB_STEP_SUMMARY\necho \"**Triggered by:** ${{ github.event_name }}\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"### üìÑ Generated Files\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"| File | Size | SHA256 |\" >> $GITHUB_STEP_SUMMARY\necho \"|------|------|---------|\" >> $GITHUB_STEP_SUMMARY\necho \"| \\`docs/runbook.pdf\\` | $(du -h docs/runbook.pdf | cut -f1) | \\`$(sha256sum docs/runbook.pdf | cut -d' ' -f1 | head -c16)...\\` |\" >> $GITHUB_STEP_SUMMARY\necho \"| \\`docs/runbook.md\\` | $(du -h docs/runbook.md | cut -f1) | \\`$(sha256sum docs/runbook.md | cut -d' ' -f1 | head -c16)...\\` |\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"### üì§ Artifact Downloads\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"- **Versioned:** \\`sre-runbook-${{ github.run_number }}\\`\" >> $GITHUB_STEP_SUMMARY\necho \"- **Latest:** \\`sre-runbook-latest\\`\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"‚úÖ All artifacts generated successfully!\" >> $GITHUB_STEP_SUMMARY\n"
            },
            {
              "name": "‚úÖ Complete",
              "run": "echo \"üéâ Runbook artifact generation completed successfully!\"\necho \"\"\necho \"üì¶ Artifacts available:\"\necho \"  - sre-runbook-${{ github.run_number }} (versioned)\"\necho \"  - sre-runbook-latest (always current)\"\necho \"\"\necho \"üìÑ Generated files:\"\nls -la docs/runbook.*\n"
            }
          ]
        }
      },
      "secrets_refs": [],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/scan-bluesky.yml",
      "filename": "scan-bluesky.yml",
      "name": "Scan Bluesky for Content",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 1,9,17 * * *"
            }
          ],
          "cron": [
            "0 1,9,17 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": null
        }
      ],
      "jobs": {
        "scan-bluesky": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Scan Bluesky",
              "run": "echo \"ü¶ã Scanning Bluesky for hotdog posts...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/bluesky/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 15}' \\\n  --fail --show-error --retry 2 --retry-delay 5\n\necho \"‚úÖ Bluesky scan completed\"\n"
            },
            {
              "name": "Handle Scan Failure",
              "if": "failure()",
              "run": "echo \"‚ùå Bluesky scanning failed - will retry on next scheduled run\"\n"
            }
          ]
        }
      },
      "secrets_refs": [
        "SITE_URL",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/scan-giphy.yml",
      "filename": "scan-giphy.yml",
      "name": "Scan Giphy for Content",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 2,10,18 * * *"
            }
          ],
          "cron": [
            "0 2,10,18 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": null
        }
      ],
      "jobs": {
        "scan-giphy": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Scan Giphy",
              "run": "echo \"üé≠ Scanning Giphy for hotdog GIFs...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/giphy/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 15}' \\\n  --fail --show-error --retry 2 --retry-delay 5\n\necho \"‚úÖ Giphy scan completed\"\n"
            },
            {
              "name": "Handle Scan Failure",
              "if": "failure()",
              "run": "echo \"‚ùå Giphy scanning failed - will retry on next scheduled run\"\n"
            }
          ]
        }
      },
      "secrets_refs": [
        "SITE_URL",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/scan-imgur.yml",
      "filename": "scan-imgur.yml",
      "name": "Scan Imgur for Content",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 4,12,20 * * *"
            }
          ],
          "cron": [
            "0 4,12,20 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": null
        }
      ],
      "jobs": {
        "scan-imgur": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Scan Imgur",
              "run": "echo \"üñºÔ∏è Scanning Imgur for hotdog images...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/imgur/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 15}' \\\n  --fail --show-error --retry 2 --retry-delay 5\n\necho \"‚úÖ Imgur scan completed\"\n"
            },
            {
              "name": "Handle Scan Failure",
              "if": "failure()",
              "run": "echo \"‚ùå Imgur scanning failed - will retry on next scheduled run\"\n"
            }
          ]
        }
      },
      "secrets_refs": [
        "SITE_URL",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/scan-lemmy.yml",
      "filename": "scan-lemmy.yml",
      "name": "Scan Lemmy for Content",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 5,13,21 * * *"
            }
          ],
          "cron": [
            "0 5,13,21 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": null
        }
      ],
      "jobs": {
        "scan-lemmy": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Scan Lemmy",
              "run": "echo \"üåê Scanning Lemmy for hotdog content...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/lemmy/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 10}' \\\n  --fail --show-error --retry 2 --retry-delay 5 || echo \"‚ö†Ô∏è Lemmy scan failed (continuing)\"\n\necho \"‚úÖ Lemmy scan attempt completed\"\n"
            },
            {
              "name": "Handle Scan Failure",
              "if": "failure()",
              "run": "echo \"‚ö†Ô∏è Lemmy scanning failed - this is expected occasionally\"\n"
            }
          ]
        }
      },
      "secrets_refs": [
        "SITE_URL",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/scan-niche-platforms.yml",
      "filename": "scan-niche-platforms.yml",
      "name": "Scan Niche Platforms",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 6,14,22 * * *"
            }
          ],
          "cron": [
            "0 6,14,22 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": null
        }
      ],
      "jobs": {
        "scan-lemmy": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Scan Lemmy",
              "run": "echo \"üåê Scanning Lemmy for hotdog content...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/lemmy/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 10}' \\\n  --fail --show-error --retry 2 || echo \"‚ö†Ô∏è Lemmy scan failed (continuing)\"\n\necho \"‚úÖ Lemmy scan attempt completed\"\n"
            }
          ]
        },
        "scan-tumblr": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Scan Tumblr",
              "run": "echo \"üé® Scanning Tumblr for hotdog content...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/tumblr/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 10}' \\\n  --fail --show-error --retry 2 || echo \"‚ö†Ô∏è Tumblr scan failed (continuing)\"\n\necho \"‚úÖ Tumblr scan attempt completed\"\n"
            }
          ]
        }
      },
      "secrets_refs": [
        "SITE_URL",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 2
    },
    {
      "path": ".github/workflows/scan-pixabay.yml",
      "filename": "scan-pixabay.yml",
      "name": "Scan Pixabay for Content",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 3,11,19 * * *"
            }
          ],
          "cron": [
            "0 3,11,19 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": null
        }
      ],
      "jobs": {
        "scan-pixabay": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Scan Pixabay",
              "run": "echo \"üì∏ Scanning Pixabay for hotdog images...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/pixabay/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 15}' \\\n  --fail --show-error --retry 2 --retry-delay 5\n\necho \"‚úÖ Pixabay scan completed\"\n"
            },
            {
              "name": "Handle Scan Failure",
              "if": "failure()",
              "run": "echo \"‚ùå Pixabay scanning failed - will retry on next scheduled run\"\n"
            }
          ]
        }
      },
      "secrets_refs": [
        "SITE_URL",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/scan-reddit.yml",
      "filename": "scan-reddit.yml",
      "name": "Scan Reddit for Content",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 2,10,18 * * *"
            }
          ],
          "cron": [
            "0 2,10,18 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": null
        }
      ],
      "jobs": {
        "scan-reddit": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Scan Reddit",
              "run": "echo \"ü§ñ Scanning Reddit for hotdog content...\"\n\nRESULT=$(curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/reddit/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 20}' \\\n  --silent --show-error --retry 2)\n\nif [ $? -eq 0 ]; then\n  FOUND=$(echo \"$RESULT\" | jq -r '.totalFound // 0')\n  PROCESSED=$(echo \"$RESULT\" | jq -r '.processed // 0')\n  echo \"‚úÖ Reddit scan successful: Found $FOUND, processed $PROCESSED\"\nelse\n  echo \"‚ùå Reddit scan failed\"\n  exit 1\nfi\n"
            },
            {
              "name": "Handle Scan Failure",
              "if": "failure()",
              "run": "echo \"‚ùå Reddit scanning failed - will retry on next scheduled run\"\n"
            }
          ]
        }
      },
      "secrets_refs": [
        "SITE_URL",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/scan-social-platforms.yml",
      "filename": "scan-social-platforms.yml",
      "name": "Scan Social Platforms",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 1,9,17 * * *"
            }
          ],
          "cron": [
            "0 1,9,17 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": null
        }
      ],
      "jobs": {
        "scan-giphy": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Scan Giphy",
              "run": "echo \"üé≠ Scanning Giphy for hotdog GIFs...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/giphy/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 15}' \\\n  --fail --show-error --retry 2\n\necho \"‚úÖ Giphy scan completed\"\n"
            }
          ]
        },
        "scan-bluesky": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Scan Bluesky",
              "run": "echo \"ü¶ã Scanning Bluesky for hotdog posts...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/bluesky/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 15}' \\\n  --fail --show-error --retry 2\n\necho \"‚úÖ Bluesky scan completed\"\n"
            }
          ]
        },
        "scan-imgur": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Scan Imgur",
              "run": "echo \"üñºÔ∏è Scanning Imgur for hotdog images...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/imgur/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 15}' \\\n  --fail --show-error --retry 2\n\necho \"‚úÖ Imgur scan completed\"\n"
            }
          ]
        },
        "scan-pixabay": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Scan Pixabay",
              "run": "echo \"üì∏ Scanning Pixabay for hotdog images...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/pixabay/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 15}' \\\n  --fail --show-error --retry 2\n\necho \"‚úÖ Pixabay scan completed\"\n"
            }
          ]
        }
      },
      "secrets_refs": [
        "SITE_URL",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 4
    },
    {
      "path": ".github/workflows/scan-tumblr.yml",
      "filename": "scan-tumblr.yml",
      "name": "Scan Tumblr for Content",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 6,14,22 * * *"
            }
          ],
          "cron": [
            "0 6,14,22 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": null
        }
      ],
      "jobs": {
        "scan-tumblr": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Scan Tumblr",
              "run": "echo \"üé® Scanning Tumblr for hotdog content...\"\n\ncurl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/tumblr/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 10}' \\\n  --fail --show-error --retry 2 --retry-delay 5 || echo \"‚ö†Ô∏è Tumblr scan failed (continuing)\"\n\necho \"‚úÖ Tumblr scan attempt completed\"\n"
            },
            {
              "name": "Handle Scan Failure",
              "if": "failure()",
              "run": "echo \"‚ö†Ô∏è Tumblr scanning failed - this is expected occasionally\"\n"
            }
          ]
        }
      },
      "secrets_refs": [
        "SITE_URL",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/scan-youtube.yml",
      "filename": "scan-youtube.yml",
      "name": "Scan YouTube for Content",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 4,16 * * *"
            }
          ],
          "cron": [
            "0 4,16 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": null
        }
      ],
      "jobs": {
        "scan-youtube": {
          "runs-on": "ubuntu-latest",
          "steps": [
            {
              "name": "Scan YouTube",
              "run": "echo \"üé¨ Scanning YouTube for hotdog content...\"\n\nRESULT=$(curl -L -X POST \"${{ secrets.SITE_URL }}/api/admin/youtube/scan\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"maxPosts\": 10}' \\\n  --silent --show-error --retry 2)\n\nif [ $? -eq 0 ]; then\n  FOUND=$(echo \"$RESULT\" | jq -r '.totalFound // 0')\n  PROCESSED=$(echo \"$RESULT\" | jq -r '.processed // 0')\n  echo \"‚úÖ YouTube scan successful: Found $FOUND, processed $PROCESSED\"\nelse\n  echo \"‚ùå YouTube scan failed (possibly quota exceeded)\"\nfi\n"
            },
            {
              "name": "Handle Quota Issues",
              "if": "failure()",
              "run": "echo \"‚ö†Ô∏è YouTube scan failed - likely quota limit reached\"\necho \"Will retry on next scheduled run\"\n"
            }
          ]
        }
      },
      "secrets_refs": [
        "SITE_URL",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/scanners.yml",
      "filename": "scanners.yml",
      "name": "Content Scanners",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 */4 * * *"
            },
            {
              "cron": "30 */6 * * *"
            },
            {
              "cron": "15 */8 * * *"
            }
          ],
          "cron": [
            "0 */4 * * *",
            "30 */6 * * *",
            "15 */8 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": {
            "inputs": {
              "platforms": {
                "description": "Platforms to scan (comma-separated: reddit,youtube,giphy,imgur,bluesky,tumblr,lemmy,pixabay,niche)",
                "type": "string",
                "default": "all"
              },
              "max-posts": {
                "description": "Maximum posts per platform",
                "type": "number",
                "default": 50
              }
            }
          }
        },
        {
          "event": "workflow_call",
          "config": {
            "inputs": {
              "platforms": {
                "description": "Platforms to scan",
                "type": "string",
                "default": "all"
              },
              "max-posts": {
                "description": "Maximum posts per platform",
                "type": "number",
                "default": 50
              }
            }
          }
        }
      ],
      "jobs": {
        "plan-scans": {
          "name": "Plan Demand-Driven Scans",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 5,
          "outputs": {
            "matrix": "${{ steps.plan.outputs.matrix }}",
            "reason": "${{ steps.plan.outputs.reason }}",
            "has_work": "${{ steps.plan.outputs.has_work }}"
          },
          "env": {
            "TARGET_URL": "${{ vars.SITE_URL || secrets.SITE_URL || 'https://hotdog-diaries.vercel.app' }}",
            "SUPABASE_URL": "${{ secrets.SUPABASE_URL }}",
            "SUPABASE_SERVICE_ROLE_KEY": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY_V2 != '' && secrets.SUPABASE_SERVICE_ROLE_KEY_V2 || secrets.SUPABASE_SERVICE_ROLE_KEY }}",
            "SCAN_MIN_PER_PLATFORM": "${{ vars.SCAN_MIN_PER_PLATFORM || '40' }}",
            "SCAN_MAX_PER_PLATFORM": "${{ vars.SCAN_MAX_PER_PLATFORM || '120' }}",
            "SCAN_GLOBAL_MAX": "${{ vars.SCAN_GLOBAL_MAX || '800' }}",
            "SCAN_COOLDOWN_MIN": "${{ vars.SCAN_COOLDOWN_MIN || '180' }}",
            "MIN_CONF": "${{ vars.MIN_CONF || '0.70' }}",
            "MIN_CANDIDATES": "${{ vars.MIN_CANDIDATES || '20' }}",
            "PLATFORM_ALLOW": "${{ vars.PLATFORM_ALLOW || 'reddit,youtube,giphy,imgur,bluesky,tumblr,lemmy,pixabay' }}"
          },
          "steps": [
            {
              "uses": "actions/checkout@v4"
            },
            {
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": 20,
                "cache": "npm"
              }
            },
            {
              "name": "Install dependencies",
              "run": "echo \"üì¶ Installing dependencies for planner tests...\"\nnpm ci --prefer-offline --no-audit --no-fund\n"
            },
            {
              "name": "Runtime policy guard",
              "id": "policy-guard",
              "run": "echo \"üõ°Ô∏è Running runtime policy enforcement...\"\n\n# Verify planner tests pass before allowing scans\nif ! npm run test:planner --silent >/dev/null 2>&1; then\n  echo \"‚ùå POLICY VIOLATION: Planner contract tests failed\"\n  echo \"üö® Blocking scan execution for safety\"\n  echo \"policy-violation=true\" >> $GITHUB_OUTPUT\n  exit 1\nfi\n\necho \"‚úÖ Policy guard passed - planner behavior validated\"\necho \"policy-violation=false\" >> $GITHUB_OUTPUT\n"
            },
            {
              "name": "Build scan plan",
              "id": "plan",
              "if": "steps.policy-guard.outputs.policy-violation == 'false'",
              "run": "echo \"üîç Analyzing queue depths and planning scans...\"\nnode scripts/scan-plan.mjs 2>&1 | tee plan_stdout.txt || true\n\n# Check if matrix has any work\nif [ -f scan_matrix.json ]; then\n  matrix=$(cat scan_matrix.json)\n  has_work=$(echo \"$matrix\" | jq -r 'if .include | length > 0 then \"true\" else \"false\" end')\nelse\n  matrix='{\"include\":[]}'\n  has_work='false'\nfi\n\n# Output for next job\necho \"matrix=$matrix\" >> $GITHUB_OUTPUT\necho \"has_work=$has_work\" >> $GITHUB_OUTPUT\n\n# Handle policy guard results\npolicy_status=\"Unknown\"\nif [ \"${{ steps.policy-guard.outputs.policy-violation }}\" = \"true\" ]; then\n  policy_status=\"‚ùå VIOLATION - Scan blocked\"\n  echo \"reason=policy_violation\" >> $GITHUB_OUTPUT\nelif [ \"${{ steps.policy-guard.outputs.policy-violation }}\" = \"false\" ]; then\n  policy_status=\"‚úÖ Passed\"\nfi\n\n# Summary for UI\necho \"## üìä Scan Planning Results\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Policy Guard:** $policy_status\" >> $GITHUB_STEP_SUMMARY\n\nif [ -f scan_plan.json ]; then\n  reason=$(jq -r '.analysis.reason // \"unknown\"' scan_plan.json)\n  echo \"reason=$reason\" >> $GITHUB_OUTPUT\n  echo \"**Reason:** $reason\" >> $GITHUB_STEP_SUMMARY\n  echo \"**Platforms to scan:** $(echo \"$matrix\" | jq -r '.include | length')\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  \n  if [ \"$has_work\" = \"true\" ]; then\n    echo \"### Platforms Needing Content\" >> $GITHUB_STEP_SUMMARY\n    echo \"| Platform | Desired Posts |\" >> $GITHUB_STEP_SUMMARY\n    echo \"|----------|---------------|\" >> $GITHUB_STEP_SUMMARY\n    echo \"$matrix\" | jq -r '.include[] | \"| \\(.platform) | \\(.desired) |\"' >> $GITHUB_STEP_SUMMARY\n  else\n    echo \"‚úÖ All platforms have sufficient content queued!\" >> $GITHUB_STEP_SUMMARY\n  fi\nelse\n  echo \"**Status:** Scan planning blocked due to policy violation\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"‚ö†Ô∏è **SAFETY BLOCK**: Planner contract tests failed. Scans are disabled until issues are resolved.\" >> $GITHUB_STEP_SUMMARY\nfi\n"
            },
            {
              "name": "Upload plan artifacts",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "scan-plan-${{ github.run_number }}",
                "path": "scan_plan.json\nscan_matrix.json\nplan_stdout.txt\n",
                "retention-days": 7,
                "if-no-files-found": "ignore"
              }
            }
          ]
        },
        "determine-platforms": {
          "name": "Determine Scan Strategy (Legacy)",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 2,
          "if": "${{ github.event_name == 'workflow_dispatch' && (inputs.platforms != '' && inputs.platforms != 'demand-driven') }}",
          "outputs": {
            "platforms": "${{ steps.strategy.outputs.platforms }}",
            "schedule-type": "${{ steps.strategy.outputs.schedule-type }}"
          },
          "steps": [
            {
              "name": "Determine scan strategy",
              "id": "strategy",
              "run": "PLATFORMS=\"${{ inputs.platforms || 'all' }}\"\nSCHEDULE_TYPE=\"manual\"\n\n# Determine platforms based on trigger\nif [[ \"${{ github.event_name }}\" == \"schedule\" ]]; then\n  SCHEDULE_TYPE=\"scheduled\"\n  \n  # Determine which platforms based on cron time\n  HOUR=$(date +%H)\n  MINUTE=$(date +%M)\n  \n  if [[ $MINUTE -eq 0 ]]; then\n    # Every 4 hours (0, 4, 8, 12, 16, 20) - high volume\n    PLATFORMS=\"reddit,youtube,giphy\"\n    echo \"üîÑ High-volume scan: reddit,youtube,giphy\"\n  elif [[ $MINUTE -eq 30 ]]; then\n    # Every 6 hours (00:30, 06:30, 12:30, 18:30) - medium volume  \n    PLATFORMS=\"imgur,bluesky,tumblr\"\n    echo \"üîÑ Medium-volume scan: imgur,bluesky,tumblr\"\n  elif [[ $MINUTE -eq 15 ]]; then\n    # Every 8 hours (00:15, 08:15, 16:15) - low volume\n    PLATFORMS=\"lemmy,pixabay,niche\"\n    echo \"üîÑ Low-volume scan: lemmy,pixabay,niche\"\n  fi\nfi\n\nif [[ \"$PLATFORMS\" == \"all\" ]]; then\n  PLATFORMS=\"reddit,youtube,giphy,imgur,bluesky,tumblr,lemmy,pixabay,niche\"\nfi\n\necho \"platforms=$PLATFORMS\" >> $GITHUB_OUTPUT\necho \"schedule-type=$SCHEDULE_TYPE\" >> $GITHUB_OUTPUT\necho \"Selected platforms: $PLATFORMS\"\n"
            }
          ]
        },
        "demand-driven-scan": {
          "name": "Scan Content (Demand-Driven)",
          "runs-on": "ubuntu-latest",
          "needs": "plan-scans",
          "if": "${{ needs.plan-scans.outputs.has_work == 'true' }}",
          "timeout-minutes": 15,
          "strategy": {
            "fail-fast": false,
            "max-parallel": 3,
            "matrix": "${{ fromJSON(needs.plan-scans.outputs.matrix) }}"
          },
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "20",
                "cache-key-suffix": "scanner-${{ matrix.platform }}"
              }
            },
            {
              "name": "Setup Supabase environment",
              "uses": "./.github/actions/setup-supabase-rest",
              "with": {
                "supabase-url": "${{ secrets.SUPABASE_URL }}",
                "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY_V2 != '' && secrets.SUPABASE_SERVICE_ROLE_KEY_V2 || secrets.SUPABASE_SERVICE_ROLE_KEY }}",
                "database-url": "${{ secrets.DATABASE_URL }}"
              }
            },
            {
              "name": "Platform-specific setup",
              "run": "echo \"üîß Setting up environment for ${{ matrix.platform }}...\"\necho \"üìä Target: ${{ matrix.desired }} posts\"\n\ncase \"${{ matrix.platform }}\" in\n  reddit)\n    echo \"REDDIT_CLIENT_ID=${{ secrets.REDDIT_CLIENT_ID }}\" >> $GITHUB_ENV\n    echo \"REDDIT_CLIENT_SECRET=${{ secrets.REDDIT_CLIENT_SECRET }}\" >> $GITHUB_ENV\n    ;;\n  youtube)\n    echo \"YOUTUBE_API_KEY=${{ secrets.YOUTUBE_API_KEY }}\" >> $GITHUB_ENV\n    ;;\n  giphy)\n    echo \"GIPHY_API_KEY=${{ secrets.GIPHY_API_KEY }}\" >> $GITHUB_ENV\n    ;;\n  imgur)\n    echo \"IMGUR_CLIENT_ID=${{ secrets.IMGUR_CLIENT_ID }}\" >> $GITHUB_ENV\n    ;;\n  bluesky)\n    echo \"BLUESKY_IDENTIFIER=${{ secrets.BLUESKY_IDENTIFIER }}\" >> $GITHUB_ENV\n    echo \"BLUESKY_APP_PASSWORD=${{ secrets.BLUESKY_APP_PASSWORD }}\" >> $GITHUB_ENV\n    ;;\n  pixabay)\n    echo \"PIXABAY_API_KEY=${{ secrets.PIXABAY_API_KEY }}\" >> $GITHUB_ENV\n    ;;\n  tumblr)\n    echo \"TUMBLR_API_KEY=${{ secrets.TUMBLR_API_KEY }}\" >> $GITHUB_ENV\n    ;;\nesac\n"
            },
            {
              "name": "Scan platform content",
              "id": "scan",
              "run": "echo \"üì° Scanning ${{ matrix.platform }} for hotdog content...\"\necho \"üéØ Targeting ${{ matrix.desired }} posts based on queue deficit\"\n\n# Use the desired count from the planning phase\nMAX_POSTS=\"${{ matrix.desired }}\"\n\ncase \"${{ matrix.platform }}\" in\n  reddit)\n    npm run scan:reddit -- --max-posts=$MAX_POSTS || true\n    ;;\n  youtube)\n    npm run scan:youtube -- --max-posts=$MAX_POSTS || true\n    ;;\n  giphy)\n    npm run scan:giphy -- --max-posts=$MAX_POSTS || true\n    ;;\n  imgur)\n    npm run scan:imgur -- --max-posts=$MAX_POSTS || true\n    ;;\n  bluesky)\n    npm run scan:bluesky -- --max-posts=$MAX_POSTS || true\n    ;;\n  tumblr)\n    npm run scan:tumblr -- --max-posts=$MAX_POSTS || true\n    ;;\n  lemmy)\n    npm run scan:lemmy -- --max-posts=$MAX_POSTS || true\n    ;;\n  pixabay)\n    npm run scan:pixabay -- --max-posts=$MAX_POSTS || true\n    ;;\n  *)\n    echo \"‚ùå Unknown platform: ${{ matrix.platform }}\"\n    ;;\nesac\n\necho \"‚úÖ Scan completed for ${{ matrix.platform }}\"\n"
            },
            {
              "name": "Upload scan logs",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "scan-logs-${{ matrix.platform }}-${{ github.run_number }}",
                "path": "logs/scan-*.log\nreports/scan-*.json\n",
                "retention-days": 3,
                "if-no-files-found": "ignore"
              }
            }
          ]
        },
        "scan": {
          "name": "Scan Content (Legacy)",
          "runs-on": "ubuntu-latest",
          "needs": "determine-platforms",
          "if": "${{ needs.determine-platforms.outputs.platforms != '' }}",
          "timeout-minutes": 15,
          "strategy": {
            "fail-fast": false,
            "max-parallel": 3,
            "matrix": {
              "platform": "${{ fromJSON(format('[{0}]', replace(replace(needs.determine-platforms.outputs.platforms, ' ', ''), ',', '\",\"'))) }}"
            }
          },
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "20",
                "cache-key-suffix": "scanner-${{ matrix.platform }}"
              }
            },
            {
              "name": "Setup Supabase environment",
              "uses": "./.github/actions/setup-supabase-rest",
              "with": {
                "supabase-url": "${{ secrets.SUPABASE_URL }}",
                "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY_V2 != '' && secrets.SUPABASE_SERVICE_ROLE_KEY_V2 || secrets.SUPABASE_SERVICE_ROLE_KEY }}",
                "database-url": "${{ secrets.DATABASE_URL }}"
              }
            },
            {
              "name": "Platform-specific setup",
              "run": "echo \"üîß Setting up environment for ${{ matrix.platform }}...\"\n\ncase \"${{ matrix.platform }}\" in\n  reddit)\n    echo \"REDDIT_CLIENT_ID=${{ secrets.REDDIT_CLIENT_ID }}\" >> $GITHUB_ENV\n    echo \"REDDIT_CLIENT_SECRET=${{ secrets.REDDIT_CLIENT_SECRET }}\" >> $GITHUB_ENV\n    ;;\n  youtube)\n    echo \"YOUTUBE_API_KEY=${{ secrets.YOUTUBE_API_KEY }}\" >> $GITHUB_ENV\n    ;;\n  giphy)\n    echo \"GIPHY_API_KEY=${{ secrets.GIPHY_API_KEY }}\" >> $GITHUB_ENV\n    ;;\n  imgur)\n    echo \"IMGUR_CLIENT_ID=${{ secrets.IMGUR_CLIENT_ID }}\" >> $GITHUB_ENV\n    ;;\n  bluesky)\n    echo \"BLUESKY_IDENTIFIER=${{ secrets.BLUESKY_IDENTIFIER }}\" >> $GITHUB_ENV\n    echo \"BLUESKY_APP_PASSWORD=${{ secrets.BLUESKY_APP_PASSWORD }}\" >> $GITHUB_ENV\n    ;;\n  pixabay)\n    echo \"PIXABAY_API_KEY=${{ secrets.PIXABAY_API_KEY }}\" >> $GITHUB_ENV\n    ;;\n  niche)\n    # Niche platforms may use multiple APIs\n    echo \"LEMMY_INSTANCE_URL=${{ secrets.LEMMY_INSTANCE_URL }}\" >> $GITHUB_ENV\n    echo \"TUMBLR_API_KEY=${{ secrets.TUMBLR_API_KEY }}\" >> $GITHUB_ENV\n    ;;\nesac\n"
            },
            {
              "name": "Scan platform content",
              "id": "scan",
              "run": "echo \"üì° Scanning ${{ matrix.platform }} for hotdog content...\"\n\nMAX_POSTS=\"${{ inputs.max-posts || 50 }}\"\n\ncase \"${{ matrix.platform }}\" in\n  reddit)\n    npm run scan:reddit -- --max-posts=$MAX_POSTS\n    ;;\n  youtube)\n    npm run scan:youtube -- --max-posts=$MAX_POSTS\n    ;;\n  giphy)\n    npm run scan:giphy -- --max-posts=$MAX_POSTS\n    ;;\n  imgur)\n    npm run scan:imgur -- --max-posts=$MAX_POSTS\n    ;;\n  bluesky)\n    npm run scan:bluesky -- --max-posts=$MAX_POSTS\n    ;;\n  tumblr)\n    npm run scan:tumblr -- --max-posts=$MAX_POSTS\n    ;;\n  lemmy)\n    npm run scan:lemmy -- --max-posts=$MAX_POSTS\n    ;;\n  pixabay)\n    npm run scan:pixabay -- --max-posts=$MAX_POSTS\n    ;;\n  niche)\n    # Scan multiple niche platforms with smaller limits\n    npm run scan:niche-platforms -- --max-posts=20\n    ;;\n  *)\n    echo \"‚ùå Unknown platform: ${{ matrix.platform }}\"\n    exit 1\n    ;;\nesac\n\necho \"‚úÖ Scan completed for ${{ matrix.platform }}\"\n"
            },
            {
              "name": "Upload scan logs",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "scan-logs-${{ matrix.platform }}",
                "path": "logs/scan-*.log\nreports/scan-*.json\n",
                "retention-days": 3,
                "if-no-files-found": "ignore"
              }
            }
          ]
        },
        "summary": {
          "name": "Scan Summary",
          "runs-on": "ubuntu-latest",
          "needs": [
            "plan-scans",
            "demand-driven-scan",
            "determine-platforms",
            "scan"
          ],
          "if": "always() && !cancelled()",
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "20",
                "cache-key-suffix": "scanner-summary"
              }
            },
            {
              "name": "Setup Supabase environment",
              "uses": "./.github/actions/setup-supabase-rest",
              "with": {
                "supabase-url": "${{ secrets.SUPABASE_URL }}",
                "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY_V2 != '' && secrets.SUPABASE_SERVICE_ROLE_KEY_V2 || secrets.SUPABASE_SERVICE_ROLE_KEY }}",
                "database-url": "${{ secrets.DATABASE_URL }}"
              }
            },
            {
              "name": "Generate scan summary",
              "run": "echo \"üìä Generating content scan summary...\"\nnpm run scan:summary || echo \"Summary generation completed\"\n"
            },
            {
              "name": "Create GitHub step summary",
              "run": "echo \"## üì° Content Scanner Results\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Trigger:** ${{ github.event_name }}\" >> $GITHUB_STEP_SUMMARY\n\n# Check if demand-driven or legacy\nif [ \"${{ needs.plan-scans.outputs.reason }}\" != \"\" ]; then\n  echo \"**Mode:** Demand-Driven\" >> $GITHUB_STEP_SUMMARY\n  echo \"**Reason:** ${{ needs.plan-scans.outputs.reason }}\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"**Mode:** Legacy/Manual\" >> $GITHUB_STEP_SUMMARY\n  echo \"**Schedule Type:** ${{ needs.determine-platforms.outputs.schedule-type }}\" >> $GITHUB_STEP_SUMMARY\n  echo \"**Platforms:** ${{ needs.determine-platforms.outputs.platforms }}\" >> $GITHUB_STEP_SUMMARY\nfi\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"| Platform | Status | Notes |\" >> $GITHUB_STEP_SUMMARY\necho \"|----------|--------|-------|\" >> $GITHUB_STEP_SUMMARY\n\n# Create a simplified status report\nPLATFORMS=($(echo \"${{ needs.determine-platforms.outputs.platforms }}\" | tr ',' ' '))\nfor platform in \"${PLATFORMS[@]}\"; do\n  STATUS=\"unknown\"\n  # This is simplified - you'd need to extract actual results\n  echo \"| $platform | $STATUS | Scan attempted |\" >> $GITHUB_STEP_SUMMARY\ndone\n\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"üìà **Next Steps:**\" >> $GITHUB_STEP_SUMMARY\necho \"- Content will be queued for review and scheduling\" >> $GITHUB_STEP_SUMMARY\necho \"- Check admin dashboard for new content\" >> $GITHUB_STEP_SUMMARY\n"
            },
            {
              "name": "Upload summary report",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "scan-summary-report",
                "path": "reports/scan-summary.json\nreports/scan-summary.md\n",
                "retention-days": 7,
                "if-no-files-found": "ignore"
              }
            }
          ]
        },
        "rate-limit-check": {
          "name": "Rate Limit Health Check",
          "runs-on": "ubuntu-latest",
          "needs": "scan",
          "if": "always()",
          "timeout-minutes": 5,
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "20",
                "cache-key-suffix": "rate-limit-check"
              }
            },
            {
              "name": "Check API rate limits",
              "run": "echo \"üîç Checking API rate limit status...\"\n\n# Check rate limits for various APIs\necho \"API Rate Limit Status:\" >> rate-limit-report.md\necho \"=====================\" >> rate-limit-report.md\necho \"\" >> rate-limit-report.md\n\n# This would ideally query actual rate limit status\necho \"- Reddit API: $(date)\" >> rate-limit-report.md\necho \"- YouTube API: $(date)\" >> rate-limit-report.md\necho \"- Giphy API: $(date)\" >> rate-limit-report.md\necho \"- Imgur API: $(date)\" >> rate-limit-report.md\necho \"\" >> rate-limit-report.md\necho \"Generated at: $(date)\" >> rate-limit-report.md\n\necho \"‚úÖ Rate limit check completed\"\n"
            },
            {
              "name": "Upload rate limit report",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "rate-limit-report",
                "path": "rate-limit-report.md",
                "retention-days": 1
              }
            }
          ]
        }
      },
      "env": {
        "NODE_ENV": "production",
        "CI": true
      },
      "concurrency": {
        "group": "scanners-${{ github.ref }}",
        "cancel-in-progress": false
      },
      "secrets_refs": [
        "SUPABASE_URL",
        "SUPABASE_SERVICE_ROLE_KEY_V2",
        "DATABASE_URL",
        "REDDIT_CLIENT_ID",
        "REDDIT_CLIENT_SECRET",
        "YOUTUBE_API_KEY",
        "GIPHY_API_KEY",
        "IMGUR_CLIENT_ID",
        "BLUESKY_IDENTIFIER",
        "BLUESKY_APP_PASSWORD",
        "PIXABAY_API_KEY",
        "TUMBLR_API_KEY",
        "LEMMY_INSTANCE_URL"
      ],
      "vars_refs": [
        "SITE_URL",
        "SCAN_MIN_PER_PLATFORM",
        "SCAN_MAX_PER_PLATFORM",
        "SCAN_GLOBAL_MAX",
        "SCAN_COOLDOWN_MIN",
        "MIN_CONF",
        "MIN_CANDIDATES",
        "PLATFORM_ALLOW"
      ],
      "composite_actions": [
        "./.github/actions/setup-node",
        "./.github/actions/setup-supabase-rest"
      ],
      "reusable_workflows": [],
      "job_count": 6
    },
    {
      "path": ".github/workflows/schedule-reconcile.yml",
      "filename": "schedule-reconcile.yml",
      "name": "Schedule Reconciliation",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "30 6 * * *"
            }
          ],
          "cron": [
            "30 6 * * *"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": {
            "inputs": {
              "date": {
                "description": "Target date (YYYY-MM-DD, defaults to yesterday)",
                "required": false,
                "type": "string"
              },
              "force_backfill": {
                "description": "Force backfill even if no orphans found",
                "required": false,
                "type": "boolean",
                "default": false
              }
            }
          }
        }
      ],
      "jobs": {
        "reconcile": {
          "name": "Schedule Reconciliation",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 10,
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js",
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "18",
                "cache": "npm"
              }
            },
            {
              "name": "Install dependencies",
              "run": "npm ci --production"
            },
            {
              "name": "Determine target date",
              "id": "date",
              "run": "if [ -n \"${{ github.event.inputs.date }}\" ]; then\n  TARGET_DATE=\"${{ github.event.inputs.date }}\"\nelse\n  # Default to yesterday in ET timezone\n  TARGET_DATE=$(TZ=America/New_York date -d \"yesterday\" +%Y-%m-%d)\nfi\necho \"target_date=$TARGET_DATE\" >> $GITHUB_OUTPUT\necho \"üóìÔ∏è Target date: $TARGET_DATE\"\n"
            },
            {
              "name": "Health Check - Timezone Handling",
              "id": "health_tz",
              "run": "echo \"üïí Checking timezone health...\"\n\nRESPONSE=$(curl -s -w \"%{http_code}\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  \"${{ vars.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/health/schedule-tz?date=${{ steps.date.outputs.target_date }}\")\n\nHTTP_CODE=\"${RESPONSE: -3}\"\nBODY=\"${RESPONSE%???}\"\n\necho \"Response code: $HTTP_CODE\"\necho \"timezone_health_status=$HTTP_CODE\" >> $GITHUB_OUTPUT\n\nif [ \"$HTTP_CODE\" != \"200\" ]; then\n  echo \"‚ùå Timezone health check failed\"\n  echo \"$BODY\" | jq -r '.issues[]?' || echo \"$BODY\"\n  echo \"timezone_issues=true\" >> $GITHUB_OUTPUT\nelse\n  echo \"‚úÖ Timezone health check passed\"\n  echo \"timezone_issues=false\" >> $GITHUB_OUTPUT\nfi\n"
            },
            {
              "name": "Health Check - Posting Source of Truth",
              "id": "health_posting",
              "run": "echo \"üîç Checking posting source of truth health...\"\n\nRESPONSE=$(curl -s -w \"%{http_code}\" \\\n  -H \"Authorization: Bearer ${{ secrets.AUTH_TOKEN }}\" \\\n  \"${{ vars.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/health/posting-source-of-truth\")\n\nHTTP_CODE=\"${RESPONSE: -3}\"\nBODY=\"${RESPONSE%???}\"\n\necho \"Response code: $HTTP_CODE\"\necho \"posting_health_status=$HTTP_CODE\" >> $GITHUB_OUTPUT\n\nORPHAN_COUNT=$(echo \"$BODY\" | jq -r '.orphan_posts // 0')\nCOMPLIANCE_SCORE=$(echo \"$BODY\" | jq -r '.posting_compliance_score // 0')\n\necho \"orphan_count=$ORPHAN_COUNT\" >> $GITHUB_OUTPUT\necho \"compliance_score=$COMPLIANCE_SCORE\" >> $GITHUB_OUTPUT\n\nif [ \"$HTTP_CODE\" != \"200\" ] || [ \"$ORPHAN_COUNT\" -gt 5 ] || [ \"$COMPLIANCE_SCORE\" -lt 90 ]; then\n  echo \"‚ùå Posting health check indicates issues\"\n  echo \"Orphan posts: $ORPHAN_COUNT, Compliance: $COMPLIANCE_SCORE%\"\n  echo \"$BODY\" | jq -r '.issues[]?' || echo \"$BODY\"\n  echo \"posting_issues=true\" >> $GITHUB_OUTPUT\nelse\n  echo \"‚úÖ Posting health check passed\"\n  echo \"posting_issues=false\" >> $GITHUB_OUTPUT\nfi\n"
            },
            {
              "name": "Run Backfill Job",
              "id": "backfill",
              "if": "steps.health_posting.outputs.orphan_count > 0 || \ngithub.event.inputs.force_backfill == 'true'\n",
              "env": {
                "DATABASE_URL": "${{ secrets.DATABASE_URL }}",
                "SUPABASE_URL": "${{ secrets.SUPABASE_URL }}",
                "SUPABASE_SERVICE_ROLE_KEY": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY_V2 != '' && secrets.SUPABASE_SERVICE_ROLE_KEY_V2 || secrets.SUPABASE_SERVICE_ROLE_KEY }}"
              },
              "run": "echo \"üîß Running backfill job for orphan posts...\"\necho \"Target date: ${{ steps.date.outputs.target_date }}\"\necho \"Orphan count detected: ${{ steps.health_posting.outputs.orphan_count }}\"\n\n# Run backfill job with write mode\nnpx tsx scripts/ops/backfill-post-links.ts \\\n  --date \"${{ steps.date.outputs.target_date }}\" \\\n  --write \\\n  --verbose\n\n# Check if report was generated\nif [ -f \"ci_audit/backfill-${{ steps.date.outputs.target_date }}.md\" ]; then\n  echo \"backfill_report_generated=true\" >> $GITHUB_OUTPUT\n  echo \"‚úÖ Backfill report generated\"\nelse\n  echo \"backfill_report_generated=false\" >> $GITHUB_OUTPUT\n  echo \"‚ö†Ô∏è No backfill report generated\"\nfi\n"
            },
            {
              "name": "Generate Daily Report",
              "id": "report",
              "run": "echo \"üìä Generating daily reconciliation report...\"\n\n# Create comprehensive report\nREPORT_FILE=\"ci_audit/reconcile-${{ steps.date.outputs.target_date }}.md\"\nmkdir -p ci_audit\n\ncat > \"$REPORT_FILE\" << 'EOF'\n# Schedule Reconciliation Report\n\n**Date:** ${{ steps.date.outputs.target_date }}  \n**Generated:** $(date -u +\"%Y-%m-%d %H:%M:%S UTC\")  \n**Workflow:** ${{ github.workflow }} #${{ github.run_number }}  \n\n## Health Check Results\n\n### Timezone Handling\n- **Status:** ${{ steps.health_tz.outputs.timezone_health_status == '200' && '‚úÖ Healthy' || '‚ùå Issues Detected' }}\n- **HTTP Code:** ${{ steps.health_tz.outputs.timezone_health_status }}\n- **Issues Found:** ${{ steps.health_tz.outputs.timezone_issues }}\n\n### Posting Source of Truth\n- **Status:** ${{ steps.health_posting.outputs.posting_health_status == '200' && '‚úÖ Healthy' || '‚ùå Issues Detected' }}\n- **HTTP Code:** ${{ steps.health_posting.outputs.posting_health_status }}\n- **Orphan Posts:** ${{ steps.health_posting.outputs.orphan_count }}\n- **Compliance Score:** ${{ steps.health_posting.outputs.compliance_score }}%\n- **Issues Found:** ${{ steps.health_posting.outputs.posting_issues }}\n\n## Backfill Results\n\n${{ steps.backfill.outcome == 'success' && '‚úÖ Backfill job completed successfully' || steps.backfill.outcome == 'skipped' && '‚è≠Ô∏è Backfill job skipped (no orphan posts)' || '‚ùå Backfill job failed or was not run' }}\n\n${{ steps.backfill.outputs.backfill_report_generated == 'true' && 'üìÑ Detailed backfill report available: `ci_audit/backfill-${{ steps.date.outputs.target_date }}.md`' || '' }}\n\n## Recommendations\n\n${{ steps.health_tz.outputs.timezone_issues == 'true' && '- üïí **Timezone Issues:** Review timezone conversion logic and DST handling' || '' }}\n${{ steps.health_posting.outputs.posting_issues == 'true' && '- üîç **Posting Issues:** Review scheduled_posts table integrity and posting service configuration' || '' }}\n${{ steps.health_posting.outputs.orphan_count > 5 && '- üîß **High Orphan Count:** Consider running backfill job more frequently or investigating posting service' || '' }}\n${{ steps.health_posting.outputs.compliance_score < 90 && '- üìä **Low Compliance:** Enable ENFORCE_SCHEDULE_SOURCE_OF_TRUTH feature flag and review posting workflows' || '' }}\n\n## Next Steps\n\n- Monitor system health endpoints daily\n- Review any generated backfill reports\n- Ensure ENFORCE_SCHEDULE_SOURCE_OF_TRUTH=true in production\n- Consider manual review if compliance score remains low\n\n---\n*Generated by Schedule Reconciliation workflow*\nEOF\n\necho \"report_file=$REPORT_FILE\" >> $GITHUB_OUTPUT\necho \"‚úÖ Daily report generated: $REPORT_FILE\"\n"
            },
            {
              "name": "Upload Reports",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "reconciliation-reports-${{ steps.date.outputs.target_date }}",
                "path": "ci_audit/",
                "retention-days": 30
              }
            },
            {
              "name": "Determine Workflow Status",
              "id": "status",
              "run": "# Determine overall workflow status\nif [[ \"${{ steps.health_tz.outputs.timezone_issues }}\" == \"true\" ]] || \\\n   [[ \"${{ steps.health_posting.outputs.posting_issues }}\" == \"true\" ]] || \\\n   [[ \"${{ steps.backfill.outcome }}\" == \"failure\" ]]; then\n  echo \"workflow_status=failure\" >> $GITHUB_OUTPUT\n  echo \"‚ùå Workflow completed with issues\"\nelse\n  echo \"workflow_status=success\" >> $GITHUB_OUTPUT\n  echo \"‚úÖ Workflow completed successfully\"\nfi\n"
            },
            {
              "name": "Notify on Failure",
              "if": "steps.status.outputs.workflow_status == 'failure'",
              "run": "echo \"üö® RECONCILIATION ISSUES DETECTED\"\necho \"\"\necho \"Date: ${{ steps.date.outputs.target_date }}\"\necho \"Timezone Health: ${{ steps.health_tz.outputs.timezone_issues == 'true' && 'ISSUES' || 'OK' }}\"\necho \"Posting Health: ${{ steps.health_posting.outputs.posting_issues == 'true' && 'ISSUES' || 'OK' }}\"\necho \"Orphan Posts: ${{ steps.health_posting.outputs.orphan_count }}\"\necho \"Compliance Score: ${{ steps.health_posting.outputs.compliance_score }}%\"\necho \"Backfill Status: ${{ steps.backfill.outcome }}\"\necho \"\"\necho \"Please review the generated reports and take corrective action.\"\necho \"Reports available in workflow artifacts.\"\n\n# Exit with failure to mark workflow as failed\nexit 1\n"
            },
            {
              "name": "Success Summary",
              "if": "steps.status.outputs.workflow_status == 'success'",
              "run": "echo \"‚úÖ RECONCILIATION COMPLETED SUCCESSFULLY\"\necho \"\"\necho \"Date: ${{ steps.date.outputs.target_date }}\"\necho \"All health checks passed\"\necho \"Orphan Posts: ${{ steps.health_posting.outputs.orphan_count }}\"\necho \"Compliance Score: ${{ steps.health_posting.outputs.compliance_score }}%\"\necho \"System operating within expected parameters\"\n"
            }
          ]
        }
      },
      "env": {
        "NODE_ENV": "production"
      },
      "secrets_refs": [
        "AUTH_TOKEN",
        "DATABASE_URL",
        "SUPABASE_URL",
        "SUPABASE_SERVICE_ROLE_KEY_V2"
      ],
      "vars_refs": [
        "SITE_URL"
      ],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/scheduler-sla-guard.yml",
      "filename": "scheduler-sla-guard.yml",
      "name": "Scheduler SLA Guard",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "10 10 * * *"
            }
          ],
          "cron": [
            "10 10 * * *"
          ]
        }
      ],
      "jobs": {
        "guard": {
          "permissions": {
            "contents": "read"
          },
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 10,
          "concurrency": {
            "group": "scheduler-sla-${{ github.ref }}",
            "cancel-in-progress": true
          },
          "steps": [
            {
              "uses": "actions/checkout@v4"
            },
            {
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "20",
                "cache": "pnpm"
              }
            },
            {
              "run": "pnpm install --frozen-lockfile || pnpm install"
            },
            {
              "name": "Check SLA (today & tomorrow)",
              "run": "pnpm tsx scripts/ops/assert-schedule-sla.ts --tz America/New_York --today 6 --tomorrow 6"
            },
            {
              "name": "Alert on failure (optional)",
              "if": "failure()",
              "env": {
                "ALERT_WEBHOOK_URL": "${{ secrets.ALERT_WEBHOOK_URL }}"
              },
              "run": "if [ -n \"$ALERT_WEBHOOK_URL\" ]; then\n  node -e \"fetch(process.env.ALERT_WEBHOOK_URL,{method:'POST',headers:{'content-type':'application/json'},body:JSON.stringify({text:'‚ùå Scheduler SLA failed: not enough scheduled rows for today/tomorrow.'})})\"\nelse\n  echo \"No ALERT_WEBHOOK_URL configured.\"\nfi\n"
            }
          ]
        }
      },
      "secrets_refs": [
        "ALERT_WEBHOOK_URL"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/scheduler.yml",
      "filename": "scheduler.yml",
      "name": "Content Scheduler",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 1 * * *"
            },
            {
              "cron": "0 12 * * *"
            },
            {
              "cron": "0 0 * * 0"
            }
          ],
          "cron": [
            "0 1 * * *",
            "0 12 * * *",
            "0 0 * * 0"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": {
            "inputs": {
              "operation": {
                "description": "Operation to perform",
                "type": "choice",
                "options": [
                  "refill",
                  "forecast",
                  "reconcile",
                  "twoDays"
                ],
                "default": "refill"
              },
              "days": {
                "description": "Number of days (for refill/forecast)",
                "type": "number",
                "default": 2
              }
            }
          }
        },
        {
          "event": "workflow_call",
          "config": {
            "inputs": {
              "operation": {
                "description": "Operation to perform",
                "type": "string",
                "default": "refill"
              },
              "days": {
                "description": "Number of days",
                "type": "number",
                "default": 2
              }
            }
          }
        }
      ],
      "jobs": {
        "determine-operation": {
          "name": "Determine Operation",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 2,
          "outputs": {
            "operation": "${{ steps.strategy.outputs.operation }}",
            "days": "${{ steps.strategy.outputs.days }}",
            "auth-token": "${{ steps.strategy.outputs.auth-token }}"
          },
          "steps": [
            {
              "name": "Determine operation strategy",
              "id": "strategy",
              "run": "OPERATION=\"${{ inputs.operation || 'refill' }}\"\nDAYS=\"${{ inputs.days || 2 }}\"\n\n# Determine operation based on schedule\nif [[ \"${{ github.event_name }}\" == \"schedule\" ]]; then\n  HOUR=$(date +%H)\n  \n  if [[ $HOUR -eq 1 ]]; then\n    OPERATION=\"refill\"\n    DAYS=2\n    echo \"üåÖ Morning refill - scheduling next 2 days\"\n  elif [[ $HOUR -eq 12 ]]; then\n    OPERATION=\"forecast\"\n    DAYS=3\n    echo \"üåû Midday forecast check - analyzing next 3 days\"\n  elif [[ $HOUR -eq 0 ]]; then\n    OPERATION=\"reconcile\"\n    DAYS=7\n    echo \"üåô Weekly reconcile - checking past 7 days\"\n  fi\nfi\n\necho \"operation=$OPERATION\" >> $GITHUB_OUTPUT\necho \"days=$DAYS\" >> $GITHUB_OUTPUT\necho \"auth-token=${{ secrets.AUTH_TOKEN }}\" >> $GITHUB_OUTPUT\necho \"Selected operation: $OPERATION for $DAYS days\"\n"
            }
          ]
        },
        "refill": {
          "name": "Refill Schedule",
          "runs-on": "ubuntu-latest",
          "needs": "determine-operation",
          "if": "needs.determine-operation.outputs.operation == 'refill' || needs.determine-operation.outputs.operation == 'twoDays'",
          "timeout-minutes": 10,
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "20",
                "cache-key-suffix": "scheduler-refill"
              }
            },
            {
              "name": "Setup Supabase environment",
              "uses": "./.github/actions/setup-supabase-rest",
              "with": {
                "supabase-url": "${{ secrets.SUPABASE_URL }}",
                "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY_V2 != '' && secrets.SUPABASE_SERVICE_ROLE_KEY_V2 || secrets.SUPABASE_SERVICE_ROLE_KEY }}",
                "database-url": "${{ secrets.DATABASE_URL }}"
              }
            },
            {
              "name": "Refill content schedule",
              "id": "refill",
              "run": "echo \"üîÑ Refilling content schedule for ${{ needs.determine-operation.outputs.days }} days...\"\n\nDAYS=\"${{ needs.determine-operation.outputs.days }}\"\nAUTH_TOKEN=\"${{ needs.determine-operation.outputs.auth-token }}\"\n\n# Call the refill API endpoint\nRESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -X POST \\\n  -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\\\"days\\\": $DAYS}\" \\\n  \"${{ secrets.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/admin/schedule/refill\" \\\n  --max-time 300)\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//')\n\necho \"refill-status=$HTTP_STATUS\" >> $GITHUB_OUTPUT\necho \"refill-response<<EOF\" >> $GITHUB_OUTPUT\necho \"$BODY\" >> $GITHUB_OUTPUT\necho \"EOF\" >> $GITHUB_OUTPUT\n\nif [ \"$HTTP_STATUS\" -eq 200 ]; then\n  echo \"‚úÖ Schedule refill completed successfully\"\nelse\n  echo \"‚ùå Schedule refill failed with status $HTTP_STATUS\"\n  echo \"Response: $BODY\"\n  exit 1\nfi\n"
            },
            {
              "name": "Upload refill logs",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "scheduler-refill-logs",
                "path": "logs/scheduler-*.log\nreports/refill-*.json\n",
                "retention-days": 7,
                "if-no-files-found": "ignore"
              }
            }
          ]
        },
        "forecast": {
          "name": "Generate Forecast",
          "runs-on": "ubuntu-latest",
          "needs": "determine-operation",
          "if": "needs.determine-operation.outputs.operation == 'forecast'",
          "timeout-minutes": 8,
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "20",
                "cache-key-suffix": "scheduler-forecast"
              }
            },
            {
              "name": "Setup Supabase environment",
              "uses": "./.github/actions/setup-supabase-rest",
              "with": {
                "supabase-url": "${{ secrets.SUPABASE_URL }}",
                "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY_V2 != '' && secrets.SUPABASE_SERVICE_ROLE_KEY_V2 || secrets.SUPABASE_SERVICE_ROLE_KEY }}",
                "database-url": "${{ secrets.DATABASE_URL }}"
              }
            },
            {
              "name": "Generate schedule forecast",
              "id": "forecast",
              "run": "echo \"üîÆ Generating schedule forecast for ${{ needs.determine-operation.outputs.days }} days...\"\n\nDAYS=\"${{ needs.determine-operation.outputs.days }}\"\nAUTH_TOKEN=\"${{ needs.determine-operation.outputs.auth-token }}\"\n\n# Get forecast for multiple days\nfor i in $(seq 0 $((DAYS-1))); do\n  DATE=$(date -d \"+$i days\" +%Y-%m-%d)\n  echo \"üìÖ Generating forecast for $DATE...\"\n  \n  RESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n    -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n    \"${{ secrets.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/admin/schedule/forecast?date=$DATE\" \\\n    --max-time 60)\n  \n  HTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\n  BODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//')\n  \n  if [ \"$HTTP_STATUS\" -eq 200 ]; then\n    echo \"‚úÖ Forecast for $DATE generated successfully\"\n    echo \"$BODY\" > \"forecast-$DATE.json\"\n  else\n    echo \"‚ö†Ô∏è Forecast for $DATE failed with status $HTTP_STATUS\"\n    echo \"Response: $BODY\"\n  fi\ndone\n\necho \"‚úÖ Forecast generation completed\"\n"
            },
            {
              "name": "Upload forecast reports",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "scheduler-forecast-reports",
                "path": "forecast-*.json\nlogs/forecast-*.log\n",
                "retention-days": 7,
                "if-no-files-found": "ignore"
              }
            }
          ]
        },
        "reconcile": {
          "name": "Reconcile Content",
          "runs-on": "ubuntu-latest",
          "needs": "determine-operation",
          "if": "needs.determine-operation.outputs.operation == 'reconcile'",
          "timeout-minutes": 15,
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "20",
                "cache-key-suffix": "scheduler-reconcile"
              }
            },
            {
              "name": "Setup Supabase environment",
              "uses": "./.github/actions/setup-supabase-rest",
              "with": {
                "supabase-url": "${{ secrets.SUPABASE_URL }}",
                "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY_V2 != '' && secrets.SUPABASE_SERVICE_ROLE_KEY_V2 || secrets.SUPABASE_SERVICE_ROLE_KEY }}",
                "database-url": "${{ secrets.DATABASE_URL }}"
              }
            },
            {
              "name": "Reconcile content and schedule",
              "id": "reconcile",
              "run": "echo \"üîÑ Reconciling content and schedule for past ${{ needs.determine-operation.outputs.days }} days...\"\n\nDAYS=\"${{ needs.determine-operation.outputs.days }}\"\nAUTH_TOKEN=\"${{ needs.determine-operation.outputs.auth-token }}\"\n\n# Call the reconcile API endpoint\nRESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -X POST \\\n  -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d \"{\\\"days\\\": $DAYS}\" \\\n  \"${{ secrets.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/admin/schedule/reconcile\" \\\n  --max-time 600)\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//')\n\necho \"reconcile-status=$HTTP_STATUS\" >> $GITHUB_OUTPUT\necho \"reconcile-response<<EOF\" >> $GITHUB_OUTPUT\necho \"$BODY\" >> $GITHUB_OUTPUT\necho \"EOF\" >> $GITHUB_OUTPUT\n\nif [ \"$HTTP_STATUS\" -eq 200 ]; then\n  echo \"‚úÖ Content reconciliation completed successfully\"\nelse\n  echo \"‚ùå Content reconciliation failed with status $HTTP_STATUS\"\n  echo \"Response: $BODY\"\n  exit 1\nfi\n"
            },
            {
              "name": "Upload reconcile logs",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "scheduler-reconcile-logs",
                "path": "logs/reconcile-*.log\nreports/reconcile-*.json\n",
                "retention-days": 14,
                "if-no-files-found": "ignore"
              }
            }
          ]
        },
        "queue-health": {
          "name": "Queue Health Check",
          "runs-on": "ubuntu-latest",
          "needs": [
            "determine-operation",
            "refill",
            "forecast",
            "reconcile"
          ],
          "if": "always()",
          "timeout-minutes": 5,
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup Node.js with cache",
              "uses": "./.github/actions/setup-node",
              "with": {
                "node-version": "20",
                "cache-key-suffix": "scheduler-health"
              }
            },
            {
              "name": "Setup Supabase environment",
              "uses": "./.github/actions/setup-supabase-rest",
              "with": {
                "supabase-url": "${{ secrets.SUPABASE_URL }}",
                "supabase-service-key": "${{ secrets.SUPABASE_SERVICE_ROLE_KEY_V2 != '' && secrets.SUPABASE_SERVICE_ROLE_KEY_V2 || secrets.SUPABASE_SERVICE_ROLE_KEY }}",
                "database-url": "${{ secrets.DATABASE_URL }}"
              }
            },
            {
              "name": "Check queue health",
              "run": "echo \"üè• Checking content queue health...\"\n\nAUTH_TOKEN=\"${{ needs.determine-operation.outputs.auth-token }}\"\n\n# Get queue metrics\nRESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n  \"${{ secrets.SITE_URL || 'https://hotdog-diaries.vercel.app' }}/api/admin/metrics\" \\\n  --max-time 30)\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\nBODY=$(echo $RESPONSE | sed -e 's/HTTPSTATUS:.*//')\n\nif [ \"$HTTP_STATUS\" -eq 200 ]; then\n  echo \"‚úÖ Queue health check completed\"\n  echo \"$BODY\" > queue-health.json\nelse\n  echo \"‚ö†Ô∏è Queue health check failed with status $HTTP_STATUS\"\n  echo \"Response: $BODY\"\nfi\n"
            },
            {
              "name": "Upload health report",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "queue-health-report",
                "path": "queue-health.json",
                "retention-days": 3,
                "if-no-files-found": "ignore"
              }
            }
          ]
        },
        "summary": {
          "name": "Scheduler Summary",
          "runs-on": "ubuntu-latest",
          "needs": [
            "determine-operation",
            "refill",
            "forecast",
            "reconcile",
            "queue-health"
          ],
          "if": "always()",
          "steps": [
            {
              "name": "Generate scheduler summary",
              "run": "echo \"## üìÖ Content Scheduler Results\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Operation:** ${{ needs.determine-operation.outputs.operation }}\" >> $GITHUB_STEP_SUMMARY\necho \"**Days:** ${{ needs.determine-operation.outputs.days }}\" >> $GITHUB_STEP_SUMMARY\necho \"**Trigger:** ${{ github.event_name }}\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"| Job | Status | Notes |\" >> $GITHUB_STEP_SUMMARY\necho \"|-----|--------|-------|\" >> $GITHUB_STEP_SUMMARY\n\n# Add job statuses\nif [[ \"${{ needs.refill.result }}\" != \"\" ]]; then\n  echo \"| Refill | ${{ needs.refill.result }} | Schedule refill operation |\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [[ \"${{ needs.forecast.result }}\" != \"\" ]]; then\n  echo \"| Forecast | ${{ needs.forecast.result }} | Schedule forecast generation |\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [[ \"${{ needs.reconcile.result }}\" != \"\" ]]; then\n  echo \"| Reconcile | ${{ needs.reconcile.result }} | Content reconciliation |\" >> $GITHUB_STEP_SUMMARY\nfi\n\necho \"| Queue Health | ${{ needs.queue-health.result }} | Content queue health check |\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Overall status\nOVERALL_SUCCESS=true\n\nif [[ \"${{ needs.refill.result }}\" == \"failure\" ]] || [[ \"${{ needs.forecast.result }}\" == \"failure\" ]] || [[ \"${{ needs.reconcile.result }}\" == \"failure\" ]]; then\n  OVERALL_SUCCESS=false\nfi\n\nif [[ \"$OVERALL_SUCCESS\" == \"true\" ]]; then\n  echo \"## ‚úÖ Scheduler operations completed successfully\" >> $GITHUB_STEP_SUMMARY\nelse\n  echo \"## ‚ùå Some scheduler operations failed\" >> $GITHUB_STEP_SUMMARY\n  echo \"Check individual job logs for details.\" >> $GITHUB_STEP_SUMMARY\nfi\n\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"üìà **Next Steps:**\" >> $GITHUB_STEP_SUMMARY\necho \"- Content is scheduled and ready for posting\" >> $GITHUB_STEP_SUMMARY\necho \"- Check admin dashboard for schedule overview\" >> $GITHUB_STEP_SUMMARY\n"
            }
          ]
        }
      },
      "env": {
        "NODE_ENV": "production",
        "CI": true
      },
      "concurrency": {
        "group": "scheduler-${{ github.ref }}-${{ inputs.operation || 'scheduled' }}",
        "cancel-in-progress": true
      },
      "secrets_refs": [
        "AUTH_TOKEN",
        "SUPABASE_URL",
        "SUPABASE_SERVICE_ROLE_KEY_V2",
        "DATABASE_URL",
        "SITE_URL"
      ],
      "vars_refs": [],
      "composite_actions": [
        "./.github/actions/setup-node",
        "./.github/actions/setup-supabase-rest"
      ],
      "reusable_workflows": [],
      "job_count": 6
    },
    {
      "path": ".github/workflows/secret-validation.yml",
      "filename": "secret-validation.yml",
      "name": "Secret Validation",
      "triggers": [
        {
          "event": "push",
          "config": {
            "branches": [
              "main",
              "develop"
            ]
          }
        },
        {
          "event": "pull_request",
          "config": {
            "branches": [
              "main",
              "develop"
            ]
          }
        },
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 9 * * 1"
            }
          ],
          "cron": [
            "0 9 * * 1"
          ]
        }
      ],
      "jobs": {
        "validate-secrets": {
          "runs-on": "ubuntu-latest",
          "name": "Validate Secret Strength & Environment Variables",
          "steps": [
            {
              "name": "Checkout",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup pnpm",
              "uses": "pnpm/action-setup@v4",
              "with": {
                "version": "10.18.3"
              }
            },
            {
              "name": "Setup Node.js with pnpm",
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "20",
                "cache": "pnpm"
              }
            },
            {
              "name": "Install dependencies",
              "run": "pnpm install --frozen-lockfile"
            },
            {
              "name": "Test JWT runtime minting capability",
              "env": {
                "JWT_SECRET": "${{ secrets.JWT_SECRET }}"
              },
              "run": "echo \"üîê Testing JWT runtime minting capability...\"\n\n# Check if this is a PR context with no secrets access\nif [ \"${{ github.event_name }}\" = \"pull_request\" ] && [ -z \"$JWT_SECRET\" ]; then\n  echo \"‚ö†Ô∏è PR context detected with no JWT_SECRET access\"\n  echo \"‚úÖ Skipping JWT tests for security (secrets not available to fork PRs)\"\n  echo \"Note: JWT validation will run on merge to main branch\"\n  exit 0\nfi\n\nif [ -n \"$JWT_SECRET\" ]; then\n  echo \"‚úÖ JWT_SECRET is configured\"\n  \n  # Test JWT minting with our utility\n  if pnpm -s tsx scripts/ci/lib/jwt.ts mint --ttl 5m --sub test --aud ci --iss hotdog-diaries > /dev/null 2>&1; then\n    echo \"‚úÖ JWT minting works correctly\"\n    \n    # Test decode functionality with improved error handling\n    TOKEN=$(pnpm -s tsx scripts/ci/lib/jwt.ts mint --ttl 5m --sub test --aud ci --iss hotdog-diaries)\n    if [ -n \"$TOKEN\" ]; then\n      if pnpm -s tsx scripts/ci/lib/jwt.ts decode --token \"$TOKEN\" > /dev/null 2>&1; then\n        echo \"‚úÖ JWT decode works correctly\"\n      else\n        echo \"‚ùå JWT decode failed\"\n        echo \"üîç Debug: Testing decode with verbose output...\"\n        pnpm tsx scripts/ci/lib/jwt.ts decode --token \"$TOKEN\" || echo \"Decode debug completed\"\n        exit 1\n      fi\n      \n      # Test verify functionality\n      if pnpm -s tsx scripts/ci/lib/jwt.ts verify --token \"$TOKEN\" > /dev/null 2>&1; then\n        echo \"‚úÖ JWT verify works correctly\"\n        echo \"üéØ JWT auth system is fully operational\"\n      else\n        echo \"‚ùå JWT verify failed\"\n        echo \"üîç Debug: Testing verify with verbose output...\"\n        pnpm tsx scripts/ci/lib/jwt.ts verify --token \"$TOKEN\" || echo \"Verify debug completed\"\n        exit 1\n      fi\n    else\n      echo \"‚ùå JWT minting returned empty token\"\n      exit 1\n    fi\n    \n  else\n    echo \"‚ùå JWT minting failed - check JWT_SECRET format (should be 64+ hex chars)\"\n    exit 1\n  fi\nelse\n  echo \"‚ùå JWT_SECRET not configured - required for runtime auth\"\n  echo \"Note: If this is a PR from a fork, secrets are not available for security\"\n  exit 1\nfi\n"
            },
            {
              "name": "Validate secret strength",
              "env": {
                "JWT_SECRET": "${{ secrets.JWT_SECRET }}",
                "AUTH_TOKEN": "${{ secrets.AUTH_TOKEN }}",
                "CRON_TOKEN": "${{ secrets.CRON_TOKEN }}",
                "ADMIN_PASSWORD": "${{ secrets.ADMIN_PASSWORD }}"
              },
              "run": "echo \"üîç Running secret validation...\"\necho \"Preferred auth method: JWT_SECRET (runtime minting)\"\necho \"Legacy fallback: AUTH_TOKEN\"\npnpm run validate-secrets -- --verbose\n"
            },
            {
              "name": "Validate in strict mode",
              "env": {
                "JWT_SECRET": "${{ secrets.JWT_SECRET }}",
                "AUTH_TOKEN": "${{ secrets.AUTH_TOKEN }}",
                "CRON_TOKEN": "${{ secrets.CRON_TOKEN }}",
                "ADMIN_PASSWORD": "${{ secrets.ADMIN_PASSWORD }}"
              },
              "run": "echo \"üîí Running strict validation (warnings = errors)...\"\npnpm run validate-secrets -- --strict\n"
            },
            {
              "name": "Comment on PR (if failed)",
              "if": "failure() && github.event_name == 'pull_request'",
              "uses": "actions/github-script@v7",
              "with": {
                "script": "github.rest.issues.createComment({\n  issue_number: context.issue.number,\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  body: `## ‚ùå Secret Validation Failed\n  \n  The secret validation check failed. This could be due to:\n  \n  - **JWT_SECRET issues**: Primary auth secret not configured or format invalid (requires 64+ hex chars)\n  - **JWT minting failures**: Runtime token generation not working correctly\n  - **Weak tokens**: Legacy tokens that don't meet security requirements (length < 32, weak patterns)\n  - **Missing environment variables**: Variables used in code but not documented in .env.example\n  - **Format violations**: Invalid hex/base64/alphanumeric formats\n  \n  ### Next Steps\n  \n  1. Check the workflow logs for specific validation errors\n  2. **Priority**: Ensure JWT_SECRET is configured and valid (64+ hex chars)\n  3. Use \\`pnpm run validate-secrets -- --verbose\\` locally to debug\n  4. For JWT issues, use JWT utility: \\`pnpm tsx scripts/ci/lib/jwt.ts mint --ttl 5m\\`\n  5. For legacy token issues, use \\`pnpm run rotate-tokens <TOKEN_NAME>\\` to generate new tokens\n  6. For missing env vars, add them to .env.example\n  \n  ### Auth Migration Note\n  \n  We prefer JWT_SECRET (runtime minting) over static AUTH_TOKEN. Ensure JWT_SECRET is properly configured.\n  \n  ### Security Note\n  \n  This validation ensures all secrets meet security standards. Do not bypass these checks without security team approval.\n  `\n})\n"
              }
            }
          ]
        },
        "environment-completeness": {
          "runs-on": "ubuntu-latest",
          "name": "Environment Variable Completeness Check",
          "steps": [
            {
              "name": "Checkout",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup pnpm",
              "uses": "pnpm/action-setup@v4",
              "with": {
                "version": "10.18.3"
              }
            },
            {
              "name": "Setup Node.js with pnpm",
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "20",
                "cache": "pnpm"
              }
            },
            {
              "name": "Install dependencies",
              "run": "pnpm install --frozen-lockfile"
            },
            {
              "name": "Check environment completeness",
              "run": "echo \"üìã Checking environment variable completeness...\"\n\n# Extract all process.env references from codebase\necho \"Variables referenced in codebase:\"\ngrep -r \"process\\.env\\.\" app/ lib/ components/ scripts/ middleware.* next.config.* 2>/dev/null \\\n  | grep -oE 'process\\.env\\.([A-Z_][A-Z0-9_]*)' \\\n  | sed 's/process\\.env\\.//' \\\n  | sort -u \\\n  | tee /tmp/codebase_vars.txt\n\necho \"\"\necho \"Variables in .env.example:\"\nif [ -f .env.example ]; then\n  grep -E '^[A-Z_][A-Z0-9_]*=' .env.example | cut -d= -f1 | sort | tee /tmp/env_example_vars.txt\nelse\n  echo \"‚ö†Ô∏è .env.example not found\"\n  touch /tmp/env_example_vars.txt\nfi\n\necho \"\"\necho \"Missing from .env.example:\"\ncomm -23 /tmp/codebase_vars.txt /tmp/env_example_vars.txt | tee /tmp/missing_vars.txt\n\n# Filter out system variables that don't need to be in .env.example\nSYSTEM_VARS=\"NODE_ENV PORT PWD PATH HOME USER VERCEL VERCEL_ENV VERCEL_URL VERCEL_REGION GITHUB_ACTIONS GITHUB_SHA GITHUB_REF GITHUB_HEAD_REF GITHUB_EVENT_NAME GITHUB_EVENT_PATH GITHUB_OUTPUT GITHUB_STEP_SUMMARY GITHUB_WORKSPACE GITHUB_REPOSITORY GITHUB_RUN_ID GITHUB_RUN_NUMBER GITHUB_ACTOR CI BUILD_ID NEXT_RUNTIME JWT_KEY_VERSION\"\n\nfor var in $SYSTEM_VARS; do\n  sed -i \"/^$var$/d\" /tmp/missing_vars.txt 2>/dev/null || true\ndone\n\nif [ -s /tmp/missing_vars.txt ]; then\n  echo \"‚ùå Missing variables found:\"\n  cat /tmp/missing_vars.txt\n  echo \"\"\n  echo \"Please add these variables to .env.example with appropriate example values.\"\n  exit 1\nelse\n  echo \"‚úÖ All codebase variables are documented in .env.example\"\nfi\n"
            }
          ]
        },
        "token-strength-check": {
          "runs-on": "ubuntu-latest",
          "name": "Token Strength Verification",
          "if": "github.event_name == 'schedule'",
          "steps": [
            {
              "name": "Checkout",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Setup pnpm",
              "uses": "pnpm/action-setup@v4",
              "with": {
                "version": "10.18.3"
              }
            },
            {
              "name": "Setup Node.js with pnpm",
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "20",
                "cache": "pnpm"
              }
            },
            {
              "name": "Install dependencies",
              "run": "pnpm install --frozen-lockfile"
            },
            {
              "name": "Check token rotation schedule",
              "env": {
                "JWT_SECRET": "${{ secrets.JWT_SECRET }}",
                "AUTH_TOKEN": "${{ secrets.AUTH_TOKEN }}",
                "CRON_TOKEN": "${{ secrets.CRON_TOKEN }}",
                "ADMIN_PASSWORD": "${{ secrets.ADMIN_PASSWORD }}"
              },
              "run": "echo \"üóìÔ∏è Checking token rotation schedule...\"\necho \"Primary auth method: JWT_SECRET (runtime minting)\"\necho \"Legacy fallback: AUTH_TOKEN\"\n\n# Check if any tokens need rotation based on docs/secrets.md\n# This is a simplified check - in production you might want to store\n# rotation dates in a database or separate tracking system\n\nCURRENT_DATE=$(date +%Y-%m-%d)\necho \"Current date: $CURRENT_DATE\"\n\n# Test JWT minting first\nif [ -n \"$JWT_SECRET\" ]; then\n  echo \"‚úÖ JWT_SECRET configured\"\n  if pnpm -s tsx scripts/ci/lib/jwt.ts mint --ttl 1m > /dev/null 2>&1; then\n    echo \"‚úÖ JWT minting operational\"\n  else\n    echo \"‚ùå JWT minting failed - may need JWT_SECRET rotation\"\n  fi\nelse\n  echo \"‚ö†Ô∏è JWT_SECRET not configured - consider migrating from AUTH_TOKEN\"\nfi\n\n# Check docs/secrets.md for last rotation dates\nif [ -f docs/secrets.md ]; then\n  echo \"Last rotation dates from docs/secrets.md:\"\n  grep -E \"JWT_SECRET|AUTH_TOKEN|CRON_TOKEN|ADMIN_PASSWORD\" docs/secrets.md || true\nfi\n\n# Run token validation to ensure current tokens are still strong\npnpm run validate-secrets -- --verbose\n\necho \"üìä Token strength verification completed\"\n"
            },
            {
              "name": "Create rotation issue (if needed)",
              "if": "failure()",
              "uses": "actions/github-script@v7",
              "with": {
                "script": "const title = `üîê Token Rotation Required - ${new Date().toISOString().split('T')[0]}`\nconst body = `## Token Rotation Required\n\nThe weekly secret audit has detected tokens that may need rotation.\n\n### Action Required\n\n1. **Priority**: Ensure JWT_SECRET is configured and operational (primary auth method)\n2. Review token rotation schedule in \\`docs/secrets.md\\`\n3. Identify tokens due for rotation\n4. For JWT_SECRET: Use \\`pnpm tsx scripts/ci/lib/jwt.ts\\` to test minting\n5. For legacy tokens: Use rotation script: \\`pnpm run rotate-tokens <TOKEN_NAME>\\`\n6. Update all storage locations (GitHub Secrets, Vercel, etc.)\n7. Test functionality with new tokens\n8. Update rotation log\n\n### Auth Migration\n\n- **Preferred**: JWT_SECRET (runtime minting with HMAC-SHA256)\n- **Legacy**: AUTH_TOKEN (static tokens, being phased out)\n\nEnsure JWT_SECRET is properly configured before deprecating AUTH_TOKEN.\n\n### Automation\n\nThis issue was created automatically by the secret validation workflow.\n\n/cc @security-team\n`\n\ngithub.rest.issues.create({\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  title: title,\n  body: body,\n  labels: ['security', 'rotation', 'automated']\n})\n"
              }
            }
          ]
        }
      },
      "secrets_refs": [
        "JWT_SECRET",
        "AUTH_TOKEN",
        "CRON_TOKEN",
        "ADMIN_PASSWORD"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 3
    },
    {
      "path": ".github/workflows/spec-drift.yml",
      "filename": "spec-drift.yml",
      "name": "OpenAPI Spec Drift Detection",
      "triggers": [
        {
          "event": "pull_request",
          "config": {
            "paths": [
              "app/api/**/*.ts",
              "docs/openapi.yaml",
              "docs/openapi.ignore.json",
              "scripts/route-inventory.ts"
            ]
          }
        },
        {
          "event": "push",
          "config": {
            "branches": [
              "main"
            ],
            "paths": [
              "app/api/**/*.ts",
              "docs/openapi.yaml",
              "docs/openapi.ignore.json"
            ]
          }
        },
        {
          "event": "workflow_dispatch",
          "config": null
        },
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 6 * * 1"
            }
          ],
          "cron": [
            "0 6 * * 1"
          ]
        }
      ],
      "jobs": {
        "validate-openapi-spec": {
          "name": "Validate OpenAPI Specification",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 5,
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "uses": "pnpm/action-setup@v4"
            },
            {
              "name": "Setup Node.js with pnpm",
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "20",
                "cache": "pnpm"
              }
            },
            {
              "name": "Install dependencies",
              "run": "pnpm install --frozen-lockfile"
            },
            {
              "name": "Install OpenAPI validation tools",
              "run": "pnpm add -g @redocly/cli @apidevtools/swagger-parser\n# Install yq for YAML parsing\nsudo snap install yq || brew install yq || (curl -L https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -o yq && chmod +x yq && sudo mv yq /usr/local/bin/)\n"
            },
            {
              "name": "Validate OpenAPI specification syntax",
              "run": "echo \"üîç Validating OpenAPI specification syntax...\"\n\n# Check if openapi.yaml exists\nif [ ! -f \"docs/openapi.yaml\" ]; then\n  echo \"‚ùå OpenAPI specification not found at docs/openapi.yaml\"\n  exit 1\nfi\n\n# Check OpenAPI version first\nOPENAPI_VERSION=$(yq eval '.openapi' docs/openapi.yaml)\necho \"üìã Detected OpenAPI version: $OPENAPI_VERSION\"\n\n# Validate with Redocly CLI (supports 3.1.0)\necho \"üìã Running Redocly validation...\"\nredocly lint docs/openapi.yaml --skip-rule=no-unused-components\n\n# For OpenAPI 3.1.0, use @apidevtools/swagger-parser which supports it better\nif [[ \"$OPENAPI_VERSION\" == \"3.1\"* ]]; then\n  echo \"üìã Running OpenAPI 3.1.0 compatible validation...\"\n  # Use installed parser directly\n  node -e \"\n    const SwaggerParser = require('@apidevtools/swagger-parser');\n    SwaggerParser.validate('docs/openapi.yaml')\n      .then(() => console.log('‚úÖ OpenAPI 3.1.0 validation passed'))\n      .catch(err => { console.error('‚ùå Validation failed:', err.message); process.exit(1); });\n  \"\nelse\n  echo \"üìã Running swagger-parser validation...\"\n  npx swagger-parser validate docs/openapi.yaml\nfi\n\necho \"‚úÖ OpenAPI specification is valid\"\n"
            },
            {
              "name": "Check specification completeness",
              "run": "echo \"üîç Checking OpenAPI specification completeness...\"\n\n# Basic grep checks for required sections (more reliable than complex yq)\nREQUIRED_STRINGS=(\n  \"^openapi:\"\n  \"^info:\"\n  \"^servers:\"\n  \"^paths:\"\n  \"^components:\"\n  \"  schemas:\"\n  \"  securitySchemes:\"\n  \"AdminToken:\"\n)\n\nfor pattern in \"${REQUIRED_STRINGS[@]}\"; do\n  if ! grep -q \"$pattern\" docs/openapi.yaml; then\n    echo \"‚ùå Missing required pattern: $pattern\"\n    exit 1\n  fi\n  echo \"‚úÖ Found pattern: $pattern\"\ndone\n\n# Check that we have actual paths defined\nPATH_COUNT=$(yq eval '.paths | keys | length' docs/openapi.yaml 2>/dev/null || echo \"0\")\nif [ \"$PATH_COUNT\" -eq 0 ]; then\n  echo \"‚ùå No API paths defined in specification\"\n  exit 1\nfi\necho \"‚úÖ Found $PATH_COUNT API paths in specification\"\n\necho \"‚úÖ OpenAPI specification is complete\"\n"
            }
          ]
        },
        "detect-api-drift": {
          "name": "Detect API Route Drift",
          "runs-on": "ubuntu-latest",
          "needs": "validate-openapi-spec",
          "timeout-minutes": 10,
          "outputs": {
            "drift-detected": "${{ steps.drift-check.outputs.drift-detected }}",
            "missing-routes": "${{ steps.drift-check.outputs.missing-routes }}"
          },
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "uses": "pnpm/action-setup@v4"
            },
            {
              "name": "Setup Node.js with pnpm",
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "20",
                "cache": "pnpm"
              }
            },
            {
              "name": "Install dependencies",
              "run": "pnpm install --frozen-lockfile"
            },
            {
              "name": "Generate current route inventory",
              "run": "echo \"üîç Generating current API route inventory...\"\npnpm tsx scripts/route-inventory.ts\n\n# Show inventory summary\necho \"üìä Current route inventory:\"\ncat docs/api-inventory.json | jq '.totalRoutes, .adminRoutes, .publicRoutes'\n"
            },
            {
              "name": "Load ignore list",
              "id": "ignore-list",
              "run": "echo \"üìã Loading route ignore list...\"\n\nif [ -f \"docs/openapi.ignore.json\" ]; then\n  echo \"‚úÖ Found ignore list with $(jq '.ignoredRoutes | length' docs/openapi.ignore.json) entries\"\nelse\n  echo \"‚ö†Ô∏è No ignore list found, creating empty one\"\n  echo '{\"ignoredRoutes\": [], \"ignorePatterns\": []}' > docs/openapi.ignore.json\nfi\n"
            },
            {
              "name": "Extract routes from OpenAPI spec",
              "run": "echo \"üîç Extracting documented routes from OpenAPI spec...\"\n\n# Extract paths from OpenAPI spec\nyq eval '.paths | keys' docs/openapi.yaml | yq eval '.[]' - > documented-routes.txt\n\n# Prefix with /api for comparison\nsed 's|^|/api|' documented-routes.txt > documented-api-routes.txt\n\necho \"üìã Documented routes:\"\ncat documented-api-routes.txt\n"
            },
            {
              "name": "Check for drift",
              "id": "drift-check",
              "run": "echo \"üîç Checking for API route drift...\"\n\n# Extract actual routes from inventory\njq -r '.routes[].path' docs/api-inventory.json > actual-routes.txt\n\n# Extract ignored routes (exact matches)\njq -r '.ignoredRoutes[].path' docs/openapi.ignore.json > ignored-routes.txt\n\n# Extract ignore patterns\njq -r '.ignorePatterns[].pattern' docs/openapi.ignore.json > ignore-patterns.txt\n\n# Function to check if route matches any ignore pattern\nis_ignored() {\n  local route=\"$1\"\n  \n  # Check exact matches\n  if grep -Fxq \"$route\" ignored-routes.txt 2>/dev/null; then\n    return 0\n  fi\n  \n  # Check patterns\n  while IFS= read -r pattern; do\n    if [[ \"$route\" =~ $pattern ]]; then\n      return 0\n    fi\n  done < ignore-patterns.txt 2>/dev/null\n  \n  return 1\n}\n\n# Find missing routes (in code but not in spec and not ignored)\nMISSING_ROUTES=()\n\nwhile IFS= read -r route; do\n  # Skip empty lines\n  [ -z \"$route\" ] && continue\n  \n  # Check if route is documented or ignored\n  if ! grep -Fxq \"$route\" documented-api-routes.txt && ! is_ignored \"$route\"; then\n    MISSING_ROUTES+=(\"$route\")\n    echo \"‚ùå Missing from OpenAPI spec: $route\"\n  fi\ndone < actual-routes.txt\n\n# Check results\nif [ ${#MISSING_ROUTES[@]} -eq 0 ]; then\n  echo \"‚úÖ No API drift detected - all routes are documented or ignored\"\n  echo \"drift-detected=false\" >> $GITHUB_OUTPUT\n  echo \"missing-routes=\" >> $GITHUB_OUTPUT\nelse\n  echo \"‚ö†Ô∏è API drift detected! ${#MISSING_ROUTES[@]} undocumented routes found\"\n  \n  # Format missing routes for output\n  MISSING_JSON=$(printf '%s\\n' \"${MISSING_ROUTES[@]}\" | jq -R . | jq -s .)\n  echo \"drift-detected=true\" >> $GITHUB_OUTPUT\n  echo \"missing-routes=$MISSING_JSON\" >> $GITHUB_OUTPUT\n  \n  # Generate suggestions\n  echo \"\"\n  echo \"üîß To fix this drift, either:\"\n  echo \"1. Add the missing routes to docs/openapi.yaml, or\"\n  echo \"2. Add them to docs/openapi.ignore.json with a reason\"\n  echo \"\"\n  echo \"Missing routes:\"\n  printf '  - %s\\n' \"${MISSING_ROUTES[@]}\"\nfi\n"
            },
            {
              "name": "Upload drift report",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "api-drift-report",
                "path": "docs/api-inventory.json\ndocumented-routes.txt\nactual-routes.txt\nignored-routes.txt\n",
                "retention-days": 14
              }
            }
          ]
        },
        "generate-drift-summary": {
          "name": "Generate Drift Summary",
          "runs-on": "ubuntu-latest",
          "needs": "detect-api-drift",
          "if": "always()",
          "steps": [
            {
              "name": "Generate summary",
              "run": "echo \"## üìã OpenAPI Spec Drift Detection\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\nif [ \"${{ needs.detect-api-drift.outputs.drift-detected }}\" = \"true\" ]; then\n  echo \"### ‚ö†Ô∏è Drift Detected\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"The following API routes exist in code but are not documented in the OpenAPI specification:\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  \n  # Parse missing routes JSON\n  MISSING_ROUTES='${{ needs.detect-api-drift.outputs.missing-routes }}'\n  echo \"$MISSING_ROUTES\" | jq -r '.[]' | while read -r route; do\n    echo \"- \\`$route\\`\" >> $GITHUB_STEP_SUMMARY\n  done\n  \n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"### üîß Resolution Options\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"1. **Document the routes**: Add them to \\`docs/openapi.yaml\\` with proper schemas\" >> $GITHUB_STEP_SUMMARY\n  echo \"2. **Ignore the routes**: Add them to \\`docs/openapi.ignore.json\\` with clear reasons\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"### üìñ Documentation\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"- [OpenAPI Specification](./docs/openapi.yaml)\" >> $GITHUB_STEP_SUMMARY\n  echo \"- [Ignore List](./docs/openapi.ignore.json)\" >> $GITHUB_STEP_SUMMARY\n  echo \"- [Route Inventory Script](./scripts/route-inventory.ts)\" >> $GITHUB_STEP_SUMMARY\n  \nelse\n  echo \"### ‚úÖ No Drift Detected\" >> $GITHUB_STEP_SUMMARY\n  echo \"\" >> $GITHUB_STEP_SUMMARY\n  echo \"All API routes are properly documented or ignored. The OpenAPI specification is up to date!\" >> $GITHUB_STEP_SUMMARY\nfi\n\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Workflow**: \\`spec-drift.yml\\`\" >> $GITHUB_STEP_SUMMARY\necho \"**Timestamp**: $(date -Iseconds)\" >> $GITHUB_STEP_SUMMARY\n"
            }
          ]
        },
        "auto-refresh-spec": {
          "name": "Auto-refresh OpenAPI Spec (PR only)",
          "runs-on": "ubuntu-latest",
          "needs": "detect-api-drift",
          "if": "github.event_name == 'pull_request' && needs.detect-api-drift.outputs.drift-detected == 'true'",
          "steps": [
            {
              "name": "Checkout code",
              "uses": "actions/checkout@v4",
              "with": {
                "token": "${{ secrets.GITHUB_TOKEN }}",
                "fetch-depth": 0
              }
            },
            {
              "uses": "pnpm/action-setup@v4"
            },
            {
              "name": "Setup Node.js with pnpm",
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "20",
                "cache": "pnpm"
              }
            },
            {
              "name": "Install dependencies",
              "run": "pnpm install --frozen-lockfile"
            },
            {
              "name": "Auto-refresh OpenAPI spec",
              "run": "echo \"üîÑ Auto-refreshing OpenAPI spec to resolve drift...\"\n\n# Check if openapi.json exists\nif [ ! -f \"docs/openapi.json\" ]; then\n  echo \"üìù Creating initial openapi.json from yaml...\"\n  # Convert YAML to JSON if needed\n  if [ -f \"docs/openapi.yaml\" ]; then\n    yq eval '.' docs/openapi.yaml > docs/openapi.json\n  else\n    echo \"‚ùå No OpenAPI spec found to refresh\"\n    exit 1\n  fi\nfi\n\n# Generate fresh route inventory\necho \"üìä Generating fresh route inventory...\"\npnpm tsx scripts/route-inventory.ts\n\n# Auto-refresh the spec based on current routes\necho \"üîÑ Refreshing OpenAPI spec with current routes...\"\nif [ -f \"scripts/openapi/refresh.ts\" ]; then\n  pnpm tsx scripts/openapi/refresh.ts --auto-commit\nelif [ -f \"scripts/openapi/export.ts\" ]; then\n  # Fallback to export if refresh doesn't exist\n  pnpm tsx scripts/openapi/export.ts --out docs/openapi.json --format json\nelse\n  echo \"‚ö†Ô∏è No OpenAPI refresh script found - updating spec format only\"\n  # Ensure JSON format is consistent\n  yq eval '.' docs/openapi.yaml > docs/openapi.json.tmp\n  mv docs/openapi.json.tmp docs/openapi.json\nfi\n"
            },
            {
              "name": "Commit refreshed spec",
              "run": "# Check if there are changes to commit\nif git diff --quiet docs/openapi.json docs/api-inventory.json; then\n  echo \"‚úÖ No changes needed - OpenAPI spec is current\"\nelse\n  echo \"üìù Committing refreshed OpenAPI spec...\"\n  git config user.name \"github-actions[bot]\"\n  git config user.email \"github-actions[bot]@users.noreply.github.com\"\n  \n  # Add both the refreshed spec and updated inventory\n  git add docs/openapi.json docs/api-inventory.json\n  \n  # Create commit with details about what was refreshed\n  CHANGED_ROUTES=$(git diff --name-only HEAD~1 -- 'app/api/**/*.ts' | wc -l)\n  git commit -m \"fix: auto-refresh OpenAPI spec to resolve drift\n  \n  üîÑ Refreshed OpenAPI specification to match current API routes\n  üìä Updated route inventory with $CHANGED_ROUTES changed route files\n  ü§ñ Auto-committed by spec-drift workflow\n  \n  Co-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>\"\n  \n  git push\n  echo \"‚úÖ OpenAPI spec refreshed and committed to PR\"\n  echo \"üìã This will automatically resolve the detected drift\"\nfi\n"
            }
          ]
        }
      },
      "env": {
        "NODE_ENV": "test",
        "CI": true
      },
      "concurrency": {
        "group": "spec-drift-${{ github.ref }}",
        "cancel-in-progress": true
      },
      "secrets_refs": [
        "GITHUB_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 4
    },
    {
      "path": ".github/workflows/token-refresh.yml",
      "filename": "token-refresh.yml",
      "name": "Token Refresh (Reusable)",
      "triggers": [
        {
          "event": "workflow_call",
          "config": {
            "outputs": {
              "auth_token": {
                "description": "Refreshed authentication token",
                "value": "${{ jobs.refresh.outputs.auth_token }}"
              }
            },
            "secrets": {
              "SITE_URL": {
                "required": true
              },
              "SERVICE_ACCOUNT_SECRET": {
                "required": false
              },
              "REFRESH_TOKEN": {
                "required": false
              },
              "AUTH_TOKEN": {
                "required": false
              }
            }
          }
        }
      ],
      "jobs": {
        "refresh": {
          "runs-on": "ubuntu-latest",
          "outputs": {
            "auth_token": "${{ steps.refresh.outputs.auth_token }}"
          },
          "steps": [
            {
              "name": "Checkout repository",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "Install jq",
              "run": "sudo apt-get update\nsudo apt-get install -y jq\n"
            },
            {
              "name": "Refresh Authentication Token",
              "id": "refresh",
              "env": {
                "SITE_URL": "${{ secrets.SITE_URL }}",
                "SERVICE_ACCOUNT_SECRET": "${{ secrets.SERVICE_ACCOUNT_SECRET }}",
                "REFRESH_TOKEN": "${{ secrets.REFRESH_TOKEN }}",
                "AUTH_TOKEN": "${{ secrets.AUTH_TOKEN }}"
              },
              "run": "echo \"üîê Starting token refresh process...\"\n\n# Initialize variables\nACCESS_TOKEN=\"\"\nTOKEN_REFRESHED=false\n\n# Try service account first\nif [ -n \"$SERVICE_ACCOUNT_SECRET\" ]; then\n  echo \"Attempting service account authentication...\"\n  RESPONSE=$(curl -s -X POST \"$SITE_URL/api/auth/refresh\" \\\n    -H \"Content-Type: application/json\" \\\n    -d \"{\\\"serviceAccount\\\": \\\"$SERVICE_ACCOUNT_SECRET\\\"}\")\n  \n  ACCESS_TOKEN=$(echo \"$RESPONSE\" | jq -r '.accessToken // empty')\n  \n  if [ -n \"$ACCESS_TOKEN\" ]; then\n    echo \"‚úÖ Service account token obtained\"\n    TOKEN_REFRESHED=true\n  else\n    echo \"‚ö†Ô∏è Service account authentication failed\"\n  fi\nfi\n\n# Try refresh token if service account failed\nif [ \"$TOKEN_REFRESHED\" = false ] && [ -n \"$REFRESH_TOKEN\" ]; then\n  echo \"Attempting refresh token authentication...\"\n  RESPONSE=$(curl -s -X POST \"$SITE_URL/api/auth/refresh\" \\\n    -H \"Content-Type: application/json\" \\\n    -d \"{\\\"refreshToken\\\": \\\"$REFRESH_TOKEN\\\"}\")\n  \n  ACCESS_TOKEN=$(echo \"$RESPONSE\" | jq -r '.accessToken // empty')\n  NEW_REFRESH_TOKEN=$(echo \"$RESPONSE\" | jq -r '.refreshToken // empty')\n  \n  if [ -n \"$ACCESS_TOKEN\" ]; then\n    echo \"‚úÖ Refresh token authentication successful\"\n    TOKEN_REFRESHED=true\n    \n    # Store new refresh token if provided\n    if [ -n \"$NEW_REFRESH_TOKEN\" ]; then\n      echo \"üìù New refresh token received (update GitHub secret manually)\"\n    fi\n  else\n    echo \"‚ö†Ô∏è Refresh token authentication failed\"\n  fi\nfi\n\n# Validate existing token as last resort\nif [ \"$TOKEN_REFRESHED\" = false ] && [ -n \"$AUTH_TOKEN\" ]; then\n  echo \"Testing existing AUTH_TOKEN...\"\n  HTTP_CODE=$(curl -s -o /dev/null -w \"%{http_code}\" \\\n    -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n    \"$SITE_URL/api/admin/queue/status\")\n  \n  if [ \"$HTTP_CODE\" = \"200\" ]; then\n    echo \"‚úÖ Existing token is still valid\"\n    ACCESS_TOKEN=\"$AUTH_TOKEN\"\n    TOKEN_REFRESHED=true\n  else\n    echo \"‚ö†Ô∏è Existing token is invalid (HTTP $HTTP_CODE)\"\n  fi\nfi\n\n# Output final status\nif [ \"$TOKEN_REFRESHED\" = true ]; then\n  echo \"‚úÖ Authentication successful\"\n  echo \"::add-mask::$ACCESS_TOKEN\"\n  echo \"auth_token=$ACCESS_TOKEN\" >> $GITHUB_OUTPUT\nelse\n  echo \"‚ùå All authentication methods failed\"\n  echo \"::error::Token refresh failed - manual intervention required\"\n  exit 1\nfi\n"
            },
            {
              "name": "Verify Token",
              "if": "success()",
              "env": {
                "SITE_URL": "${{ secrets.SITE_URL }}",
                "AUTH_TOKEN": "${{ steps.refresh.outputs.auth_token }}"
              },
              "run": "echo \"üîç Verifying refreshed token...\"\nHTTP_CODE=$(curl -s -o /dev/null -w \"%{http_code}\" \\\n  -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n  \"$SITE_URL/api/admin/queue/status\")\n\nif [ \"$HTTP_CODE\" = \"200\" ]; then\n  echo \"‚úÖ Token verification successful\"\nelse\n  echo \"‚ùå Token verification failed (HTTP $HTTP_CODE)\"\n  exit 1\nfi\n"
            }
          ]
        }
      },
      "secrets_refs": [
        "SITE_URL",
        "SERVICE_ACCOUNT_SECRET",
        "REFRESH_TOKEN",
        "AUTH_TOKEN"
      ],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    },
    {
      "path": ".github/workflows/weekly-smoke-test.yml",
      "filename": "weekly-smoke-test.yml",
      "name": "üî• Weekly Smoke Test",
      "triggers": [
        {
          "event": "schedule",
          "config": [
            {
              "cron": "0 8 * * 1"
            }
          ],
          "cron": [
            "0 8 * * 1"
          ]
        },
        {
          "event": "workflow_dispatch",
          "config": {
            "inputs": {
              "target_environment": {
                "description": "Target environment to test",
                "required": false,
                "type": "choice",
                "options": [
                  "production",
                  "development"
                ],
                "default": "production"
              },
              "include_load_tests": {
                "description": "Include load testing",
                "required": false,
                "type": "boolean",
                "default": false
              }
            }
          }
        }
      ],
      "jobs": {
        "weekly-smoke-test": {
          "name": "üè• Production Health Check",
          "runs-on": "ubuntu-latest",
          "timeout-minutes": 15,
          "permissions": {
            "contents": "read",
            "actions": "write",
            "issues": "write"
          },
          "steps": [
            {
              "name": "üì• Checkout code",
              "uses": "actions/checkout@v4"
            },
            {
              "name": "üìã Setup Node.js",
              "uses": "actions/setup-node@v4",
              "with": {
                "node-version": "18",
                "cache": "npm"
              }
            },
            {
              "name": "üì¶ Install dependencies",
              "run": "npm ci\n\n# Install additional system dependencies for smoke tests\nsudo apt-get update\nsudo apt-get install -y bc jq curl\n"
            },
            {
              "name": "üîß Configure test environment",
              "run": "# Determine target URL\nif [ \"${{ github.event.inputs.target_environment }}\" = \"development\" ]; then\n  echo \"BASE_URL_OVERRIDE=${{ env.DEVELOPMENT_URL }}\" >> $GITHUB_ENV\n  echo \"üß™ Testing development environment\"\nelse\n  echo \"BASE_URL_OVERRIDE=${{ env.PRODUCTION_URL }}\" >> $GITHUB_ENV\n  echo \"üåê Testing production environment\"\nfi\n\n# Configure load testing\nif [ \"${{ github.event.inputs.include_load_tests }}\" = \"true\" ]; then\n  echo \"LOAD_TEST_FLAG=--load-test\" >> $GITHUB_ENV\n  echo \"üöÄ Load testing enabled\"\nelse\n  echo \"LOAD_TEST_FLAG=\" >> $GITHUB_ENV\n  echo \"üìä Standard testing only\"\nfi\n"
            },
            {
              "name": "üî• Run smoke tests",
              "id": "smoke_tests",
              "run": "echo \"üî• Starting comprehensive smoke tests...\"\necho \"Target: $BASE_URL_OVERRIDE\"\necho \"\"\n\n# Make smoke test script executable\nchmod +x scripts/smoke.sh\n\n# Create results directory\nmkdir -p test-results\n\n# Run smoke tests with JSON output for parsing\nset +e  # Don't exit on failure, we want to capture results\n\n# Run public endpoint tests (no auth required)\necho \"üìä Testing public endpoints...\"\nif scripts/smoke.sh --json $LOAD_TEST_FLAG > test-results/smoke-results.json; then\n  SMOKE_EXIT_CODE=0\n  echo \"‚úÖ Public smoke tests passed\"\nelse\n  SMOKE_EXIT_CODE=$?\n  echo \"‚ùå Public smoke tests failed (exit code: $SMOKE_EXIT_CODE)\"\nfi\n\n# Also run with human-readable output for logs\necho \"\"\necho \"üìã Detailed test results:\"\nscripts/smoke.sh $LOAD_TEST_FLAG || true\n\n# Save exit code for later steps\necho \"SMOKE_EXIT_CODE=$SMOKE_EXIT_CODE\" >> $GITHUB_ENV\n\n# Parse results if JSON exists\nif [ -f \"test-results/smoke-results.json\" ]; then\n  echo \"üìä Parsing test results...\"\n  \n  TOTAL_TESTS=$(jq -r '.summary.total // 0' test-results/smoke-results.json)\n  PASSED_TESTS=$(jq -r '.summary.passed // 0' test-results/smoke-results.json)\n  FAILED_TESTS=$(jq -r '.summary.failed // 0' test-results/smoke-results.json)\n  SUCCESS_RATE=$(jq -r '.summary.success_rate // 0' test-results/smoke-results.json)\n  \n  echo \"TOTAL_TESTS=$TOTAL_TESTS\" >> $GITHUB_ENV\n  echo \"PASSED_TESTS=$PASSED_TESTS\" >> $GITHUB_ENV\n  echo \"FAILED_TESTS=$FAILED_TESTS\" >> $GITHUB_ENV\n  echo \"SUCCESS_RATE=$SUCCESS_RATE\" >> $GITHUB_ENV\n  \n  echo \"üìà Test Summary:\"\n  echo \"  Total: $TOTAL_TESTS\"\n  echo \"  Passed: $PASSED_TESTS\"\n  echo \"  Failed: $FAILED_TESTS\"\n  echo \"  Success Rate: $SUCCESS_RATE%\"\nfi\n"
            },
            {
              "name": "üè• Quick admin health check",
              "id": "admin_health",
              "if": "always()",
              "run": "echo \"üè• Running quick admin health check...\"\n\n# Make poke-admin script executable\nchmod +x scripts/poke-admin.sh\n\n# Run admin health check (will skip auth tests without token)\nset +e\nif scripts/poke-admin.sh > test-results/admin-health.log 2>&1; then\n  ADMIN_HEALTH_CODE=0\n  echo \"‚úÖ Admin health check passed\"\nelse\n  ADMIN_HEALTH_CODE=$?\n  echo \"‚ùå Admin health check failed (exit code: $ADMIN_HEALTH_CODE)\"\nfi\n\necho \"ADMIN_HEALTH_CODE=$ADMIN_HEALTH_CODE\" >> $GITHUB_ENV\n\necho \"üìã Admin health results:\"\ncat test-results/admin-health.log\n"
            },
            {
              "name": "üìä Generate test report",
              "if": "always()",
              "run": "echo \"üìä Generating comprehensive test report...\"\n\n# Create report file\nREPORT_FILE=\"test-results/weekly-smoke-report.md\"\n\ncat > \"$REPORT_FILE\" << EOF\n# üî• Weekly Smoke Test Report\n\n**Date:** $(date -u -Iseconds)  \n**Environment:** $BASE_URL_OVERRIDE  \n**Triggered by:** ${{ github.event_name }}  \n**Workflow Run:** [\\#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})  \n\n## üìà Test Results Summary\n\n| Metric | Value | Status |\n|--------|-------|--------|\n| **Total Tests** | ${TOTAL_TESTS:-N/A} | - |\n| **Passed Tests** | ${PASSED_TESTS:-N/A} | $([ \"${PASSED_TESTS:-0}\" -eq \"${TOTAL_TESTS:-1}\" ] && echo \"‚úÖ\" || echo \"‚ùå\") |\n| **Failed Tests** | ${FAILED_TESTS:-N/A} | $([ \"${FAILED_TESTS:-1}\" -eq \"0\" ] && echo \"‚úÖ\" || echo \"‚ùå\") |\n| **Success Rate** | ${SUCCESS_RATE:-N/A}% | $([ \"${SUCCESS_RATE:-0}\" = \"100\" ] && echo \"‚úÖ\" || echo \"‚ùå\") |\n| **Admin Health** | Exit Code ${ADMIN_HEALTH_CODE:-N/A} | $([ \"${ADMIN_HEALTH_CODE:-1}\" -eq \"0\" ] && echo \"‚úÖ\" || echo \"‚ùå\") |\n\n## üéØ Overall Status\n\nEOF\n\n# Determine overall status\nif [ \"${SMOKE_EXIT_CODE:-1}\" -eq \"0\" ] && [ \"${ADMIN_HEALTH_CODE:-1}\" -eq \"0\" ]; then\n  echo \"**üéâ ALL TESTS PASSED** - System is healthy\" >> \"$REPORT_FILE\"\n  echo \"OVERALL_STATUS=success\" >> $GITHUB_ENV\nelse\n  echo \"**üö® TESTS FAILED** - Issues detected that require attention\" >> \"$REPORT_FILE\"\n  echo \"OVERALL_STATUS=failure\" >> $GITHUB_ENV\nfi\n\ncat >> \"$REPORT_FILE\" << EOF\n\n## üìã Detailed Results\n\n### üî• Smoke Test Details\n\\`\\`\\`json\n$(cat test-results/smoke-results.json 2>/dev/null || echo '{\"error\": \"No smoke test results available\"}')\n\\`\\`\\`\n\n### üè• Admin Health Check\n\\`\\`\\`\n$(cat test-results/admin-health.log 2>/dev/null || echo \"No admin health results available\")\n\\`\\`\\`\n\n## üîó Quick Links\n\n- [Production Site](${{ env.PRODUCTION_URL }})\n- [Admin Dashboard](${{ env.PRODUCTION_URL }}/admin)\n- [API Documentation](${{ env.PRODUCTION_URL }}/admin/docs)\n- [System Metrics](${{ env.PRODUCTION_URL }}/api/system/metrics)\n\n---\n\nGenerated by [Weekly Smoke Test Workflow](${{ github.server_url }}/${{ github.repository }}/actions/workflows/weekly-smoke-test.yml)\nEOF\n\necho \"‚úÖ Test report generated\"\n"
            },
            {
              "name": "üì§ Upload test results",
              "if": "always()",
              "uses": "actions/upload-artifact@v4",
              "with": {
                "name": "weekly-smoke-test-${{ github.run_number }}",
                "path": "test-results/\n",
                "retention-days": 30,
                "compression-level": 6
              }
            },
            {
              "name": "üìù Add to job summary",
              "if": "always()",
              "run": "echo \"## üî• Weekly Smoke Test Summary\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\n\n# Add report content to job summary\ncat test-results/weekly-smoke-report.md >> $GITHUB_STEP_SUMMARY\n"
            },
            {
              "name": "üö® Create issue on failure",
              "if": "failure() && github.event_name == 'schedule'",
              "uses": "actions/github-script@v7",
              "with": {
                "script": "const issueTitle = `üö® Weekly Smoke Test Failed - ${new Date().toISOString().split('T')[0]}`;\nconst issueBody = `# Weekly Smoke Test Failure Report\n\nThe weekly smoke test failed on ${new Date().toISOString()}.\n\n**Workflow Run:** [#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})\n**Environment:** ${{ env.BASE_URL_OVERRIDE }}\n\n## Failure Details\n\n- **Smoke Tests Exit Code:** ${{ env.SMOKE_EXIT_CODE }}\n- **Admin Health Exit Code:** ${{ env.ADMIN_HEALTH_CODE }}\n- **Overall Status:** ${{ env.OVERALL_STATUS }}\n\n## Next Steps\n\n1. üîç Review the [workflow logs](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})\n2. üè• Check the [admin dashboard](${{ env.PRODUCTION_URL }}/admin)\n3. üìä Review [system metrics](${{ env.PRODUCTION_URL }}/api/system/metrics)\n4. üìã Consult the [SRE runbook](./docs/runbook.md) for troubleshooting\n\n## Auto-Generated Actions\n\n- [ ] Investigate root cause\n- [ ] Apply fixes if needed\n- [ ] Re-run smoke tests to verify\n- [ ] Close this issue when resolved\n\n---\n\nThis issue was automatically created by the Weekly Smoke Test workflow.\n`;\n\nawait github.rest.issues.create({\n  owner: context.repo.owner,\n  repo: context.repo.repo,\n  title: issueTitle,\n  body: issueBody,\n  labels: ['bug', 'monitoring', 'auto-created']\n});\n"
              }
            },
            {
              "name": "‚úÖ Success notification",
              "if": "success()",
              "run": "echo \"üéâ Weekly smoke test completed successfully!\"\necho \"\"\necho \"üìä Results summary:\"\necho \"  - Total tests: ${TOTAL_TESTS:-N/A}\"\necho \"  - Passed: ${PASSED_TESTS:-N/A}\"\necho \"  - Failed: ${FAILED_TESTS:-N/A}\"\necho \"  - Success rate: ${SUCCESS_RATE:-N/A}%\"\necho \"  - Admin health: $([ \"${ADMIN_HEALTH_CODE:-1}\" -eq \"0\" ] && echo \"‚úÖ Healthy\" || echo \"‚ùå Issues\")\"\necho \"\"\necho \"üåê System is healthy and operating normally.\"\n"
            }
          ]
        }
      },
      "env": {
        "PRODUCTION_URL": "https://hotdog-diaries.vercel.app",
        "DEVELOPMENT_URL": "http://localhost:3000"
      },
      "secrets_refs": [],
      "vars_refs": [],
      "composite_actions": [],
      "reusable_workflows": [],
      "job_count": 1
    }
  ],
  "composite_actions": [
    {
      "path": ".github/actions/cache-pnpm/action.yml",
      "name": "Cache PNPM Dependencies",
      "description": "Advanced caching for PNPM with fallback strategies",
      "inputs": {
        "cache-key-prefix": {
          "description": "Cache key prefix",
          "required": false,
          "default": "pnpm"
        },
        "working-directory": {
          "description": "Working directory",
          "required": false,
          "default": "."
        },
        "cache-strategy": {
          "description": "Cache strategy: aggressive, conservative, or disabled",
          "required": false,
          "default": "aggressive"
        }
      },
      "outputs": {
        "cache-hit": {
          "description": "Whether cache was hit",
          "value": "${{ steps.cache.outputs.cache-hit }}"
        },
        "cache-key": {
          "description": "Cache key used",
          "value": "${{ steps.cache.outputs.cache-primary-key }}"
        }
      },
      "runs": {
        "using": "composite",
        "steps": [
          {
            "name": "Get PNPM store directory",
            "id": "pnpm-cache",
            "shell": "bash",
            "working-directory": "${{ inputs.working-directory }}",
            "run": "if command -v pnpm >/dev/null 2>&1; then\n  echo \"store-path=$(pnpm store path)\" >> $GITHUB_OUTPUT\n  echo \"lock-hash=$(sha256sum pnpm-lock.yaml | cut -d' ' -f1)\" >> $GITHUB_OUTPUT\nelse\n  echo \"store-path=~/.pnpm-store\" >> $GITHUB_OUTPUT\n  echo \"lock-hash=no-lockfile\" >> $GITHUB_OUTPUT\nfi\n"
          },
          {
            "name": "Cache PNPM dependencies",
            "id": "cache",
            "if": "inputs.cache-strategy != 'disabled'",
            "uses": "actions/cache@v3",
            "with": {
              "path": "${{ steps.pnpm-cache.outputs.store-path }}\n${{ inputs.working-directory }}/node_modules\n~/.pnpm\n",
              "key": "${{ inputs.cache-key-prefix }}-${{ runner.os }}-${{ steps.pnpm-cache.outputs.lock-hash }}",
              "restore-keys": "${{ inputs.cache-key-prefix }}-${{ runner.os }}-\n${{ inputs.cache-key-prefix }}-\n"
            }
          },
          {
            "name": "Conservative cache fallback",
            "id": "conservative-cache",
            "if": "inputs.cache-strategy == 'conservative' && steps.cache.outputs.cache-hit != 'true'",
            "uses": "actions/cache@v3",
            "with": {
              "path": "${{ steps.pnpm-cache.outputs.store-path }}",
              "key": "${{ inputs.cache-key-prefix }}-store-${{ runner.os }}-${{ github.sha }}",
              "restore-keys": "${{ inputs.cache-key-prefix }}-store-${{ runner.os }}-\n"
            }
          },
          {
            "name": "Setup PNPM",
            "if": "steps.cache.outputs.cache-hit != 'true'",
            "shell": "bash",
            "run": "if ! command -v pnpm >/dev/null 2>&1; then\n  echo \"üì¶ Installing PNPM...\"\n  npm install -g pnpm@latest\nfi\necho \"‚úÖ PNPM $(pnpm --version)\"\n"
          },
          {
            "name": "Verify cache status",
            "shell": "bash",
            "run": "if [[ \"${{ steps.cache.outputs.cache-hit }}\" == \"true\" ]]; then\n  echo \"üéØ Cache hit! Using cached dependencies\"\nelse\n  echo \"üíæ Cache miss - dependencies will be installed\"\nfi\n"
          }
        ]
      },
      "secrets_refs": [],
      "vars_refs": []
    },
    {
      "path": ".github/actions/neutralize/action.yaml",
      "name": "Neutralize With Summary",
      "description": "Sets a neutral outcome and writes a job summary explaining why the workflow was neutralized",
      "inputs": {
        "reason": {
          "description": "Reason for neutralization",
          "required": true
        },
        "deploy_state": {
          "description": "Deployment state from context",
          "required": false
        },
        "deploy_reason": {
          "description": "Deployment reason from context",
          "required": false
        },
        "deploy_url": {
          "description": "Deployment URL from context",
          "required": false
        }
      },
      "runs": {
        "using": "composite",
        "steps": [
          {
            "name": "Write neutralization summary",
            "shell": "bash",
            "run": "echo \"‚è∏Ô∏è Workflow Neutralized\" >> $GITHUB_STEP_SUMMARY\necho \"======================\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**Reason**: ${{ inputs.reason }}\" >> $GITHUB_STEP_SUMMARY\n\nif [ -n \"${{ inputs.deploy_state }}\" ]; then\n  echo \"**Deployment State**: \\`${{ inputs.deploy_state }}\\`\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [ -n \"${{ inputs.deploy_reason }}\" ]; then\n  echo \"**Details**: ${{ inputs.deploy_reason }}\" >> $GITHUB_STEP_SUMMARY\nfi\n\nif [ -n \"${{ inputs.deploy_url }}\" ]; then\n  echo \"**URL**: ${{ inputs.deploy_url }}\" >> $GITHUB_STEP_SUMMARY\nfi\n\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"### üîÑ Next Steps\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"This is normal and expected for:\" >> $GITHUB_STEP_SUMMARY\necho \"- Failed deployments (fix deployment issues first)\" >> $GITHUB_STEP_SUMMARY\necho \"- Deployments still in progress (wait for completion)\" >> $GITHUB_STEP_SUMMARY\necho \"- Missing preview URLs (check Vercel configuration)\" >> $GITHUB_STEP_SUMMARY\necho \"- Fork PRs (restricted permissions)\" >> $GITHUB_STEP_SUMMARY\necho \"\" >> $GITHUB_STEP_SUMMARY\necho \"**No action required** - this is an expected neutral outcome.\" >> $GITHUB_STEP_SUMMARY\n\necho \"‚è∏Ô∏è Workflow neutralized: ${{ inputs.reason }}\"\n"
          }
        ]
      },
      "secrets_refs": [],
      "vars_refs": []
    },
    {
      "path": ".github/actions/setup-node/action.yml",
      "name": "Setup Node.js with Cache",
      "description": "Setup Node.js with optimized dependency caching",
      "inputs": {
        "node-version": {
          "description": "Node.js version to use",
          "required": false,
          "default": "18"
        },
        "cache-key-suffix": {
          "description": "Additional cache key suffix",
          "required": false,
          "default": ""
        },
        "install-dependencies": {
          "description": "Whether to install dependencies",
          "required": false,
          "default": "true"
        },
        "working-directory": {
          "description": "Working directory for npm operations",
          "required": false,
          "default": "."
        }
      },
      "outputs": {
        "node-version": {
          "description": "Node.js version that was installed",
          "value": "${{ steps.setup.outputs.node-version }}"
        },
        "cache-hit": {
          "description": "Whether cache was hit",
          "value": "${{ steps.cache.outputs.cache-hit }}"
        }
      },
      "runs": {
        "using": "composite",
        "steps": [
          {
            "name": "Setup Node.js",
            "id": "setup",
            "uses": "actions/setup-node@v4",
            "with": {
              "node-version": "${{ inputs.node-version }}",
              "cache": "npm",
              "cache-dependency-path": "${{ inputs.working-directory }}/package-lock.json"
            }
          },
          {
            "name": "Cache node_modules",
            "id": "cache",
            "uses": "actions/cache@v3",
            "with": {
              "path": "${{ inputs.working-directory }}/node_modules\n~/.npm\n",
              "key": "npm-${{ runner.os }}-${{ hashFiles(format('{0}/package-lock.json', inputs.working-directory)) }}-${{ inputs.cache-key-suffix }}",
              "restore-keys": "npm-${{ runner.os }}-${{ hashFiles(format('{0}/package-lock.json', inputs.working-directory)) }}-\nnpm-${{ runner.os }}-\n"
            }
          },
          {
            "name": "Install dependencies",
            "if": "inputs.install-dependencies == 'true' && steps.cache.outputs.cache-hit != 'true'",
            "shell": "bash",
            "working-directory": "${{ inputs.working-directory }}",
            "run": "echo \"üì¶ Installing dependencies...\"\nnpm ci --prefer-offline --no-audit --no-fund\n"
          },
          {
            "name": "Verify installation",
            "if": "inputs.install-dependencies == 'true'",
            "shell": "bash",
            "working-directory": "${{ inputs.working-directory }}",
            "run": "echo \"‚úÖ Node.js $(node --version)\"\necho \"‚úÖ npm $(npm --version)\"\necho \"‚úÖ Dependencies: $(ls node_modules | wc -l) packages\"\n"
          }
        ]
      },
      "secrets_refs": [],
      "vars_refs": []
    },
    {
      "path": ".github/actions/setup-supabase-rest/action.yml",
      "name": "Setup Supabase REST Environment",
      "description": "Configure Supabase connection and environment variables for API testing",
      "inputs": {
        "supabase-url": {
          "description": "Supabase project URL",
          "required": true
        },
        "supabase-service-key": {
          "description": "Supabase service role key",
          "required": true
        },
        "database-url": {
          "description": "Direct database connection URL",
          "required": false
        },
        "node-env": {
          "description": "Node environment",
          "required": false,
          "default": "production"
        },
        "verify-connection": {
          "description": "Whether to verify the connection",
          "required": false,
          "default": "true"
        }
      },
      "outputs": {
        "connection-status": {
          "description": "Connection verification status",
          "value": "${{ steps.verify.outputs.status }}"
        },
        "tables-available": {
          "description": "Number of accessible tables",
          "value": "${{ steps.verify.outputs.tables }}"
        }
      },
      "runs": {
        "using": "composite",
        "steps": [
          {
            "name": "Configure environment variables",
            "shell": "bash",
            "run": "echo \"üîß Configuring Supabase environment...\"\necho \"SUPABASE_URL=${{ inputs.supabase-url }}\" >> $GITHUB_ENV\necho \"SUPABASE_SERVICE_ROLE_KEY=${{ inputs.supabase-service-key }}\" >> $GITHUB_ENV\necho \"NODE_ENV=${{ inputs.node-env }}\" >> $GITHUB_ENV\n\n# Sanity guard\nif [ -z \"${{ inputs.supabase-service-key }}\" ]; then\n  echo \"Missing SUPABASE_SERVICE_ROLE_KEY\" >&2\n  exit 1\nfi\n\n# Set database URL if provided\nif [[ -n \"${{ inputs.database-url }}\" ]]; then\n  echo \"DATABASE_URL=${{ inputs.database-url }}\" >> $GITHUB_ENV\nfi\n\n# Configure for production-like environment\necho \"POSTGRES_URL=${{ inputs.database-url }}\" >> $GITHUB_ENV\necho \"VERCEL_ENV=production\" >> $GITHUB_ENV\n"
          },
          {
            "name": "Supabase reachability (status only)",
            "shell": "bash",
            "run": "code=$(curl -sS -o /dev/null -w \"%{http_code}\" \\\n  -H \"apikey: ${SUPABASE_SERVICE_ROLE_KEY}\" \\\n  -H \"Authorization: Bearer ${SUPABASE_SERVICE_ROLE_KEY}\" \\\n  \"${SUPABASE_URL}/rest/v1/\")\necho \"SUPABASE_REST_CODE=${code}\"\ntest \"${code}\" = \"200\"\n"
          },
          {
            "name": "Verify Supabase connection",
            "id": "verify",
            "if": "inputs.verify-connection == 'true'",
            "shell": "bash",
            "run": "echo \"üîç Verifying Supabase connection...\"\n\n# Test REST API connection\nRESPONSE=$(curl -s -w \"HTTPSTATUS:%{http_code}\" \\\n  -H \"apikey: ${{ inputs.supabase-service-key }}\" \\\n  -H \"Authorization: Bearer ${{ inputs.supabase-service-key }}\" \\\n  \"${{ inputs.supabase-url }}/rest/v1/\" \\\n  --max-time 10)\n\nHTTP_STATUS=$(echo $RESPONSE | tr -d '\\n' | sed -e 's/.*HTTPSTATUS://')\n\nif [ \"$HTTP_STATUS\" -eq 200 ]; then\n  echo \"‚úÖ Supabase REST API connection successful\"\n  echo \"status=connected\" >> $GITHUB_OUTPUT\n  \n  # Try to get table count\n  TABLES_RESPONSE=$(curl -s \\\n    -H \"apikey: ${{ inputs.supabase-service-key }}\" \\\n    -H \"Authorization: Bearer ${{ inputs.supabase-service-key }}\" \\\n    \"${{ inputs.supabase-url }}/rest/v1/?select=*\" \\\n    --max-time 5 || echo \"[]\")\n  \n  # Count accessible tables (rough estimate)\n  TABLE_COUNT=$(echo \"$TABLES_RESPONSE\" | grep -o '\"' | wc -l | awk '{print int($1/2)}')\n  echo \"üìä Estimated accessible tables: $TABLE_COUNT\"\n  echo \"tables=$TABLE_COUNT\" >> $GITHUB_OUTPUT\n  \nelse\n  echo \"‚ùå Supabase connection failed (HTTP $HTTP_STATUS)\"\n  echo \"status=failed\" >> $GITHUB_OUTPUT\n  echo \"tables=0\" >> $GITHUB_OUTPUT\n  exit 1\nfi\n"
          },
          {
            "name": "Configure database-specific settings",
            "shell": "bash",
            "run": "echo \"‚öôÔ∏è Configuring database settings...\"\n\n# Production database configuration\necho \"USE_POSTGRES_IN_DEV=true\" >> $GITHUB_ENV\necho \"CI=true\" >> $GITHUB_ENV\n\n# Disable SQLite fallback in production environment\necho \"DATABASE_URL_SQLITE=\" >> $GITHUB_ENV\n\necho \"‚úÖ Supabase environment configured\"\n"
          }
        ]
      },
      "secrets_refs": [],
      "vars_refs": []
    }
  ],
  "summary": {
    "workflow_count": 52,
    "composite_action_count": 4,
    "total_jobs": 105,
    "workflows_with_schedule": 36,
    "workflows_with_errors": 2
  }
}