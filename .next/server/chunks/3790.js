"use strict";exports.id=3790,exports.ids=[3790],exports.modules={73790:(a,b,c)=>{c.a(a,async(a,d)=>{try{c.d(b,{z:()=>j});var e=c(55229),f=c(99986),g=c(7462),h=a([e,f,g]);[e,f,g]=h.then?(await h)():h;class i{constructor(){this.maxBatchSize=50,this.flushInterval=1e4,this.metricBuffer=[],this.environment="production",this.startPeriodicFlush(),this.ensureMetricsTable()}async recordAPIMetric(a,b,c,d,e,f,g){let h={name:"api_response_time",value:c,unit:"ms",timestamp:new Date,platform:a,endpoint:b,responseTime:c,statusCode:d,rateLimitRemaining:e,quotaUsed:f,tags:{platform:a,endpoint:b.replace(/\/\d+/g,"/:id"),status:d>=200&&d<300?"success":"error"},metadata:{statusCode:d,rateLimitRemaining:e,quotaUsed:f,...g}};await this.addToBuffer(h)}async recordDatabaseQueryMetric(a,b,c,d,e){let f={name:"database_query_time",value:b,unit:"ms",timestamp:new Date,component:"database",operation:this.normalizeQuery(a),duration:b,success:c,tags:{operation:this.normalizeQuery(a),status:c?"success":"error"},metadata:{query:a.substring(0,500),rowCount:d,...e}};await this.addToBuffer(f)}async recordContentProcessingMetric(a,b,c,d=1,e){let f={name:"content_processing_time",value:b,unit:"ms",timestamp:new Date,component:"content_processor",operation:a,duration:b,success:c,tags:{operation:a,status:c?"success":"error"},metadata:{itemsProcessed:d,...e}};await this.addToBuffer(f);let g={name:"content_processing_throughput",value:d/(b/1e3),unit:"items/sec",timestamp:new Date,metric:"content_processed",count:d,period:"minute",tags:{operation:a,status:c?"success":"error"}};await this.addToBuffer(g)}async recordSystemMetrics(){try{let a=process.memoryUsage(),b={rss:Math.round(a.rss/1024/1024),heapTotal:Math.round(a.heapTotal/1024/1024),heapUsed:Math.round(a.heapUsed/1024/1024),external:Math.round(a.external/1024/1024)};b.heapUsed,b.heapTotal,await this.addToBuffer({name:"system_memory_usage",value:b.heapUsed,unit:"MB",timestamp:new Date,tags:{metric:"memory",type:"heap_used"},metadata:b});let c=process.cpuUsage(),d=(c.user+c.system)/1e6*100;await this.addToBuffer({name:"system_cpu_usage",value:d,unit:"percent",timestamp:new Date,tags:{metric:"cpu"},metadata:{user:Math.round(c.user/1e3),system:Math.round(c.system/1e3)}}),await this.addToBuffer({name:"process_uptime",value:process.uptime(),unit:"seconds",timestamp:new Date,tags:{metric:"uptime"}})}catch(a){await f.RE.logError("MetricsService","Failed to record system metrics",{error:a.message},a)}}async recordBusinessMetric(a,b,c="hour",d){let e={name:`business_${a}`,value:b,unit:"error_rate"===a?"percent":"count",timestamp:new Date,metric:a,count:"error_rate"===a?0:b,period:c,tags:{metric:a,period:c},metadata:d};await this.addToBuffer(e)}async recordCustomMetric(a,b,c,d,e){let f={name:a,value:b,unit:c,timestamp:new Date,tags:d,metadata:e};await this.addToBuffer(f)}async queryMetrics(a={}){try{let{name:b,tags:c,dateRange:d,limit:f=100,offset:g=0,aggregation:h,groupBy:i}=a,j=(0,e.P)("system_metrics").select(["id","name","value","unit","timestamp","tags","metadata"]).orderBy("timestamp","DESC");if(b&&b.length>0&&(j=j.whereIn("name",b)),d&&(j=j.where("timestamp",">=",d.start).where("timestamp","<=",d.end)),c)for(let[a,b]of Object.entries(c))j=j.whereRaw("tags->? = ?",[a,b]);if(h&&!i){let a,b=j.clone();switch(h){case"avg":default:a="AVG(value) as aggregated_value";break;case"sum":a="SUM(value) as aggregated_value";break;case"count":a="COUNT(*) as aggregated_value";break;case"min":a="MIN(value) as aggregated_value";break;case"max":a="MAX(value) as aggregated_value"}let c=await b.select([a]).first();return{metrics:[],total:0,hasMore:!1,aggregatedValue:parseFloat(c?.aggregated_value||"0")}}let k=await j.clone().count("*").first(),l=parseInt(k?.count||"0"),m=(await j.limit(f).offset(g).execute()).map(a=>({id:a.id,name:a.name,value:parseFloat(a.value),unit:a.unit,timestamp:new Date(a.timestamp),tags:a.tags||{},metadata:a.metadata||{}}));return{metrics:m,total:l,hasMore:g+m.length<l}}catch(b){throw await f.RE.logError("MetricsService","Failed to query metrics",{filters:a,error:b.message},b),b}}async getMetricsSummary(){try{let a=new Date,b=new Date(a.getTime()-864e5),c=new Date(a.getTime()-36e5),d=await (0,e.P)("system_metrics").count("*").first(),f=parseInt(d?.count||"0"),g=await this.queryMetrics({name:["api_response_time"],dateRange:{start:c,end:a},limit:1e3}),h={reddit:0,instagram:0,tiktok:0},i={reddit:0,instagram:0,tiktok:0};g.metrics.forEach(a=>{let b=a.tags?.platform;b&&h.hasOwnProperty(b)&&(h[b]+=a.value,i[b]++)}),Object.keys(h).forEach(a=>{i[a]>0&&(h[a]=Math.round(h[a]/i[a]))});let j=await this.queryMetrics({name:["system_memory_usage","system_cpu_usage"],dateRange:{start:c,end:a},limit:10}),k=0,l=0;j.metrics.forEach(a=>{"system_memory_usage"===a.name&&a.metadata?.heapTotal?k=Math.round(a.value/a.metadata.heapTotal*100):"system_cpu_usage"===a.name&&(l=Math.round(a.value))});let m=await this.queryMetrics({name:["business_content_processed"],dateRange:{start:b,end:a},aggregation:"sum"}),n=await this.queryMetrics({name:["business_posts_created"],dateRange:{start:b,end:a},aggregation:"sum"}),o=await this.queryMetrics({name:["business_error_rate"],dateRange:{start:c,end:a},aggregation:"avg"}),p=await (0,e.P)("content_queue").where("status","pending").count("*").first(),q=parseInt(p?.count||"0"),r=(await (0,e.P)("system_metrics").select(["tags","AVG(value) as avg_time","COUNT(*) as count"]).where("name","content_processing_time").where("timestamp",">",b).groupBy("tags").orderBy("avg_time","DESC").limit(5).execute()).map(a=>({operation:a.tags?.operation||"unknown",avgResponseTime:Math.round(parseFloat(a.avg_time||"0")),count:parseInt(a.count||"0")}));return{totalMetrics:f,recentAPIResponseTimes:h,systemResources:{memoryUsagePercent:k,cpuUsagePercent:l,diskUsagePercent:0},businessKPIs:{contentProcessedLast24h:Math.round(m.aggregatedValue||0),postsCreatedLast24h:Math.round(n.aggregatedValue||0),errorRateLast1h:Math.round(100*(o.aggregatedValue||0))/100,queueSize:q},topSlowOperations:r}}catch(a){throw await f.RE.logError("MetricsService","Failed to get metrics summary",{error:a.message},a),a}}async cleanupOldMetrics(a=7){try{let b=new Date;b.setDate(b.getDate()-a);let c=await (0,e.P)("system_metrics").where("timestamp","<",b).delete();return await f.RE.logInfo("MetricsService",`Cleaned up ${c} old metric entries`,{retentionDays:a,cutoffDate:b.toISOString()}),c}catch(b){throw await f.RE.logError("MetricsService","Failed to cleanup old metrics",{retentionDays:a,error:b.message},b),b}}async exportMetrics(a={}){try{let b=await this.queryMetrics({...a,limit:1e4});return JSON.stringify(b.metrics,null,2)}catch(b){throw await f.RE.logError("MetricsService","Failed to export metrics",{filters:a},b),b}}async getPerformanceStats(){try{let a=new Date,b=new Date(a.getTime()-3e5),c=await this.queryMetrics({name:["api_response_time"],dateRange:{start:b,end:a},aggregation:"avg"}),d=await this.queryMetrics({name:["database_query_time"],dateRange:{start:b,end:a},aggregation:"avg"}),e=await this.queryMetrics({name:["content_processing_time"],dateRange:{start:b,end:a},aggregation:"avg"}),f=await this.queryMetrics({tags:{status:"success"},dateRange:{start:b,end:a}}),g=await this.queryMetrics({dateRange:{start:b,end:a}}),h=g.total>0?f.total/g.total*100:100,i=Math.round(g.total/5);return{avgAPIResponseTime:Math.round(c.aggregatedValue||0),avgDatabaseQueryTime:Math.round(d.aggregatedValue||0),avgContentProcessingTime:Math.round(e.aggregatedValue||0),successRate:Math.round(100*h)/100,requestsPerMinute:i}}catch(a){return await f.RE.logError("MetricsService","Failed to get performance stats",{error:a.message},a),{avgAPIResponseTime:0,avgDatabaseQueryTime:0,avgContentProcessingTime:0,successRate:0,requestsPerMinute:0}}}async addToBuffer(a){this.metricBuffer.push(a),this.metricBuffer.length>=this.maxBatchSize&&await this.flushBuffer()}async flushBuffer(){if(0===this.metricBuffer.length)return;let a=[...this.metricBuffer];this.metricBuffer=[];try{await this.ensureMetricsTable(),await (0,e.Yr)("system_metrics").values(a.map(a=>({name:a.name,value:a.value,unit:a.unit,timestamp:a.timestamp,tags:JSON.stringify(a.tags||{}),metadata:JSON.stringify(a.metadata||{}),environment:this.environment}))).execute()}catch(b){console.error("Failed to flush metrics buffer:",b),this.metricBuffer.unshift(...a)}}startPeriodicFlush(){this.flushTimer&&clearInterval(this.flushTimer),this.flushTimer=setInterval(async()=>{await this.flushBuffer()},this.flushInterval),setInterval(async()=>{await this.recordSystemMetrics()},3e4)}async ensureMetricsTable(){try{await g.db.query(`
        CREATE TABLE IF NOT EXISTS system_metrics (
          id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
          name VARCHAR(100) NOT NULL,
          value NUMERIC NOT NULL,
          unit VARCHAR(50) NOT NULL,
          timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
          tags JSONB DEFAULT '{}',
          metadata JSONB DEFAULT '{}',  
          environment VARCHAR(50) NOT NULL,
          created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
        )
      `),await g.db.query(`
        CREATE INDEX IF NOT EXISTS idx_system_metrics_name ON system_metrics(name);
        CREATE INDEX IF NOT EXISTS idx_system_metrics_timestamp ON system_metrics(timestamp DESC);
        CREATE INDEX IF NOT EXISTS idx_system_metrics_tags ON system_metrics USING GIN(tags);
        CREATE INDEX IF NOT EXISTS idx_system_metrics_environment ON system_metrics(environment);
        CREATE INDEX IF NOT EXISTS idx_system_metrics_name_timestamp ON system_metrics(name, timestamp DESC);
      `)}catch(a){console.error("Failed to ensure metrics table exists:",a)}}normalizeQuery(a){return a.toLowerCase().replace(/\s+/g," ").replace(/\b\d+\b/g,"?").replace(/'[^']*'/g,"?").substring(0,100).split(" ")[0]||"unknown"}async shutdown(){this.flushTimer&&clearInterval(this.flushTimer),await this.flushBuffer()}}let j=new i;"undefined"!=typeof process&&(process.on("SIGTERM",async()=>{await j.shutdown()}),process.on("SIGINT",async()=>{await j.shutdown()})),d()}catch(a){d(a)}})}};